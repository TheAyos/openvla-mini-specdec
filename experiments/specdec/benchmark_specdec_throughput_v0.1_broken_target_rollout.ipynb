{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 16:03:22.324441: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-15 16:03:22.327730: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-15 16:03:22.360177: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-15 16:03:22.360225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-15 16:03:22.362132: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-15 16:03:22.370888: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-15 16:03:24.891631: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "/mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2025-12-15 16:03:41.747545: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 15 16:03:50 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 NVL                Off |   00000000:46:00.0 Off |                    0 |\n",
      "| N/A   42C    P0             65W /  400W |       4MiB /  95830MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 NVL                Off |   00000000:49:00.0 Off |                    0 |\n",
      "| N/A   40C    P0             63W /  400W |       4MiB /  95830MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA H100 NVL                Off |   00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             64W /  400W |       4MiB /  95830MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA H100 NVL                Off |   00000000:4F:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             62W /  400W |       4MiB /  95830MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PRISMATIC_DATA_ROOT'] = ''\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "\n",
    "import sys\n",
    "# sys.path.insert(0, str(Path(__file__).resolve().parents[3]))\n",
    "# sys.path.insert(0, str(Path().resolve().parents[1]))\n",
    "sys.path.append(\"../..\")\n",
    "from libero.libero import benchmark\n",
    "\n",
    "from experiments.robot.libero.libero_utils import get_libero_env, get_libero_image, quat2axisangle\n",
    "from experiments.robot.openvla_utils import get_processor, get_vla, get_prismatic_vla\n",
    "from experiments.robot.robot_utils import get_image_resize_size, set_seed_everywhere\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available(), \"ERROR: CUDA not available!\"\n",
    "\n",
    "os.system(\"nvidia-smi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Speculative Decoding Throughput Benchmark\n",
      "================================================================================\n",
      "Target: /pub/scratch/aagouzoul/ovla/openvla-mini/ft_experiments_logs/openvla-7b+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug_step-75000_l1-loss-0.0012_tokacc-0.955\n",
      "Draft: Stanford-ILIAD/minivla-libero90-prismatic\n",
      "Gamma: 7\n",
      "Iterations: 10\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class BenchmarkConfig:\n",
    "    # fmt: off\n",
    "    # target_checkpoint: Union[str, Path] = \"/pub/scratch/aagouzoul/ovla/openvla-mini/ft_experiments_logs/openvla-7b+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug\"\n",
    "    target_checkpoint: Union[str, Path] = \"/pub/scratch/aagouzoul/ovla/openvla-mini/ft_experiments_logs/openvla-7b+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug_step-75000_l1-loss-0.0012_tokacc-0.955\"\n",
    "    draft_checkpoint: Union[str, Path] = \"Stanford-ILIAD/minivla-libero90-prismatic\"\n",
    "    hf_token: str = Path(\"/pub/scratch/aagouzoul/ovla/openvla-mini/.hf_token\")\n",
    "    load_in_8bit: bool = False\n",
    "    load_in_4bit: bool = False\n",
    "    center_crop: bool = True\n",
    "    \n",
    "    # Speculative decoding parameters\n",
    "    gamma: int = 7\n",
    "    temperature: float = 0.0\n",
    "    relaxed_acceptance_r: int = 7\n",
    "    \n",
    "    # Benchmark parameters\n",
    "    task_suite_name: str = \"libero_90\"\n",
    "    task_id: int = 0\n",
    "    num_iterations: int = 10\n",
    "    warmup_iterations: int = 3\n",
    "    seed: int = 42\n",
    "    \n",
    "    # fmt: on\n",
    "    \n",
    "cfg = BenchmarkConfig()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Speculative Decoding Throughput Benchmark\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Target: {cfg.target_checkpoint}\")\n",
    "print(f\"Draft: {cfg.draft_checkpoint}\")\n",
    "print(f\"Gamma: {cfg.gamma}\")\n",
    "print(f\"Iterations: {cfg.num_iterations}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "set_seed_everywhere(cfg.seed)\n",
    "\n",
    "class TargetConfig:\n",
    "    def __init__(self, c):\n",
    "        self.pretrained_checkpoint = c.target_checkpoint\n",
    "        self.load_in_8bit = c.load_in_8bit\n",
    "        self.load_in_4bit = c.load_in_4bit\n",
    "        self.hf_token = c.hf_token\n",
    "\n",
    "class DraftConfig:\n",
    "    def __init__(self, c):\n",
    "        self.pretrained_checkpoint = c.draft_checkpoint\n",
    "        self.model_family = \"prismatic\"\n",
    "        self.hf_token = c.hf_token\n",
    "        self.center_crop = c.center_crop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/4] Loading TARGET model (OpenVLA)...\n",
      "[*] Instantiating Pretrained VLA model\n",
      "[*] Loading in BF16 with Flash-Attention Enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae468b2b669743c193ce22b5f78b1c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;255;165;0m[SRPPPPPPPPPP] -> .eval() + torch.compile(vla) DISABLED\u001b[0m\n",
      "\n",
      "[2/4] Loading DRAFT model (MiniVLA)...\n",
      "[*] Initializing Generation Playground with `prismatic`\n",
      "Loading VLM from checkpoint: Stanford-ILIAD/minivla-libero90-prismatic\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">12/15 [16:03:57] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; <span style=\"font-weight: bold\">[</span>*<span style=\"font-weight: bold\">]</span> Checking HF for `Stanford-ILIAD/minivla-libero90-prismatic`          <a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">load.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py#163\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m12/15 [16:03:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> \u001b[1m[\u001b[0m*\u001b[1m]\u001b[0m Checking HF for `Stanford-ILIAD/minivla-libero90-prismatic`          \u001b]8;id=234053;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py\u001b\\\u001b[2mload.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=146316;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py#163\u001b\\\u001b[2m163\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; <span style=\"font-weight: bold\">[</span>*<span style=\"font-weight: bold\">]</span> Downloading Model `Stanford-ILIAD/minivla-libero90-prismatic` Config <a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">load.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         &amp; Checkpoint `step-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">122500</span>-epoch-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55</span>-<span style=\"color: #808000; text-decoration-color: #808000\">loss</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0743</span>.pt`                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> \u001b[1m[\u001b[0m*\u001b[1m]\u001b[0m Downloading Model `Stanford-ILIAD/minivla-libero90-prismatic` Config \u001b]8;id=571858;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py\u001b\\\u001b[2mload.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=91161;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         & Checkpoint `step-\u001b[1;36m122500\u001b[0m-epoch-\u001b[1;36m55\u001b[0m-\u001b[33mloss\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0743\u001b[0m.pt`                            \u001b[2m           \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">12/15 [16:03:58] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; <span style=\"font-weight: bold\">[</span>*<span style=\"font-weight: bold\">]</span> Base vlm: prism-qwen25-extra-dinosiglip-224px+0_5b                   <a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">load.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py#212\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">212</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m12/15 [16:03:58]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> \u001b[1m[\u001b[0m*\u001b[1m]\u001b[0m Base vlm: prism-qwen25-extra-dinosiglip-224px+0_5b                   \u001b]8;id=229258;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py\u001b\\\u001b[2mload.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=243962;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py#212\u001b\\\u001b[2m212\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; <span style=\"font-weight: bold\">[</span>*<span style=\"font-weight: bold\">]</span> Found Config =&gt;&gt; Loading &amp; Freezing                                  <a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">load.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py#221\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">221</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">prism-qwen25-extra-dinosiglip-224px+0_5b</span> with:                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>                      Vision Backbone =&gt;&gt; <span style=\"font-weight: bold\">dinosiglip-vit-so-224px</span>                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>                      LLM Backbone    =&gt;&gt; <span style=\"font-weight: bold\">qwen25-0_5b-extra</span>                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>                      Arch Specifier  =&gt;&gt; <span style=\"font-weight: bold\">no-align+fused-gelu-mlp</span>                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>                      Checkpoint Path =&gt;&gt;                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"text-decoration: underline\">`</span><span style=\"color: #800080; text-decoration-color: #800080; text-decoration: underline\">/pub/scratch/aagouzoul/huggingface/hub/models--Stanford-ILIAD--minivla-liber</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #800080; text-decoration-color: #800080; text-decoration: underline\">o90-prismatic/snapshots/4289f87e8e00706e188c7a3a61fc6e7d72ab2564/checkpoints/</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; text-decoration: underline\">step-122500-epoch-55-</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; text-decoration: underline\">loss</span><span style=\"text-decoration: underline\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; text-decoration: underline\">0.0743</span><span style=\"text-decoration: underline\">.pt`</span>                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> \u001b[1m[\u001b[0m*\u001b[1m]\u001b[0m Found Config =>> Loading & Freezing                                  \u001b]8;id=750800;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py\u001b\\\u001b[2mload.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=681453;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py#221\u001b\\\u001b[2m221\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[1;34mprism-qwen25-extra-dinosiglip-224px+0_5b\u001b[0m with:                                \u001b[2m           \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m                      Vision Backbone =>> \u001b[1mdinosiglip-vit-so-224px\u001b[0m                      \u001b[2m           \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m                      LLM Backbone    =>> \u001b[1mqwen25-0_5b-extra\u001b[0m                            \u001b[2m           \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m                      Arch Specifier  =>> \u001b[1mno-align+fused-gelu-mlp\u001b[0m                      \u001b[2m           \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m                      Checkpoint Path =>>                                              \u001b[2m           \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[4m`\u001b[0m\u001b[4;35m/pub/scratch/aagouzoul/huggingface/hub/models--Stanford-ILIAD--minivla-liber\u001b[0m \u001b[2m           \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[4;35mo90-prismatic/snapshots/4289f87e8e00706e188c7a3a61fc6e7d72ab2564/checkpoints/\u001b[0m \u001b[2m           \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[4;95mstep-122500-epoch-55-\u001b[0m\u001b[4;95mloss\u001b[0m\u001b[4m=\u001b[0m\u001b[1;4;36m0\u001b[0m\u001b[1;4;36m.0743\u001b[0m\u001b[4m.pt`\u001b[0m                                          \u001b[2m           \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; <span style=\"font-weight: bold\">[</span>*<span style=\"font-weight: bold\">]</span> Loading Vision Backbone <span style=\"font-weight: bold\">dinosiglip-vit-so-224px</span>                      <a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">load.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py#236\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">236</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> \u001b[1m[\u001b[0m*\u001b[1m]\u001b[0m Loading Vision Backbone \u001b[1mdinosiglip-vit-so-224px\u001b[0m                      \u001b]8;id=617889;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py\u001b\\\u001b[2mload.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=291704;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py#236\u001b\\\u001b[2m236\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">12/15 [16:04:03] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; Loading pretrained weights from Hugging Face hub                     <a href=\"file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_builder.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_builder.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_builder.py#186\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">186</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"font-weight: bold\">(</span>timm/vit_large_patch14_reg4_dinov2.lvd142m<span style=\"font-weight: bold\">)</span>                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m12/15 [16:04:03]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> Loading pretrained weights from Hugging Face hub                     \u001b]8;id=732052;file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_builder.py\u001b\\\u001b[2m_builder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=443143;file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_builder.py#186\u001b\\\u001b[2m186\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[1m(\u001b[0mtimm/vit_large_patch14_reg4_dinov2.lvd142m\u001b[1m)\u001b[0m                              \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt;  Safe alternative available for <span style=\"color: #008000; text-decoration-color: #008000\">'pytorch_model.bin'</span> <span style=\"font-weight: bold\">(</span>as                  <a href=\"file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_hub.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_hub.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_hub.py#180\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">180</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'model.safetensors'</span><span style=\"font-weight: bold\">)</span>. Loading weights using safetensors.                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >>  Safe alternative available for \u001b[32m'pytorch_model.bin'\u001b[0m \u001b[1m(\u001b[0mas                  \u001b]8;id=107175;file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_hub.py\u001b\\\u001b[2m_hub.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=97251;file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_hub.py#180\u001b\\\u001b[2m180\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[32m'model.safetensors'\u001b[0m\u001b[1m)\u001b[0m. Loading weights using safetensors.                      \u001b[2m           \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; Resized position embedding: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37</span><span style=\"font-weight: bold\">)</span> to <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">)</span>.                    <a href=\"file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/layers/pos_embed.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">pos_embed.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/layers/pos_embed.py#55\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> Resized position embedding: \u001b[1m(\u001b[0m\u001b[1;36m37\u001b[0m, \u001b[1;36m37\u001b[0m\u001b[1m)\u001b[0m to \u001b[1m(\u001b[0m\u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m)\u001b[0m.                    \u001b]8;id=277370;file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/layers/pos_embed.py\u001b\\\u001b[2mpos_embed.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=846335;file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/layers/pos_embed.py#55\u001b\\\u001b[2m55\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">12/15 [16:04:09] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; Loading pretrained weights from Hugging Face hub                     <a href=\"file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_builder.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_builder.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_builder.py#186\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">186</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"font-weight: bold\">((</span><span style=\"color: #008000; text-decoration-color: #008000\">'timm/ViT-SO400M-14-SigLIP'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'open_clip_pytorch_model.bin'</span><span style=\"font-weight: bold\">))</span>            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m12/15 [16:04:09]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> Loading pretrained weights from Hugging Face hub                     \u001b]8;id=130889;file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_builder.py\u001b\\\u001b[2m_builder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=967096;file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_builder.py#186\u001b\\\u001b[2m186\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'timm/ViT-SO400M-14-SigLIP'\u001b[0m, \u001b[32m'open_clip_pytorch_model.bin'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m            \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt;  Safe alternative available for <span style=\"color: #008000; text-decoration-color: #008000\">'open_clip_pytorch_model.bin'</span> <span style=\"font-weight: bold\">(</span>as        <a href=\"file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_hub.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_hub.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_hub.py#180\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">180</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'open_clip_model.safetensors'</span><span style=\"font-weight: bold\">)</span>. Loading weights using safetensors.            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >>  Safe alternative available for \u001b[32m'open_clip_pytorch_model.bin'\u001b[0m \u001b[1m(\u001b[0mas        \u001b]8;id=869693;file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_hub.py\u001b\\\u001b[2m_hub.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=659176;file:///mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/timm/models/_hub.py#180\u001b\\\u001b[2m180\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[32m'open_clip_model.safetensors'\u001b[0m\u001b[1m)\u001b[0m. Loading weights using safetensors.            \u001b[2m           \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">12/15 [16:04:10] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; <span style=\"font-weight: bold\">[</span>*<span style=\"font-weight: bold\">]</span> Loading Pretrained LLM <span style=\"font-weight: bold\">qwen25-0_5b-extra</span> via HF Transformers         <a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">load.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py#244\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">244</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m12/15 [16:04:10]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> \u001b[1m[\u001b[0m*\u001b[1m]\u001b[0m Loading Pretrained LLM \u001b[1mqwen25-0_5b-extra\u001b[0m via HF Transformers         \u001b]8;id=201629;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py\u001b\\\u001b[2mload.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=738797;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py#244\u001b\\\u001b[2m244\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt;     |=&gt; Building empty <span style=\"font-weight: bold\">qwen2.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> LLM from <span style=\"text-decoration: underline\">`Qwen/Qwen2.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; text-decoration: underline\">5</span><span style=\"text-decoration: underline\">-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; text-decoration: underline\">0.</span><span style=\"text-decoration: underline\">5B`</span>          <a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/backbones/llm/base_llm.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_llm.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/backbones/llm/base_llm.py#134\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">134</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >>     |=> Building empty \u001b[1mqwen2.\u001b[0m\u001b[1;36m5\u001b[0m LLM from \u001b[4m`Qwen/Qwen2.\u001b[0m\u001b[1;4;36m5\u001b[0m\u001b[4m-\u001b[0m\u001b[1;4;36m0.\u001b[0m\u001b[4m5B`\u001b[0m          \u001b]8;id=83667;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/backbones/llm/base_llm.py\u001b\\\u001b[2mbase_llm.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=896865;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/backbones/llm/base_llm.py#134\u001b\\\u001b[2m134\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">12/15 [16:04:17] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt;     |=&gt; Loading <span style=\"font-weight: bold\">qwen2.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> <span style=\"font-weight: bold\">(</span>Fast<span style=\"font-weight: bold\">)</span> Tokenizer via the AutoTokenizer API   <a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/backbones/llm/base_llm.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_llm.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/backbones/llm/base_llm.py#156\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">156</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m12/15 [16:04:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >>     |=> Loading \u001b[1mqwen2.\u001b[0m\u001b[1;36m5\u001b[0m \u001b[1m(\u001b[0mFast\u001b[1m)\u001b[0m Tokenizer via the AutoTokenizer API   \u001b]8;id=475435;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/backbones/llm/base_llm.py\u001b\\\u001b[2mbase_llm.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=666563;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/backbones/llm/base_llm.py#156\u001b\\\u001b[2m156\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 256 extra tokens.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">12/15 [16:04:18] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; <span style=\"font-weight: bold\">[</span>*<span style=\"font-weight: bold\">]</span> Loading VLA <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">prism-qwen25-extra-dinosiglip-224px+0_5b</span> from Checkpoint <a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">load.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py#257\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">257</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m12/15 [16:04:18]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> \u001b[1m[\u001b[0m*\u001b[1m]\u001b[0m Loading VLA \u001b[1;34mprism-qwen25-extra-dinosiglip-224px+0_5b\u001b[0m from Checkpoint \u001b]8;id=219684;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py\u001b\\\u001b[2mload.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=702729;file:///pub/scratch/aagouzoul/ovla/openvla-mini/experiments/specdec/../../prismatic/models/load.py#257\u001b\\\u001b[2m257\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;255;165;0m[SRPPPPPPPPPP] -> .eval() + torch.compile(prismatic) DISABLED\u001b[0m\n",
      "Using unnorm_keys: libero_90_no_noops (target), libero_90 (draft)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[1/4] Loading TARGET model (OpenVLA)...\")\n",
    "target_cfg = TargetConfig(cfg)\n",
    "target_model = get_vla(target_cfg)\n",
    "target_processor = get_processor(target_cfg)\n",
    "\n",
    "print(\"\\n[2/4] Loading DRAFT model (MiniVLA)...\")\n",
    "draft_cfg = DraftConfig(cfg)\n",
    "draft_model = get_prismatic_vla(draft_cfg)\n",
    "\n",
    "# Set unnorm key\n",
    "unnorm_key_target = cfg.task_suite_name\n",
    "if unnorm_key_target not in target_model.norm_stats:\n",
    "    if f\"{unnorm_key_target}_no_noops\" in target_model.norm_stats:\n",
    "        unnorm_key_target = f\"{unnorm_key_target}_no_noops\"\n",
    "    elif f\"{unnorm_key_target.replace('_no_noops', '')}\" in target_model.norm_stats:\n",
    "        unnorm_key_target = f\"{unnorm_key_target}\"\n",
    "    else:\n",
    "        unnorm_key_target = list(target_model.norm_stats.keys())[0]\n",
    "        \n",
    "unnorm_key_draft = cfg.task_suite_name\n",
    "if unnorm_key_draft not in draft_model.norm_stats:\n",
    "    if f\"{unnorm_key_draft}_no_noops\" in draft_model.norm_stats:\n",
    "        unnorm_key_draft = f\"{unnorm_key_draft}_no_noops\"\n",
    "    elif f\"{unnorm_key_draft.replace('_no_noops', '')}\" in draft_model.norm_stats:\n",
    "        unnorm_key_draft = f\"{unnorm_key_draft}\"\n",
    "    else:\n",
    "        unnorm_key_draft = list(draft_model.norm_stats.keys())[0]\n",
    "\n",
    "print(f\"Using unnorm_keys: {unnorm_key_target} (target), {unnorm_key_draft} (draft)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/4] Loading LIBERO task: libero_90 (task 0)...\n",
      "Task: close the top drawer of the cabinet\n",
      "Image shape: (224, 224, 3)\n",
      "\u001b[38;2;255;165;0m[SRP] -> \u001b[0m call params: get_vla_action(target_model..., target_processor..., cfg.target_checkpoint=/pub/scratch/aagouzoul/ovla/openvla-mini/ft_experiments_logs/openvla-7b+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug_step-75000_l1-loss-0.0012_tokacc-0.955, observation=..., task_description=close the top drawer of the cabinet, unnorm_key=libero_90_no_noops, center_crop=True)\n",
      "\u001b[38;2;255;165;0m[SRP] -> \u001b[0m call params: get_prismatic_vla_action(draft_model..., processor=None, cfg.draft_checkpoint=Stanford-ILIAD/minivla-libero90-prismatic, observation..., task_description=close the top drawer of the cabinet, unnorm_key=libero_90, center_crop=True)\n",
      "\n",
      "Running warmup (3 iterations each)...\n",
      "  Warming up TARGET...\n",
      "  Warming up DRAFT...\n",
      "\n",
      "Benchmarking TARGET (10 iterations)...\n",
      "  Progress: 10/10, last: 219.9ms\n",
      "\n",
      "Benchmarking DRAFT (10 iterations)...\n",
      "  Progress: 10/10, last: 143.9ms\n"
     ]
    }
   ],
   "source": [
    "# Load LIBERO task and get observation\n",
    "print(f\"\\n[4/4] Loading LIBERO task: {cfg.task_suite_name} (task {cfg.task_id})...\")\n",
    "benchmark_dict = benchmark.get_benchmark_dict()\n",
    "task_suite = benchmark_dict[cfg.task_suite_name]()\n",
    "task = task_suite.get_task(cfg.task_id)\n",
    "env, task_description = get_libero_env(task, \"openvla\", resolution=224)\n",
    "\n",
    "initial_states = task_suite.get_task_init_states(cfg.task_id)\n",
    "env.reset()\n",
    "obs = env.set_init_state(initial_states[0])\n",
    "\n",
    "# Prepare observation\n",
    "img = get_libero_image(obs, 224)\n",
    "observation = {\n",
    "    \"full_image\": img,\n",
    "    \"state\": np.concatenate(\n",
    "        (obs[\"robot0_eef_pos\"], quat2axisangle(obs[\"robot0_eef_quat\"]), obs[\"robot0_gripper_qpos\"])\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"Task: {task_description}\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"call params: get_vla_action(target_model..., target_processor..., cfg.target_checkpoint={cfg.target_checkpoint}, observation=..., task_description={task_description}, unnorm_key={unnorm_key_target}, center_crop={cfg.center_crop})\")\n",
    "print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"call params: get_prismatic_vla_action(draft_model..., processor=None, cfg.draft_checkpoint={cfg.draft_checkpoint}, observation..., task_description={task_description}, unnorm_key={unnorm_key_draft}, center_crop={cfg.center_crop})\")\n",
    "\n",
    "def run_target_inference():\n",
    "    from experiments.robot.openvla_utils import get_vla_action\n",
    "    return get_vla_action(\n",
    "        target_model,\n",
    "        target_processor,\n",
    "        str(cfg.target_checkpoint),\n",
    "        observation,\n",
    "        task_description,\n",
    "        unnorm_key_target,\n",
    "        center_crop=cfg.center_crop,\n",
    "    )\n",
    "\n",
    "def run_draft_inference():\n",
    "    from experiments.robot.openvla_utils import get_prismatic_vla_action    \n",
    "    return get_prismatic_vla_action(\n",
    "        draft_model,\n",
    "        None,\n",
    "        str(cfg.draft_checkpoint),\n",
    "        observation,\n",
    "        task_description,\n",
    "        unnorm_key_draft,\n",
    "        center_crop=cfg.center_crop,\n",
    "    )\n",
    "\n",
    "def timed_cuda(fn):\n",
    "    \"\"\"Time a function using CUDA events for accurate GPU timing.\"\"\"\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end) / 1000  # Convert to seconds\n",
    "\n",
    "print(f\"\\nRunning warmup ({cfg.warmup_iterations} iterations each)...\")\n",
    "\n",
    "print(\"  Warming up TARGET...\")\n",
    "for _ in range(cfg.warmup_iterations):\n",
    "    run_target_inference()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(\"  Warming up DRAFT...\")\n",
    "for _ in range(cfg.warmup_iterations):\n",
    "    run_draft_inference()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "# Benchmark TARGET\n",
    "print(f\"\\nBenchmarking TARGET ({cfg.num_iterations} iterations)...\")\n",
    "target_times = []\n",
    "for i in range(cfg.num_iterations):\n",
    "    result, dt = timed_cuda(run_target_inference)\n",
    "    target_times.append(dt)\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Progress: {i+1}/{cfg.num_iterations}, last: {dt*1000:.1f}ms\")\n",
    "\n",
    "# Benchmark DRAFT\n",
    "print(f\"\\nBenchmarking DRAFT ({cfg.num_iterations} iterations)...\")\n",
    "draft_times = []\n",
    "for i in range(cfg.num_iterations):\n",
    "    result, dt = timed_cuda(run_draft_inference)\n",
    "    draft_times.append(dt)\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Progress: {i+1}/{cfg.num_iterations}, last: {dt*1000:.1f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load vla_speculative_decoding.py\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "# ============================================================================\n",
    "# Cache utilities for KV cache pruning\n",
    "# ============================================================================\n",
    "def prune_cache(\n",
    "    cache: Union[Tuple[Tuple[torch.Tensor, torch.Tensor]], DynamicCache, None],\n",
    "    num_tokens_to_discard: int,\n",
    ") -> Union[Tuple[Tuple[torch.Tensor, torch.Tensor]], DynamicCache, None]:\n",
    "    \"\"\"Prune the KV cache by removing tokens from the end.\"\"\"\n",
    "    if cache is None or num_tokens_to_discard <= 0:\n",
    "        return cache\n",
    "    \n",
    "    if isinstance(cache, DynamicCache):\n",
    "        for layer in range(len(cache)):\n",
    "            cache.key_cache[layer] = cache.key_cache[layer][:, :, :-num_tokens_to_discard, :]\n",
    "            cache.value_cache[layer] = cache.value_cache[layer][:, :, :-num_tokens_to_discard, :]\n",
    "        cache._seen_tokens -= num_tokens_to_discard\n",
    "        return cache\n",
    "    \n",
    "    elif isinstance(cache, tuple):\n",
    "        new_cache = []\n",
    "        for layer_cache in cache:\n",
    "            if layer_cache is None:\n",
    "                new_cache.append(None)\n",
    "                continue\n",
    "            layer = []\n",
    "            for tensor in layer_cache:\n",
    "                new_tensor = tensor[:, :, :-num_tokens_to_discard, :]\n",
    "                layer.append(new_tensor)\n",
    "            new_cache.append(tuple(layer))\n",
    "        return tuple(new_cache)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported cache type: {type(cache)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Image preprocessing utilities\n",
    "# ============================================================================\n",
    "def apply_center_crop(im: np.ndarray, t_h: int, t_w: int) -> np.ndarray:\n",
    "    \"\"\"Center crop an image to target dimensions.\"\"\"\n",
    "    assert im.shape[-3] >= t_h and im.shape[-2] >= t_w\n",
    "    crop_h = int((im.shape[-3] - t_h) / 2)\n",
    "    crop_w = int((im.shape[-2] - t_w) / 2)\n",
    "    return im[..., crop_h : crop_h + t_h, crop_w : crop_w + t_w, :]\n",
    "\n",
    "def prepare_image(full_image: Union[np.ndarray, List[np.ndarray]], center_crop: bool = False) -> Image.Image:\n",
    "    \"\"\"Convert numpy image to PIL Image with optional center crop.\"\"\"\n",
    "    if isinstance(full_image, list):\n",
    "        full_image = full_image[0]\n",
    "    \n",
    "    image = Image.fromarray(full_image).convert(\"RGB\")\n",
    "    \n",
    "    if center_crop:\n",
    "        temp_image = np.array(image)\n",
    "        crop_scale = 0.9\n",
    "        sqrt_crop_scale = math.sqrt(crop_scale)\n",
    "        temp_image_cropped = apply_center_crop(\n",
    "            temp_image,\n",
    "            t_h=int(sqrt_crop_scale * temp_image.shape[0]),\n",
    "            t_w=int(sqrt_crop_scale * temp_image.shape[1]),\n",
    "        )\n",
    "        image = Image.fromarray(temp_image_cropped)\n",
    "        image = image.resize((224, 224), Image.Resampling.BILINEAR)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# ============================================================================\n",
    "# Speculative decoding core implementation\n",
    "# ============================================================================\n",
    "@dataclass\n",
    "class SpeculativeDecodingStats:\n",
    "    \"\"\"Statistics from speculative decoding run.\"\"\"\n",
    "    total_tokens_generated: int = 0\n",
    "    total_draft_tokens_proposed: int = 0\n",
    "    total_draft_tokens_accepted: int = 0\n",
    "    total_target_forward_passes: int = 0\n",
    "    total_draft_forward_passes: int = 0\n",
    "    \n",
    "    @property\n",
    "    def acceptance_rate(self) -> float:\n",
    "        if self.total_draft_tokens_proposed == 0:\n",
    "            return 0.0\n",
    "        return self.total_draft_tokens_accepted / self.total_draft_tokens_proposed\n",
    "    \n",
    "    @property\n",
    "    def tokens_per_target_forward(self) -> float:\n",
    "        if self.total_target_forward_passes == 0:\n",
    "            return 0.0\n",
    "        return self.total_tokens_generated / self.total_target_forward_passes\n",
    "\n",
    "def max_fn(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Normalize max(0, x) to create a valid probability distribution.\"\"\"\n",
    "    x_max = torch.where(x > 0, x, torch.zeros_like(x))\n",
    "    x_max_sum = torch.sum(x_max, dim=-1, keepdim=True)\n",
    "    # Avoid division by zero\n",
    "    return x_max / (x_max_sum + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VLASpeculativeDecoderBatchedLM: Uses language model directly for batched verification\n",
    "# Key insight from SpecVLA: After multimodal prefill, call language_model directly\n",
    "# with inputs_embeds and cached KV, bypassing the restrictive multimodal forward.\n",
    "\n",
    "\n",
    "class VLASpeculativeDecoderBatchedLM:\n",
    "    \"\"\"\n",
    "    Speculative decoding for VLA models with EFFICIENT batched verification.\n",
    "    \n",
    "    Key difference from VLASpeculativeDecoderDDDRKVB:\n",
    "    - Initial prefill: Full multimodal forward (processes image once)\n",
    "    - Verification: Calls language_model DIRECTLY with embeddings + cached KV\n",
    "    \n",
    "    Why 10 forward passes for 7 tokens?\n",
    "    1 prefill\n",
    "    4 rejection rounds * (1 verify + 1 advance) = 8\n",
    "    1 acceptance round * 1 verify = 1\n",
    "    Total = 10\n",
    "    The problem is: after each rejection, we do 2 target forward passes:\n",
    "    Batched verification (wasted - we only needed position 0's logits)\n",
    "    Advance with corrected token (to get logits for next round)\n",
    "    - Early rejection: Skips batched verification if first draft token will definitely be rejected\n",
    "    \n",
    "    This bypasses the Prismatic multimodal forward restriction that only allows\n",
    "    single-token inference with KV cache, enabling true batched verification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model,\n",
    "        draft_model,\n",
    "        target_processor=None,\n",
    "        gamma: int = 4,\n",
    "        temperature: float = 0.0,\n",
    "        n_action_bins: int = 256,\n",
    "        relaxed_acceptance_r: int = 0,\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.target = target_model\n",
    "        self.draft = draft_model\n",
    "        self.target_processor = target_processor\n",
    "        self.gamma = gamma\n",
    "        self.temperature = temperature\n",
    "        self.n_action_bins = n_action_bins\n",
    "        self.relaxed_acceptance_r = relaxed_acceptance_r\n",
    "        \n",
    "        self.device = next(target_model.parameters()).device\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "        self._setup_token_mapping()\n",
    "        \n",
    "    \n",
    "    def _setup_token_mapping(self):\n",
    "        \"\"\"Setup token mapping between draft and target vocabularies.\"\"\"\n",
    "        # Target model\n",
    "        if hasattr(self.target, 'language_model') and hasattr(self.target.language_model, 'model'):\n",
    "            self.target_logit_dim = self.target.language_model.model.embed_tokens.weight.shape[0]\n",
    "        elif hasattr(self.target, 'get_output_embeddings'):\n",
    "            self.target_logit_dim = self.target.get_output_embeddings().weight.shape[0]\n",
    "        else:\n",
    "            self.target_logit_dim = self.target.config.vocab_size\n",
    "        \n",
    "        if hasattr(self.target, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.vocab_size\n",
    "        elif hasattr(self.target, 'config') and hasattr(self.target.config, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.config.vocab_size\n",
    "        else:\n",
    "            self.target_vocab_size = self.target_logit_dim\n",
    "        \n",
    "        # Draft model\n",
    "        if hasattr(self.draft, 'llm_backbone'):\n",
    "            draft_tokenizer = self.draft.llm_backbone.tokenizer\n",
    "            self.draft_vocab_size = len(draft_tokenizer) if hasattr(draft_tokenizer, '__len__') else draft_tokenizer.vocab_size\n",
    "            if hasattr(self.draft.llm_backbone, 'llm') and hasattr(self.draft.llm_backbone.llm, 'lm_head'):\n",
    "                self.draft_logit_dim = self.draft.llm_backbone.llm.lm_head.weight.shape[0]\n",
    "            else:\n",
    "                self.draft_logit_dim = self.draft_vocab_size\n",
    "        else:\n",
    "            self.draft_vocab_size = self.draft.config.vocab_size\n",
    "            self.draft_logit_dim = self.draft_vocab_size\n",
    "        \n",
    "        self.vocab_compatible = (self.target_logit_dim == self.draft_logit_dim)\n",
    "        \n",
    "        # Action token ranges\n",
    "        self.target_action_start = self.target_vocab_size - self.n_action_bins\n",
    "        self.draft_action_start = self.draft_vocab_size - self.n_action_bins\n",
    "        \n",
    "        if self.verbose: print(\"\\033[38;2;255;165;0m[BatchedLM] -> \\033[0m\", f\"Target vocab_size: {self.target_vocab_size}, logit_dim: {self.target_logit_dim}\")\n",
    "        if self.verbose: print(\"\\033[38;2;255;165;0m[BatchedLM] -> \\033[0m\", f\"Draft vocab_size: {self.draft_vocab_size}, logit_dim: {self.draft_logit_dim}\")\n",
    "        if self.verbose: print(\"\\033[38;2;255;165;0m[BatchedLM] -> \\033[0m\", f\"Vocabularies compatible: {self.vocab_compatible}\")\n",
    "        \n",
    "    def _draft_token_to_target(self, draft_token_id: int) -> int:\n",
    "        if self.vocab_compatible:\n",
    "            return draft_token_id\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            action_bin = draft_token_id - self.draft_action_start\n",
    "            return self.target_action_start + action_bin\n",
    "        return min(draft_token_id, self.target_vocab_size - 1)\n",
    "    \n",
    "    def _target_token_to_draft(self, target_token_id: int) -> int:\n",
    "        if self.vocab_compatible:\n",
    "            return target_token_id\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            action_bin = target_token_id - self.target_action_start\n",
    "            return self.draft_action_start + action_bin\n",
    "        return min(target_token_id, self.draft_vocab_size - 1)\n",
    "\n",
    "    def _remap_logits_draft_to_target(self, draft_logits: torch.Tensor, target_logit_dim: int = None) -> torch.Tensor:\n",
    "        if self.vocab_compatible:\n",
    "            return draft_logits\n",
    "        if target_logit_dim is None:\n",
    "            target_logit_dim = self.target_logit_dim\n",
    "        target_logits = torch.full(\n",
    "            (draft_logits.shape[0], target_logit_dim), float('-inf'),\n",
    "            device=draft_logits.device, dtype=draft_logits.dtype\n",
    "        )\n",
    "        draft_action_logits = draft_logits[:, self.draft_action_start:self.draft_vocab_size]\n",
    "        target_logits[:, self.target_action_start:self.target_vocab_size] = draft_action_logits\n",
    "        return target_logits\n",
    "    \n",
    "    def _get_action_bin_from_draft_token(self, draft_token_id: int) -> int:\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            return draft_token_id - self.draft_action_start\n",
    "        return -1\n",
    "    \n",
    "    def _get_action_bin_from_target_token(self, target_token_id: int) -> int:\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            return target_token_id - self.target_action_start\n",
    "        return -1\n",
    "    \n",
    "    def _get_continuous_action_from_bin(self, bin_idx: int) -> float:\n",
    "        if 0 <= bin_idx < len(self.target.bin_centers):\n",
    "            return self.target.bin_centers[bin_idx]\n",
    "        return float('nan')\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "        \n",
    "    def _sample_token(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        if self.temperature <= 0:\n",
    "            return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        probs = F.softmax(logits / self.temperature, dim=-1)\n",
    "        return torch.multinomial(probs.squeeze(0), num_samples=1).unsqueeze(0)\n",
    "    \n",
    "    def _get_probs(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        if self.temperature <= 0:\n",
    "            return F.softmax(logits / 0.01, dim=-1)\n",
    "        return F.softmax(logits / self.temperature, dim=-1)\n",
    "\n",
    "    def _prepare_target_inputs(self, image: Image.Image, instruction: str) -> Dict[str, torch.Tensor]:\n",
    "        prompt = f\"In: What action should the robot take to {instruction.lower()}?\\nOut:\"\n",
    "        inputs = self.target_processor(prompt, image).to(self.device, dtype=torch.bfloat16)\n",
    "        # Add special token 29871 if not present\n",
    "        if not torch.all(inputs[\"input_ids\"][:, -1] == 29871):\n",
    "            inputs[\"input_ids\"] = torch.cat(\n",
    "                (inputs[\"input_ids\"], torch.tensor([[29871]], device=self.device)), dim=1\n",
    "            )\n",
    "            if \"attention_mask\" in inputs:\n",
    "                inputs[\"attention_mask\"] = torch.cat(\n",
    "                    (inputs[\"attention_mask\"], torch.ones((1, 1), device=self.device, dtype=inputs[\"attention_mask\"].dtype)), dim=1\n",
    "                )\n",
    "        return inputs\n",
    "    \n",
    "    def _prepare_draft_inputs(self, image: Image.Image, instruction: str) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        prompt_builder = self.draft.get_prompt_builder()\n",
    "        prompt_builder.add_turn(role=\"human\", message=f\"What action should the robot take to {instruction.lower()}?\")\n",
    "        prompt_text = prompt_builder.get_prompt()\n",
    "        \n",
    "        tokenizer = self.draft.llm_backbone.tokenizer\n",
    "        input_ids = tokenizer(prompt_text, truncation=True, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        \n",
    "        from transformers import LlamaTokenizerFast\n",
    "        if isinstance(tokenizer, LlamaTokenizerFast):\n",
    "            if not torch.all(input_ids[:, -1] == 29871):\n",
    "                input_ids = torch.cat((input_ids, torch.tensor([[29871]], device=self.device)), dim=1)\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        image_transform = self.draft.vision_backbone.get_image_transform()\n",
    "        pixel_values = image_transform(image)\n",
    "        if isinstance(pixel_values, torch.Tensor):\n",
    "            pixel_values = pixel_values[None, ...].to(self.device)\n",
    "        elif isinstance(pixel_values, dict):\n",
    "            pixel_values = {k: v[None, ...].to(self.device) for k, v in pixel_values.items()}\n",
    "        \n",
    "        return input_ids, attention_mask, pixel_values\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict_action_speculative(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "        unnorm_key_target: str,\n",
    "    ) -> Tuple[np.ndarray, SpeculativeDecodingStats]:\n",
    "        \"\"\"\n",
    "        Generate action using speculative decoding with efficient batched verification.\n",
    "        \n",
    "        The key innovation: After multimodal prefill, we call the language model DIRECTLY\n",
    "        with embeddings and cached KV, bypassing the restrictive multimodal forward.\n",
    "        \"\"\"\n",
    "        call_stats = SpeculativeDecodingStats()\n",
    "        action_dim = self.target.get_action_dim(unnorm_key_target)\n",
    "        \n",
    "        # Prepare inputs\n",
    "        target_inputs = self._prepare_target_inputs(image, instruction)\n",
    "        draft_input_ids, draft_attention_mask, draft_pixel_values = self._prepare_draft_inputs(image, instruction)\n",
    "        autocast_dtype = self.draft.llm_backbone.half_precision_dtype\n",
    "        \n",
    "        generated_token_ids = []\n",
    "        \n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16, enabled=True):\n",
    "            # === PHASE 1: Multimodal Prefill (processes image ONCE) ===\n",
    "            if self.verbose: print(\"\\033[38;2;100;200;255m[BatchedLM] Phase 1: Multimodal Prefill\\033[0m\")\n",
    "            \n",
    "            # Target prefill - get KV cache with image embeddings\n",
    "            target_out = self.target(\n",
    "                **target_inputs,\n",
    "                past_key_values=None,\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "            target_cache = target_out.past_key_values\n",
    "            target_logits = target_out.logits[:, -1, :]\n",
    "            call_stats.total_target_forward_passes += 1\n",
    "            \n",
    "            # Get the sequence length after multimodal prefill (includes patch embeddings)\n",
    "            kv_seq_len = target_cache[0][0].shape[2]  # [batch, heads, seq_len, head_dim]\n",
    "            if self.verbose: print(f\"  Target KV cache seq_len after prefill: {kv_seq_len}\")\n",
    "            \n",
    "            # Draft prefill\n",
    "            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                draft_out = self.draft(\n",
    "                    input_ids=draft_input_ids,\n",
    "                    attention_mask=draft_attention_mask,\n",
    "                    pixel_values=draft_pixel_values,\n",
    "                    past_key_values=None,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "            draft_cache = draft_out.past_key_values\n",
    "            draft_logits = draft_out.logits[:, -1, :]\n",
    "            call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            # Get embed_tokens function for converting tokens to embeddings\n",
    "            embed_tokens = self.target.language_model.model.embed_tokens\n",
    "            \n",
    "            # === PHASE 2: Speculative Decoding Loop ===\n",
    "            if self.verbose: print(\"\\033[38;2;100;200;255m[BatchedLM] Phase 2: Speculative Decoding\\033[0m\")\n",
    "            \n",
    "            while len(generated_token_ids) < action_dim:\n",
    "                gamma = min(self.gamma, action_dim - len(generated_token_ids))\n",
    "                \n",
    "                # Generate FIRST draft token to check for early rejection\n",
    "                draft_probs_first = self._get_probs(draft_logits)\n",
    "                draft_token_first = self._sample_token(draft_logits)\n",
    "                draft_token_first_target = self._draft_token_to_target(draft_token_first.item())\n",
    "                \n",
    "                # EARLY REJECTION CHECK: Can we determine rejection without batched verification?\n",
    "                # For greedy decoding (temperature=0), we can check bin distance directly\n",
    "                target_preferred = torch.argmax(target_logits, dim=-1).item()\n",
    "                first_draft_bin = self._get_action_bin_from_target_token(draft_token_first_target)\n",
    "                target_preferred_bin = self._get_action_bin_from_target_token(target_preferred)\n",
    "                first_bin_distance = abs(first_draft_bin - target_preferred_bin) if first_draft_bin >= 0 and target_preferred_bin >= 0 else float('inf')\n",
    "                \n",
    "                # Check if first token would be accepted (relaxed or standard greedy)\n",
    "                p_target_first = self._get_probs(target_logits)[0, draft_token_first_target].item()\n",
    "                p_draft_first = draft_probs_first[0, draft_token_first_target].item()\n",
    "                acceptance_prob_first = min(1.0, p_target_first / p_draft_first) if p_draft_first > 0 else (1.0 if p_target_first > 0 else 0.0)\n",
    "                \n",
    "                first_relaxed_accept = (self.relaxed_acceptance_r > 0 and first_bin_distance <= self.relaxed_acceptance_r)\n",
    "                first_standard_accept = (torch.rand(1).item() < acceptance_prob_first)\n",
    "                first_will_accept = first_relaxed_accept or first_standard_accept\n",
    "                \n",
    "                if not first_will_accept:\n",
    "                    # EARLY REJECTION: Skip batched verification entirely!\n",
    "                    # We already have target_logits, so just sample corrected token from it\n",
    "                    call_stats.total_draft_tokens_proposed += 1\n",
    "                    call_stats.total_draft_forward_passes += 1  # We did generate first draft token\n",
    "                    \n",
    "                    # Sample corrected token from target's distribution\n",
    "                    draft_probs_first_remapped = self._get_probs(self._remap_logits_draft_to_target(draft_probs_first, target_logits.shape[-1]))\n",
    "                    target_probs = self._get_probs(target_logits)\n",
    "                    adjusted_probs = max_fn(target_probs - draft_probs_first_remapped)\n",
    "                    if adjusted_probs.sum() > 0:\n",
    "                        corrected_token = torch.multinomial(adjusted_probs, num_samples=1).item()\n",
    "                    else:\n",
    "                        corrected_token = target_preferred  # Use argmax\n",
    "                    \n",
    "                    corrected_bin = self._get_action_bin_from_target_token(corrected_token)\n",
    "                    if self.verbose: print(f\"\\033[38;2;255;200;100m[EARLY REJECT]\\033[0m draft_bin={first_draft_bin}, target_bin={target_preferred_bin}, corrected_bin={corrected_bin} (saved verify pass!)\")\n",
    "                    \n",
    "                    generated_token_ids.append(corrected_token)\n",
    "                    call_stats.total_tokens_generated += 1\n",
    "                    \n",
    "                    # Advance target with corrected token (1 forward pass - this is necessary)\n",
    "                    if len(generated_token_ids) < action_dim:\n",
    "                        corrected_embeds = embed_tokens(torch.tensor([[corrected_token]], device=self.device))\n",
    "                        corrected_pos = torch.tensor([[target_cache[0][0].shape[2]]], device=self.device)\n",
    "                        lm_step = self.target.language_model(\n",
    "                            inputs_embeds=corrected_embeds,\n",
    "                            past_key_values=target_cache,\n",
    "                            position_ids=corrected_pos,\n",
    "                            use_cache=True,\n",
    "                        )\n",
    "                        target_cache = lm_step.past_key_values\n",
    "                        target_logits = lm_step.logits[:, -1, :]\n",
    "                        call_stats.total_target_forward_passes += 1\n",
    "                        \n",
    "                        # Update draft with corrected token (in draft vocab)\n",
    "                        corrected_draft = self._target_token_to_draft(corrected_token)\n",
    "                        with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                            draft_step = self.draft(\n",
    "                                input_ids=torch.tensor([[corrected_draft]], device=self.device),\n",
    "                                past_key_values=draft_cache,\n",
    "                                use_cache=True,\n",
    "                            )\n",
    "                        draft_cache = draft_step.past_key_values\n",
    "                        draft_logits = draft_step.logits[:, -1, :]\n",
    "                        call_stats.total_draft_forward_passes += 1\n",
    "                    \n",
    "                    continue  # Next iteration\n",
    "                \n",
    "                # First token looks good - generate remaining draft tokens\n",
    "                draft_tokens = [draft_token_first]\n",
    "                draft_probs_list = [draft_probs_first]\n",
    "                current_draft_cache = draft_cache\n",
    "                \n",
    "                # Advance draft with first token\n",
    "                with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                    draft_step = self.draft(\n",
    "                        input_ids=draft_token_first.to(self.device),\n",
    "                        past_key_values=current_draft_cache,\n",
    "                        use_cache=True,\n",
    "                    )\n",
    "                current_draft_cache = draft_step.past_key_values\n",
    "                current_draft_logits = draft_step.logits[:, -1, :]\n",
    "                call_stats.total_draft_forward_passes += 1\n",
    "                \n",
    "                # Generate remaining gamma-1 draft tokens\n",
    "                for _ in range(gamma - 1):\n",
    "                    draft_probs = self._get_probs(current_draft_logits)\n",
    "                    draft_token = self._sample_token(current_draft_logits)\n",
    "                    draft_tokens.append(draft_token)\n",
    "                    draft_probs_list.append(draft_probs)\n",
    "                    \n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=draft_token.to(self.device),\n",
    "                            past_key_values=current_draft_cache,\n",
    "                            use_cache=True,\n",
    "                        )\n",
    "                    current_draft_cache = draft_step.past_key_values\n",
    "                    current_draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                \n",
    "                call_stats.total_draft_tokens_proposed += gamma\n",
    "                \n",
    "                # Map draft tokens to target vocab\n",
    "                draft_token_ids_target = [self._draft_token_to_target(dt.item()) for dt in draft_tokens]\n",
    "                \n",
    "                # === BATCHED VERIFICATION via Language Model ===\n",
    "                draft_tokens_tensor = torch.tensor([draft_token_ids_target], device=self.device)\n",
    "                draft_embeds = embed_tokens(draft_tokens_tensor)\n",
    "                \n",
    "                current_kv_len = target_cache[0][0].shape[2]\n",
    "                position_ids = torch.arange(current_kv_len, current_kv_len + gamma, device=self.device).unsqueeze(0)\n",
    "                \n",
    "                lm_out = self.target.language_model(\n",
    "                    inputs_embeds=draft_embeds,\n",
    "                    past_key_values=target_cache,\n",
    "                    position_ids=position_ids,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "                call_stats.total_target_forward_passes += 1\n",
    "                \n",
    "                target_logits_batch = lm_out.logits\n",
    "                new_target_cache = lm_out.past_key_values\n",
    "                \n",
    "                # eval_logits[i] evaluates draft_token[i]\n",
    "                eval_logits = torch.cat([target_logits.unsqueeze(1), target_logits_batch[:, :-1, :]], dim=1)\n",
    "                target_probs_batch = self._get_probs(eval_logits)\n",
    "                last_target_logits = target_logits_batch[:, -1, :]\n",
    "                \n",
    "                if self.verbose: print(f\"\\033[38;2;100;200;255m[BatchedLM] Batched verification: 1 forward pass for {gamma} tokens\\033[0m\")\n",
    "                \n",
    "                actual_target_logit_dim = target_probs_batch.shape[-1]\n",
    "                draft_probs_remapped = [self._get_probs(self._remap_logits_draft_to_target(dp, actual_target_logit_dim)) for dp in draft_probs_list]\n",
    "                \n",
    "                # Rejection sampling (first token already checked, but re-verify for consistency)\n",
    "                n_accepted = 0\n",
    "                \n",
    "                for i in range(gamma):\n",
    "                    draft_token_id_target = draft_token_ids_target[i]\n",
    "                    draft_prob_remapped = draft_probs_remapped[i]\n",
    "                    target_prob = target_probs_batch[:, i, :]\n",
    "                    \n",
    "                    p_target = target_prob[0, draft_token_id_target].item()\n",
    "                    p_draft = draft_prob_remapped[0, draft_token_id_target].item()\n",
    "                    \n",
    "                    target_preferred = torch.argmax(target_prob, dim=-1).item()\n",
    "                    draft_bin = self._get_action_bin_from_target_token(draft_token_id_target)\n",
    "                    target_bin = self._get_action_bin_from_target_token(target_preferred)\n",
    "                    \n",
    "                    bin_distance = abs(draft_bin - target_bin) if draft_bin >= 0 and target_bin >= 0 else float('inf')\n",
    "                    relaxed_accept = (self.relaxed_acceptance_r > 0 and bin_distance <= self.relaxed_acceptance_r)\n",
    "                    \n",
    "                    acceptance_prob = min(1.0, p_target / p_draft) if p_draft > 0 else (1.0 if p_target > 0 else 0.0)\n",
    "                    standard_accept = (torch.rand(1).item() < acceptance_prob)\n",
    "                    \n",
    "                    if relaxed_accept or standard_accept:\n",
    "                        generated_token_ids.append(draft_token_id_target)\n",
    "                        n_accepted += 1\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        call_stats.total_draft_tokens_accepted += 1\n",
    "                        \n",
    "                        accept_reason = \"RELAXED\" if relaxed_accept and not standard_accept else \"STANDARD\"\n",
    "                        if self.verbose: print(f\"\\033[38;2;0;255;0m[ACCEPT-{accept_reason}]\\033[0m [{i}] bin={draft_bin}, target_bin={target_bin}, dist={bin_distance}\")\n",
    "                        \n",
    "                        if len(generated_token_ids) >= action_dim:\n",
    "                            break\n",
    "                    else:\n",
    "                        adjusted_probs = max_fn(target_prob - draft_prob_remapped)\n",
    "                        corrected_token = torch.multinomial(adjusted_probs, num_samples=1).item() if adjusted_probs.sum() > 0 else target_preferred\n",
    "                        \n",
    "                        corrected_bin = self._get_action_bin_from_target_token(corrected_token)\n",
    "                        if self.verbose: print(f\"\\033[38;2;255;100;100m[REJECT]\\033[0m [{i}] draft_bin={draft_bin}, target_bin={target_bin}, corrected_bin={corrected_bin}\")\n",
    "                        \n",
    "                        generated_token_ids.append(corrected_token)\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        break\n",
    "                \n",
    "                # Update caches based on acceptance\n",
    "                if n_accepted == gamma and len(generated_token_ids) < action_dim:\n",
    "                    # All accepted - sample bonus token\n",
    "                    target_cache = new_target_cache\n",
    "                    target_logits = last_target_logits\n",
    "                    \n",
    "                    bonus_token = self._sample_token(target_logits).item()\n",
    "                    bonus_bin = self._get_action_bin_from_target_token(bonus_token)\n",
    "                    if self.verbose: print(f\"\\033[38;2;0;255;0m[ALL ACCEPTED]\\033[0m Bonus token: bin={bonus_bin}\")\n",
    "                    generated_token_ids.append(bonus_token)\n",
    "                    call_stats.total_tokens_generated += 1\n",
    "                    \n",
    "                    # Advance caches\n",
    "                    bonus_embeds = embed_tokens(torch.tensor([[bonus_token]], device=self.device))\n",
    "                    bonus_pos = torch.tensor([[target_cache[0][0].shape[2]]], device=self.device)\n",
    "                    lm_step = self.target.language_model(\n",
    "                        inputs_embeds=bonus_embeds,\n",
    "                        past_key_values=target_cache,\n",
    "                        position_ids=bonus_pos,\n",
    "                        use_cache=True,\n",
    "                    )\n",
    "                    target_cache = lm_step.past_key_values\n",
    "                    target_logits = lm_step.logits[:, -1, :]\n",
    "                    call_stats.total_target_forward_passes += 1\n",
    "                    \n",
    "                    draft_cache = current_draft_cache\n",
    "                    bonus_draft = self._target_token_to_draft(bonus_token)\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=torch.tensor([[bonus_draft]], device=self.device),\n",
    "                            past_key_values=draft_cache,\n",
    "                            use_cache=True,\n",
    "                        )\n",
    "                    draft_cache = draft_step.past_key_values\n",
    "                    draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                    \n",
    "                elif len(generated_token_ids) < action_dim:\n",
    "                    # Some rejected - update caches\n",
    "                    if n_accepted > 0:\n",
    "                        tokens_to_discard = gamma - n_accepted\n",
    "                        target_cache = prune_cache(new_target_cache, tokens_to_discard) if tokens_to_discard > 0 else new_target_cache\n",
    "                    \n",
    "                    last_token = generated_token_ids[-1]\n",
    "                    last_embeds = embed_tokens(torch.tensor([[last_token]], device=self.device))\n",
    "                    last_pos = torch.tensor([[target_cache[0][0].shape[2]]], device=self.device)\n",
    "                    \n",
    "                    lm_step = self.target.language_model(\n",
    "                        inputs_embeds=last_embeds,\n",
    "                        past_key_values=target_cache,\n",
    "                        position_ids=last_pos,\n",
    "                        use_cache=True,\n",
    "                    )\n",
    "                    target_cache = lm_step.past_key_values\n",
    "                    target_logits = lm_step.logits[:, -1, :]\n",
    "                    call_stats.total_target_forward_passes += 1\n",
    "                    \n",
    "                    draft_cache = prune_cache(current_draft_cache, gamma - n_accepted)\n",
    "                    last_draft = self._target_token_to_draft(last_token)\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=torch.tensor([[last_draft]], device=self.device),\n",
    "                            past_key_values=draft_cache,\n",
    "                            use_cache=True,\n",
    "                        )\n",
    "                    draft_cache = draft_step.past_key_values\n",
    "                    draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            if self.verbose: print(f\"\\033[38;2;255;165;0m[BatchedLM] -> \\033[0m Stats: {call_stats}\")\n",
    "        \n",
    "        # Decode tokens to actions\n",
    "        predicted_action_token_ids = np.array(generated_token_ids[:action_dim], dtype=np.int64)\n",
    "        vocab_size = self.target.vocab_size\n",
    "        discretized_actions = vocab_size - predicted_action_token_ids\n",
    "        discretized_actions = np.clip(discretized_actions - 1, a_min=0, a_max=self.target.bin_centers.shape[0] - 1)\n",
    "        normalized_actions = self.target.bin_centers[discretized_actions]\n",
    "        \n",
    "        # Un-normalize\n",
    "        action_norm_stats = self.target.get_action_stats(unnorm_key_target)\n",
    "        mask = action_norm_stats.get(\"mask\", np.ones_like(action_norm_stats[\"q01\"], dtype=bool))\n",
    "        action_high, action_low = np.array(action_norm_stats[\"q99\"]), np.array(action_norm_stats[\"q01\"])\n",
    "        actions = np.where(\n",
    "            mask,\n",
    "            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,\n",
    "            normalized_actions,\n",
    "        )\n",
    "        \n",
    "        # Update global stats\n",
    "        self.stats.total_tokens_generated += call_stats.total_tokens_generated\n",
    "        self.stats.total_draft_tokens_proposed += call_stats.total_draft_tokens_proposed\n",
    "        self.stats.total_draft_tokens_accepted += call_stats.total_draft_tokens_accepted\n",
    "        self.stats.total_target_forward_passes += call_stats.total_target_forward_passes\n",
    "        self.stats.total_draft_forward_passes += call_stats.total_draft_forward_passes\n",
    "        \n",
    "        return actions, call_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/4] Creating speculative decoder...\n",
      "  Warming up SPECDEC...\n",
      "\n",
      "Benchmarking SPECDEC (10 iterations)...\n",
      "  Progress: 10/10, last: 493.4ms, accept: 42.86%\n",
      "\n",
      "================================================================================\n",
      "RESULTS\n",
      "================================================================================\n",
      "\n",
      "TARGET:\n",
      "  Mean time: 297.06  26.32 ms\n",
      "  Throughput: 3.37 Hz\n",
      "\n",
      "DRAFT:\n",
      "  Mean time: 155.88  37.05 ms\n",
      "  Throughput: 6.42 Hz\n",
      "\n",
      "SPECULATIVE DECODING (gamma=7, r=7):\n",
      "  Mean time: 420.17  85.65 ms\n",
      "  Throughput: 2.38 Hz\n",
      "  Acceptance rate: 40.26%  3.18%\n",
      "  Tokens/target forward: 1.17\n",
      "\n",
      "SPEEDUPS:\n",
      "  SpecDec vs Target: 0.71x\n",
      "  Draft vs Target: 1.91x\n",
      "\n",
      "OVERALL SPECDEC STATS:\n",
      "  Total tokens generated: 70\n",
      "  Total draft tokens proposed: 86\n",
      "  Total draft tokens accepted: 34\n",
      "  Overall acceptance rate: 39.53%\n",
      "  Total target forward passes: 60\n",
      "  Total draft forward passes: 132\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# from experiments.specdec.vla_speculative_decoding import (\n",
    "#     VLASpeculativeDecoder,\n",
    "#     prepare_image,\n",
    "# )\n",
    "# FIXME: when temperature=0.0, current implementation uses 0.01...\n",
    "cfg.gamma = 7\n",
    "\n",
    "# Create speculative decoder\n",
    "print(\"\\n[3/4] Creating speculative decoder...\")\n",
    "\n",
    "specdec_decoder = VLASpeculativeDecoderBatchedLM(\n",
    "    target_model=target_model,\n",
    "    draft_model=draft_model,\n",
    "    target_processor=target_processor,\n",
    "    gamma=cfg.gamma,\n",
    "    temperature=cfg.temperature,\n",
    "    relaxed_acceptance_r=cfg.relaxed_acceptance_r,  # Relaxed acceptance with r=7 bins\n",
    ")\n",
    "\n",
    "# Prepare PIL image for specdec\n",
    "pil_image = prepare_image(observation[\"full_image\"], center_crop=cfg.center_crop)\n",
    "\n",
    "def run_specdec_inference():\n",
    "    action, stats = specdec_decoder.predict_action_speculative(\n",
    "        pil_image, task_description, unnorm_key_target\n",
    "    )\n",
    "    return action, stats\n",
    "\n",
    "print(\"  Warming up SPECDEC...\")\n",
    "for _ in range(cfg.warmup_iterations):\n",
    "    run_specdec_inference()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Benchmark SPECDEC\n",
    "print(f\"\\nBenchmarking SPECDEC ({cfg.num_iterations} iterations)...\")\n",
    "specdec_times = []\n",
    "specdec_acceptance_rates = []\n",
    "specdec_tokens_per_forward = []\n",
    "specdec_decoder.reset_stats()\n",
    "\n",
    "for i in range(cfg.num_iterations):\n",
    "    (action, stats), dt = timed_cuda(run_specdec_inference)\n",
    "    specdec_times.append(dt)\n",
    "    specdec_acceptance_rates.append(stats.acceptance_rate)\n",
    "    specdec_tokens_per_forward.append(stats.tokens_per_target_forward)\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Progress: {i+1}/{cfg.num_iterations}, last: {dt*1000:.1f}ms, \"\n",
    "                f\"accept: {stats.acceptance_rate:.2%}\")\n",
    "\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def compute_statistics():\n",
    "    # Compute statistics\n",
    "    target_mean = np.mean(target_times)\n",
    "    target_std = np.std(target_times)\n",
    "    draft_mean = np.mean(draft_times)\n",
    "    draft_std = np.std(draft_times)\n",
    "    specdec_mean = np.mean(specdec_times)\n",
    "    specdec_std = np.std(specdec_times)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"\\nTARGET:\")\n",
    "    print(f\"  Mean time: {target_mean*1000:.2f}  {target_std*1000:.2f} ms\")\n",
    "    print(f\"  Throughput: {1/target_mean:.2f} Hz\")\n",
    "\n",
    "    print(f\"\\nDRAFT:\")\n",
    "    print(f\"  Mean time: {draft_mean*1000:.2f}  {draft_std*1000:.2f} ms\")\n",
    "    print(f\"  Throughput: {1/draft_mean:.2f} Hz\")\n",
    "\n",
    "    print(f\"\\nSPECULATIVE DECODING (gamma={cfg.gamma}, r={cfg.relaxed_acceptance_r}):\")\n",
    "    print(f\"  Mean time: {specdec_mean*1000:.2f}  {specdec_std*1000:.2f} ms\")\n",
    "    print(f\"  Throughput: {1/specdec_mean:.2f} Hz\")\n",
    "    print(f\"  Acceptance rate: {np.mean(specdec_acceptance_rates):.2%}  {np.std(specdec_acceptance_rates):.2%}\")\n",
    "    print(f\"  Tokens/target forward: {np.mean(specdec_tokens_per_forward):.2f}\")\n",
    "\n",
    "    print(f\"\\nSPEEDUPS:\")\n",
    "    print(f\"  SpecDec vs Target: {target_mean/specdec_mean:.2f}x\")\n",
    "    print(f\"  Draft vs Target: {target_mean/draft_mean:.2f}x\")\n",
    "\n",
    "    # Overall stats from decoder\n",
    "    global_stats = specdec_decoder.stats\n",
    "    print(f\"\\nOVERALL SPECDEC STATS:\")\n",
    "    print(f\"  Total tokens generated: {global_stats.total_tokens_generated}\")\n",
    "    print(f\"  Total draft tokens proposed: {global_stats.total_draft_tokens_proposed}\")\n",
    "    print(f\"  Total draft tokens accepted: {global_stats.total_draft_tokens_accepted}\")\n",
    "    print(f\"  Overall acceptance rate: {global_stats.acceptance_rate:.2%}\")\n",
    "    print(f\"  Total target forward passes: {global_stats.total_target_forward_passes}\")\n",
    "    print(f\"  Total draft forward passes: {global_stats.total_draft_forward_passes}\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "compute_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking with relaxation threshold r=0\n",
      "  Acceptance rate: 25.00%\n",
      "Benchmarking with relaxation threshold r=5\n",
      "  Acceptance rate: 33.33%\n",
      "Benchmarking with relaxation threshold r=10\n",
      "  Acceptance rate: 38.89%\n",
      "Benchmarking with relaxation threshold r=15\n",
      "  Acceptance rate: 38.89%\n",
      "Benchmarking with relaxation threshold r=20\n",
      "  Acceptance rate: 44.44%\n",
      "Benchmarking with relaxation threshold r=25\n",
      "  Acceptance rate: 60.00%\n",
      "Benchmarking with relaxation threshold r=30\n",
      "  Acceptance rate: 60.00%\n",
      "Benchmarking with relaxation threshold r=45\n",
      "  Acceptance rate: 100.00%\n",
      "\n",
      "================================================================================\n",
      "Computing baseline (no speculation)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Benchmarking with gamma=0\n",
      "================================================================================\n",
      "  Speedup: 1.00x, Acceptance rate: 0.00%\n",
      "\n",
      "================================================================================\n",
      "Benchmarking with gamma=1\n",
      "================================================================================\n",
      "  Speedup: 0.88x, Acceptance rate: 42.86%\n",
      "\n",
      "================================================================================\n",
      "Benchmarking with gamma=2\n",
      "================================================================================\n",
      "  Speedup: 0.90x, Acceptance rate: 45.00%\n",
      "\n",
      "================================================================================\n",
      "Benchmarking with gamma=3\n",
      "================================================================================\n",
      "  Speedup: 1.05x, Acceptance rate: 46.67%\n",
      "\n",
      "================================================================================\n",
      "Benchmarking with gamma=4\n",
      "================================================================================\n",
      "  Speedup: 0.78x, Acceptance rate: 43.75%\n",
      "\n",
      "================================================================================\n",
      "Benchmarking with gamma=5\n",
      "================================================================================\n",
      "  Speedup: 0.94x, Acceptance rate: 40.91%\n",
      "\n",
      "================================================================================\n",
      "Benchmarking with gamma=6\n",
      "================================================================================\n",
      "  Speedup: 0.96x, Acceptance rate: 38.89%\n",
      "\n",
      "================================================================================\n",
      "Benchmarking with gamma=7\n",
      "================================================================================\n",
      "  Speedup: 0.98x, Acceptance rate: 38.89%\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# === Plot 1: Acceptance Rate vs Relaxation Threshold ===\n",
    "# You'll need to run benchmarks with different relaxation thresholds\n",
    "# For demonstration, I'll show how to structure the code\n",
    "\n",
    "relaxation_thresholds = [0, 5, 10, 15, 20, 25, 30, 45]\n",
    "acceptance_rates = []\n",
    "\n",
    "# Run benchmarks for each threshold\n",
    "for r in relaxation_thresholds:\n",
    "    print(f\"Benchmarking with relaxation threshold r={r}\")\n",
    "    \n",
    "    # Create new decoder with this threshold\n",
    "    specdec_decoder_r = VLASpeculativeDecoderBatchedLM(\n",
    "        target_model=target_model,\n",
    "        draft_model=draft_model,\n",
    "        target_processor=target_processor,\n",
    "        gamma=cfg.gamma,\n",
    "        temperature=cfg.temperature,\n",
    "        relaxed_acceptance_r=r,\n",
    "    )\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(cfg.warmup_iterations):\n",
    "        specdec_decoder_r.predict_action_speculative(\n",
    "            pil_image, task_description, unnorm_key_target\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    specdec_decoder_r.reset_stats()\n",
    "    for i in range(cfg.num_iterations):\n",
    "        action, stats = specdec_decoder_r.predict_action_speculative(\n",
    "            pil_image, task_description, unnorm_key_target\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Store acceptance rate\n",
    "    acceptance_rates.append(specdec_decoder_r.stats.acceptance_rate)\n",
    "    print(f\"  Acceptance rate: {specdec_decoder_r.stats.acceptance_rate:.2%}\")\n",
    "\n",
    "# === Plot 2: Speedup vs Gamma ===\n",
    "gamma_values = list(range(0, 8))  # 0 to 7\n",
    "speedups = []\n",
    "acceptance_rates_gamma = []\n",
    "\n",
    "# Compute baseline (gamma=0, no speculation)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Computing baseline (no speculation)\")\n",
    "print(f\"{'='*80}\")\n",
    "target_mean_baseline = np.mean(target_times)\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    print(f\"\\nBenchmarking with gamma={gamma}\")\n",
    "    \n",
    "    if gamma == 0:\n",
    "        # No speculation - just use target model\n",
    "        speedup = 1.0\n",
    "        acceptance_rate = 0.0\n",
    "    else:\n",
    "        # Create decoder with this gamma\n",
    "        specdec_decoder_gamma = VLASpeculativeDecoderBatchedLM(\n",
    "            target_model=target_model,\n",
    "            draft_model=draft_model,\n",
    "            target_processor=target_processor,\n",
    "            gamma=gamma,\n",
    "            temperature=cfg.temperature,\n",
    "            relaxed_acceptance_r=7,  # Use your preferred threshold\n",
    "        )\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(cfg.warmup_iterations):\n",
    "            specdec_decoder_gamma.predict_action_speculative(\n",
    "                pil_image, task_description, unnorm_key_target\n",
    "            )\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        times_gamma = []\n",
    "        specdec_decoder_gamma.reset_stats()\n",
    "        for i in range(cfg.num_iterations):\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            start.record()\n",
    "            action, stats = specdec_decoder_gamma.predict_action_speculative(\n",
    "                pil_image, task_description, unnorm_key_target\n",
    "            )\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            times_gamma.append(start.elapsed_time(end) / 1000)\n",
    "        \n",
    "        mean_time = np.mean(times_gamma)\n",
    "        speedup = target_mean_baseline / mean_time\n",
    "        acceptance_rate = specdec_decoder_gamma.stats.acceptance_rate\n",
    "    \n",
    "    speedups.append(speedup)\n",
    "    acceptance_rates_gamma.append(acceptance_rate)\n",
    "    print(f\"  Speedup: {speedup:.2f}x, Acceptance rate: {acceptance_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAGGCAYAAACNCg6xAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAATDRJREFUeJzt3XlcVPX+P/DXDMsM67DINrKIggLikqiIS6KiZGlws8WWe92qb9elyPvLJfc0Ubt1zbKs203r3izTcr03cwUzFRHDXUDFFVlcYFgHmDm/P4jJke2wDDMDr+fj0YM7n7PM+8xnkNc9y+cjEQRBABERERE1SGrsAoiIiIjMBYMTERERkUgMTkREREQiMTgRERERicTgRERERCQSgxMRERGRSAxORERERCIxOBERERGJxOBEREREJBKDExEREZFIDE5EREREIjE4EVGdVq1ahaCgIGi12hbb54YNGyCRSHD16tUW26chmGKdixcvhkQiwZ07d4xdCgDD1CP2c69+7wetW7cOvr6+UKvVLVYP0cMYnIioViqVCitXrsTs2bMhlbbNfyqOHDmCxYsXIz8/39il6JhiTeZi4sSJKC8vx2effWbsUqgNa5v/GhJRs3355ZeorKzE888/b+xSDObIkSNYsmRJrSHlz3/+M0pLS+Hn52cyNVH95HI5JkyYgA8++ACCIBi7HGqjGJyI2rGysrI6l61fvx5PPvkk5HJ5vfsoLi5u6bJMgoWFBeRyeY3LQeaqrfbTw5599llcu3YNBw8eNHYp1EYxOBG1oE2bNqFPnz6wsbFBcHAw9u3bB0EQ0L17d7z77ru1brNlyxZIJBIkJibWWPbZZ59BIpHg7NmzAIDCwkLExcWhU6dOkMlkcHd3x8iRI3Hy5MkGaxs5ciQGDhyIX375BUOHDoWNjQ3eeOONWtfNzMzE6dOnERUVpddefV/J+fPn8cILL8DZ2RmDBw/WLb916xYmT54MDw8PyGQydO/eHV9++WWDtV27dg1Tp05Ft27dYGNjA1dXVzzzzDN697mUlpYiKCgIQUFBKC0t1bXfu3cPXl5eGDhwIDQajej9LV68GG+99RYAwN/fHxKJRO/emtrutfntt98wevRoODo6wt7eHiNGjMCxY8dq/YwuXbqEiRMnwsnJCQqFApMmTUJJSUm9n0NDNVXLz8+vd98t0U+N+a41VI/Yz64uhw8fRr9+/SCXy9GlS5d6L8WFhYXBxcUF27dvF7VvosayNHYBRG3F/Pnz8e6772LixImYMmUKPvzwQ/zlL3/BP//5T9y8eRPTp0+vdbsnnngC9vb2+P777zF06FC9ZZs2bUL37t0RGhoKAHjttdewZcsWTJ8+HSEhIbh79y4OHz6MCxcuoE+fPvXWd/r0aTg5OSE2NhavvPIKXnjhBQQEBNS67pEjRwCgzn0+88wzCAwMxPLly3WXRHJycjBgwABIJBJMnz4dbm5u+OmnnzBlyhSoVCrExcXVWVtycjKOHDmC8ePHw9vbG1evXsWnn36KyMhInD9/Hra2trCxscFXX32FQYMGYd68efjggw8AANOmTUNBQQE2bNgACwsL0ft76qmnkJ6ejm+//Rb/+Mc/0KFDBwCAm5tbrTWeO3cOQ4YMgaOjI2bNmgUrKyt89tlniIyMRGJiIsLDw/XWf/bZZ+Hv74/4+HicPHkSX3zxBdzd3bFy5co6PwexNYndd3P6qTHftYbqaexn96AzZ85g1KhRcHNzw+LFi1FZWYlFixbBw8Ojzm369OmDX3/9tc7lRM0iEFGzHTp0SAAgzJ49W9e2ZcsWAYAQGhqq116b559/XnB3dxcqKyt1bbdv3xakUqnwzjvv6NoUCoUwbdq0RteXk5MjABDs7e2FCxcuNLj+/PnzBQBCYWGhXvuiRYsEAMLzzz9fY5spU6YIXl5ewp07d/Tax48fLygUCqGkpEQQBEFYv369AEDIzMzUrVO97EFHjx4VAAhff/21XvvcuXMFqVQqHDp0SNi8ebMAQFi9erXeOmL3995779WopdrDdcbGxgrW1tbC5cuXdetkZWUJDg4OwqOPPqprq/6MJk+erLe/P/3pT4Krq2uN93lYfTWJ3XdL9JOY75rYesR+doJQ++cul8uFa9eu6dY5f/68YGFhIdT1J+zVV18VbGxs6q2dqKl4qY6oBXz44YdwdnbG/PnzdW2dOnUCAFy6dAlvvvlmvds/99xzyM3NRUJCgq5ty5Yt0Gq1eO6553RtTk5OSEpKQlZWVqPqO336NADg7bffRlBQUIPr3717F5aWlrC3t691+Wuvvab3WhAE/PDDDxg7diwEQcCdO3d0/0VHR6OgoKDey4k2Nja6/11RUYG7d+8iICAATk5ONbZbvHgxunfvjgkTJmDq1KkYOnQoXn/99SbvTwyNRoM9e/YgNjYWnTt31rV7eXnhhRdewOHDh6FSqfS2efgzGjJkCO7evVtjvaYQu+/m9FNjvmv11dOUz66aRqPBzz//jNjYWPj6+urag4ODER0dXWc9zs7OKC0tbfDSKFFTMDgRNVP1H4bRo0fXGjQmTZpU72UFAHjsscegUCiwadMmXdumTZvQu3dvdO3aVde2atUqnD17Fj4+Pujfvz8WL16MK1euNFjjmTNnAEAvhDWHv7+/3uu8vDzk5+fj888/h5ubm95/kyZNAgDk5ubWub/S0lIsXLgQPj4+kMlk6NChA9zc3JCfn4+CggK9da2trfHll18iMzMThYWFWL9+fY0buBuzPzHy8vJQUlKCbt261VgWHBwMrVaLGzdu6LU/+IceqPpjDgD3799v9Ps/TOy+m9NPjfmu1VdPUz67B+stLS1FYGBgjWW17a+a8PtlybZyYz+ZFt7jRNRMV65cQWFhYY37PvLy8gBU3YPTEJlMhtjYWGzduhWffPIJcnJy8Ouvv2L58uV66z377LMYMmQItm7dij179uC9997DypUr8eOPP2L06NF17v/06dPw8vLS+3/89XF1dUVlZSUKCwvh4OBQY/mDZ3QA6AbIfOmllzBhwoRa99mzZ88632/GjBlYv3494uLiEBERAYVCAYlEgvHjx9c6+ObPP/8MoOqpwIyMjBoBobH7M4Tq+60eJrTAY/Ji992cfmrMd82Qx9oU9+/f190XR9TSGJyImqk6IFXfyFstPj6+1va6PPfcc/jqq6+wf/9+XLhwAYIg1HqGyMvLC1OnTsXUqVORm5uLPn364N13320wOPXq1UvsIeku52VmZtYbeKq5ubnBwcEBGo2mxpN4YmzZsgUTJkzA+++/r2srKyurdSyj06dP45133sGkSZOQmpqKl19+GWfOnIFCoWj0/sSekXBzc4OtrS3S0tJqLLt48SKkUil8fHxE7ashhjxL0th+asp3rbb3bOpn5+bmBhsbG2RkZNRYVtv+qmVmZiI4OFh0jUSNwUt1RM1U/Qe7esgAANi4cSMOHToEoP6xkh4UFRUFFxcXbNq0CZs2bUL//v31zqRoNJoal5nc3d2hVCrrnWJCo9Hg/PnzjQpOERERAIATJ06IWt/CwgLjxo3DDz/8oPc5VKsOl/Vt//DZiY8++kg3vEC1iooKTJw4EUqlEh9++CE2bNiAnJycGveQid2fnZ0dADQ42KSFhQVGjRqF7du36w0NkJOTg40bN2Lw4MFwdHSsdx9iia2pKcT2U1O/a3W9Z1M/OwsLC0RHR2Pbtm24fv26rv3ChQu6s461OXnyJAYOHNioOonE4hknomYKDg6Gv78/1qxZA1tbW0ilUqxYsQLPPvssvv/+eyxevBgzZ85Ejx496t2PlZUVnnrqKXz33XcoLi7G3//+d73lhYWF8Pb2xtNPP41evXrB3t4e+/btQ3Jyst6ZlYdlZGSgrKysUcGpc+fOCA0Nxb59+zB58mRR26xYsQIHDx5EeHg4XnnlFYSEhODevXs4efIk9u3bh3v37tW57ZgxY/Dvf/8bCoUCISEhOHr0KPbt2wdXV1e99ZYtW4bU1FTs378fDg4O6NmzJxYuXIj58+fj6aefxuOPP96o/YWFhQEA5s2bh/Hjx8PKygpjx47VhZeH33vv3r0YPHgwpk6dCktLS3z22WdQq9VYtWqVqM9IjMbU1BRi+qmp37W6NOezW7JkCXbv3o0hQ4Zg6tSpqKysxEcffYTu3bvrHnp4UEpKCu7du4eYmJhG10kkipGe5iNqU3777TdhwIABgkwmE5ydnYV58+YJWq1WmDx5smBpaSls2LBB1H727t0rABAkEolw48YNvWVqtVp46623hF69egkODg6CnZ2d0KtXL+GTTz6pd5/ff/+9AEA4e/Zso47pgw8+EOzt7fUe7a9+/DwvL6/WbXJycoRp06YJPj4+gpWVleDp6SmMGDFC+Pzzz3Xr1DYcwf3794VJkyYJHTp0EOzt7YXo6Gjh4sWLgp+fnzBhwgRBEAQhJSVFsLS0FGbMmKH3npWVlUK/fv0EpVIp3L9/X/T+qi1dulTo2LGjIJVK9eqqrc6TJ08K0dHRgr29vWBraysMGzZMOHLkiN7+6vqMattfXeqqSey+m9tPYr9rjTlWMZ9dXdsmJiYKYWFhgrW1tdC5c2dh3bp1uvd+2OzZswVfX19Bq9XWeuxEzSURBE7oQ0Q1FRQUoHPnzli1ahWmTJli7HKIGqRWq9GpUyfMmTOnzlHxiZqL9zgRUa0UCgVmzZqF9957r9WeRCNqjvXr18PKyqrGuFJELYlnnIiIiIhE4hknIiIiIpEYnIiIiIhEYnAiIiIiEonBiYiIiEgkDoBZC61Wi6ysLDg4OHCSSCIiojZOEAQUFhZCqVRCKq3/nBKDUy2ysrJabN4pIiIiMg83btyAt7d3veswONWiejb4GzdutNj8U9W0Wi3y8vLg5ubWYKql1sW+MV3sG9PFvjFd7BvxVCoVfHx8dH//68PgVIvqy3OOjo4GCU5lZWVwdHTkF9nEsG9MF/vGdLFvTBf7pvHE3J7DT5KIiIhIJAYnIiIiIpEYnIiIiIhEYnAiIiIiEonBiYiIiEgkBiciIiIikTgcAREREZksdaUGey7cxMGMLBSUqqGwkWFYoBKjgr0hs7Ro9XpM6ozToUOHMHbsWCiVSkgkEmzbtk1vuSAIWLhwIby8vGBjY4OoqChkZGTorXPv3j28+OKLcHR0hJOTE6ZMmYKioqJWPAoiIiJqCQkZWRjx0S7M35WMg+m3cOL6HRxMv4X5u5Ix4qNdSMjIavWaTCo4FRcXo1evXli7dm2ty1etWoU1a9Zg3bp1SEpKgp2dHaKjo1FWVqZb58UXX8S5c+ewd+9e7Nq1C4cOHcKrr77aWodARERELSAhIwtxW46gqKwCAKAVoPezqKwCcVuOtHp4MqlLdaNHj8bo0aNrXSYIAlavXo358+cjJiYGAPD111/Dw8MD27Ztw/jx43HhwgXs3r0bycnJ6Nu3LwDgo48+wuOPP46///3vUCqVrXYsRERE1DTqSg3m70oGAAh1rCMAkABYsCsZ+2aMabXLdiZ1xqk+mZmZyM7ORlRUlK5NoVAgPDwcR48eBQAcPXoUTk5OutAEAFFRUZBKpUhKSmr1momIiKjx9ly4icKyijpDUzUBgKqsAnsv3myNsgCY2Bmn+mRnZwMAPDw89No9PDx0y7Kzs+Hu7q633NLSEi4uLrp1aqNWq6FWq3WvVSoVgKp5frRabYvUX02r1UIQhBbfLzUf+8Z0sW9MF/vGdJlz3xxIvwWp5I/LcvWRSoD9abfweIhPk9+vMZ+R2QQnQ4qPj8eSJUtqtOfl5endP9UStFotCgoKIAgCJ100Mewb08W+MV3sG9Nlzn1zR1UkKjQBVeHqjqoIubm5TX6/wsJC0euaTXDy9PQEAOTk5MDLy0vXnpOTg969e+vWefiDq6ysxL1793Tb12bu3LmYOXOm7rVKpYKPjw/c3Nzg6OjYgkdR9UWWSCRwc3Mzuy9yW8e+MV3sG9PFvjFd5tw3Egvx8UQqATo42te44tQYcrlc9LpmE5z8/f3h6emJ/fv364KSSqVCUlIS/vrXvwIAIiIikJ+fj5SUFISFhQEADhw4AK1Wi/Dw8Dr3LZPJIJPJarRLpVKDfNkkEonB9k3Nw74xXewb08W+MV3m1jcVGi0+TDiDU7fuid5GKwAjunVs1jE2ZluTCk5FRUW4dOmS7nVmZiZSU1Ph4uICX19fxMXFYdmyZQgMDIS/vz8WLFgApVKJ2NhYAEBwcDAee+wxvPLKK1i3bh0qKiowffp0jB8/nk/UERERmbDr94owe3sSzmffF72NBICD3Aojg7wNV9hDTCo4nThxAsOGDdO9rr58NmHCBGzYsAGzZs1CcXExXn31VeTn52Pw4MHYvXu33im2b775BtOnT8eIESMglUoxbtw4rFmzptWPhYiIiMT56dx1LN19EsXllQAAKwspnuzhhx9TMwHUPiSB5PefS8f0a9URxCWCIIi8/ar9UKlUUCgUKCgoMMg9Trm5uXB3dzebU6ftBfvGdLFvTBf7xnSZQ9+UlFdi5d5UbDt9Vdfm62yPVbHhCPZ0RkJGFhbsSoaqrEL3lF31T0e5FZaO6YfIwOZfUWrM332TOuNERERE7UN6bj5mbUtC5t0/nmgbE+qLt0c9AjuZFQAgMlCJfTPGYO/FmziQnoWC0nIobKwxvKsSI4OMM1cdgxMRERG1GkEQsPm3K3hv3ymUa6rGT7KxssC86D4Y28OvxvoySwuMCfXDmNCay4yBwYmIiIhahaq0HEt+SsG+tFu6tm4eTlgVE45Org5GrEw8BiciIiIyuFM372LOjiRkFZTo2p4PC8Cbw3sY5ZJbUzE4ERERkcFoBQHrj6VhbeI5aH5/Hs1RboV3nuiHYV3Nb6ggBiciIiIyiLvFZXh7x3Ecu/rHrB6PeLtiRUw4PB1tjVhZ0zE4ERERUYs7mpmDeTuP426xGkDVuEsvDwzCa0NCYGmiwyOIweBERERELaZCo8Unv5zDl0fTdG0d7ORY/mR/hHdq+nxypoLBiYiIiFrErfxizNmehNNZf8w1N7izJ94Z0xeuduIn0jVlDE5ERETUbPsu3sTi/6WgUF0BALCUSvB6ZA/8uX8gpBJJA1ubDwYnIiIiarKyCg3+vv8UNv92RdfW0ckOK2PC0UPpYsTKDIPBiYiIiJrkyh0VZm1LQkZega4tOtgbCx4Lg4PcyoiVGQ6DExERETWKIAjYdvoqVuxNRVmFBgAgt7TA7JG98adenSBpQ5fmHsbgRERERKIVqSuwbPdJ/HT+hq6tSwdHrIoNR4CbwoiVtQ4GJyIiIhLl3O17mL0tCTfyi3Vt43r7462oXrCxah+Ron0cJRERETWZIAj4T3IGVh88g0pt1bQp9jJLLBrdF6OCvY1cXeticCIiIqI63StRY+GuZPxyOVvXFurljJWxA+DtZGfEyoyDwYmIiIhqlXwtF3N3HEdeUZmubeKArpj+aCisLMx32pTmYHAiIiIiPZVaLT4/fAGf/3oBwu9tzrYyvDu2HwZ19jRqbcbG4EREREQ6OaoSzN1xHCk37ujawju5492x/eBmb2PEykwDgxMREREBABIysrBw1wkUlJUDACwkEkx9NASTBgTBQtp2x2ZqDAYnIiKidq68UoPVB8/gmxOXdG1ejraIj+mPR7w7GLEy08PgRERE1I5du1eI2duTcCE7X9c2vKsSix/vC4WNtfEKM1EMTkRERO3UrrPX8O7Pv6GkvBIAYG0hxf8b0QvP9uncpqdNaQ4GJyIionampLwS8Xt+w44z13RtnVwcsDI2HEEeTsYrzAwwOBEREbUjaTn5mLUtCVfvFeranuzhh7mjHoGtNWNBQ/gJERERtQOCIGDTyct4f/9plGu0AABba0vMi34EY0L9jFyd+WBwIiIiauMKSsux6H8ncDA9S9cW7OmElTHh8HNxMGJl5ofBiYiIqA1LvXkXb+9Mxm1Via7tpX4BeCOyB6wtLYxYmXlicCIiImqDNFoB3566gX//dh0aoWriFIXcGu+M6YvIQKWRqzNfDE5ERERtTF5RKd7ecRzHr+Xp2vr4dMCKJ/vDw9HWiJWZPwYnIiKiNuTXy9mYtysZ90vUAAAJgP8bHIxXBgXDUio1bnFtAIMTERFRG1Ch0eLjQ2ex4Vi6rs3V1horYsLRv5OHEStrWxiciIiIzNzN/GLM2Z6EM1n3dG1DunhiRn8/BPq6GbGytofBiYiIyIztuXATS346gSJ11bQpllIJ4ob1wAthXZCXl9fA1tRYDE5ERERmqLSiEu/tO4UfUjN1bT5OdlgZG47uXi7QarVGrK7tYnAiIiIyM5fyCjBrWxIu31Hp2kaH+GD+Y31gL7MyYmVtH4MTERGRmRAEAVtPXcXKvakoq9QAAOSWFpgzqjdie3aCRCIxcoVtH4MTERGRGSgsq8DS3Sn4+cJNXVugmwKrYsPRuYOjEStrXxiciIiITNyZrHuYvT0Jt/KLdW3PPtIZfxvRC3IrTpvSmhiciIiITJRWEPDv4+lYk3AWldqqaVMcZFZY/HgYooK8jVxd+8TgREREZILuFpdhwa5k/HolR9fWU+mCFTHh6OhkZ8TK2jcGJyIiIhOTdDUX83YeR15RGYCqaVMmRXTD1CHdYWXBaVOMicGJiIjIRFRqtVj3y3l8ceQihN/bXGxlWP5kf0T4c9oUU8DgREREZAJuF5Rgzo4kpN68q2sb0Mkd747tjw72ciNWRg9icCIiIjKyA+m3sOi/J6AqqwAAWEgkmD60OyYO6AYpx2YyKWZ1oVSj0WDBggXw9/eHjY0NunTpgqVLl0IQBN06giBg4cKF8PLygo2NDaKiopCRkWHEqomIiGqnrtRgxZ5UvPnDUV1oUipssf6lSEyOCGJoMkFmdcZp5cqV+PTTT/HVV1+he/fuOHHiBCZNmgSFQoHXX38dALBq1SqsWbMGX331Ffz9/bFgwQJER0fj/PnzkMt5qpOIiEzD1buFmLU9CWk5+bq2qG4dsejxMDjKrY1XGNXLrILTkSNHEBMTgyeeeAIA0KlTJ3z77bc4fvw4gKqzTatXr8b8+fMRExMDAPj666/h4eGBbdu2Yfz48UarnYiIqNrOM9fw7s8nUVpRNW2KtYUUb0X1wjOPdOa0KSbOrILTwIED8fnnnyM9PR1du3bFqVOncPjwYXzwwQcAgMzMTGRnZyMqKkq3jUKhQHh4OI4ePVpncFKr1VCr1brXKlXVpIlarbbFZ5fWarUQBIGzVpsg9o3pYt+YLvZN45SUV2L5nlT899x1XZu/qwNWPNkfXd0VEARB7/aT5mDfiNeYz8isgtOcOXOgUqkQFBQECwsLaDQavPvuu3jxxRcBANnZ2QAADw/9RzY9PDx0y2oTHx+PJUuW1GjPy8tDWVlZCx5BVecUFBRAEARIpWZ1i1mbx74xXewb08W+Ee/S3SIsT7iIW6o//q48FuiBv4Z3hhxq5Obmtuj7sW/EKywsFL2uWQWn77//Ht988w02btyI7t27IzU1FXFxcVAqlZgwYUKT9zt37lzMnDlT91qlUsHHxwdubm5wdGzZiRO1Wi0kEgnc3Nz4RTYx7BvTxb4xXeybhgmCgG9TLmN1wllUaKrObNhZW2Je9CMYHeJjsPdl34jXmHugzSo4vfXWW5gzZ47ukluPHj1w7do1xMfHY8KECfD09AQA5OTkwMvLS7ddTk4OevfuXed+ZTIZZDJZjXapVGqQL5tEIjHYvql52Demi31jutg3dcsvUWPR/04gIeO2ri3E0xkrY8Lh62Jv8Pdn34jTmM/HrD7JkpKSGgdnYWGhuzbp7+8PT09P7N+/X7dcpVIhKSkJERERrVorERG1bydv5OHZL/fphaY/9w/E138Z1iqhiQzDrM44jR07Fu+++y58fX3RvXt3/Pbbb/jggw8wefJkAFXJOi4uDsuWLUNgYKBuOAKlUonY2FjjFk9ERO2CRivgiyMXsO7weWh/v8/bycYaS8f0w6MBXvVvTCbPrILTRx99hAULFmDq1KnIzc2FUqnE//3f/2HhwoW6dWbNmoXi4mK8+uqryM/Px+DBg7F7926O4URERAaXW1iKt3ceR/K1PF1bX183LH+yPzwcbIxYGbUUidBSzz22ISqVCgqFAgUFBQa5OTw3Nxfu7u685mxi2Demi31jutg3fzh8+Tbm70zG/dJyAIBUArw2OAQvDwyGhbT1x2Zi34jXmL/7ZnXGiYiIyNRUaLRYk3AGXx//Y3ovdwcbrHiyP8J83YxYGRkCgxMREVET3bhfhNnbknAu+76ubWiAF955oi+cbGs+rU3mj8GJiIioCXafv4F3fkpBcXklAMDKQoo3h/XAC30DOG1KG8bgRERE1AilFZVYtfcUfjyVqWvzdbbHqthwBHs6G7Eyag0MTkRERCJl5BZg1rZjuHL3jyk6Hu/ui/nRj8BOZmXEyqi1MDgRERE1QBAEbEnNxHv7UqGurBp0WW5lgXmjHsHYHn68NNeOMDgRERHVQ1VWjqU/ncSeizd1bd3cFVgVOwCdXB2MWBkZA4MTERFRHU7fuovZ25OQVVCia3uuTxf8bURPyCwtjFgZGQuDExER0UO0goCvktLxceJZVP4+b4qD3ApLHu+LEd06Grk6MiYGJyIiogfcLS7DvJ3JOJqZo2vr3dEV8TH9oVTYGbEyMgUMTkRERL87lpmDt3cex91iNQBAAuDlgUF4bUgILDltCYHBiYiICBUaLT795Ry+PJqG6glcO9jJsfzJfgjv5GHU2si0MDgREVG7llVQjLnbjyP11l1d20B/Dywb2w+udnIjVkamiMGJiIjarX1pt7D4fydQWFYBALCUSjBjaCj+Et4VUo7NRLVgcCIionZHXanB+/tPY9PJy7o2pcIWK2PC0bOjqxErI1PH4ERERO1K5l0VZm1LQnpuga5tVJA3FozuA0e5tRErI3PA4ERERO2CIAjYceYalu/5DWUVGgCAzFKK2SN746le/pw2hURhcCIiojavWF2BZT//hv+du65r69zBEatiwxHopjBiZWRuGJyIiKhNO3/7PmZvT8L1+0W6tnG9/fFWVC/YWPHPIDUOvzFERNQmCYKAb5Iv4R8HT+umTbGXWWLBY2F4LMTHyNWRuWJwIiKiNud+iRoL/3sChy7d1rV193LGqphweDvbG7EyMncMTkRE1KacuJ6HOduTkFdUpmubEN4VM4aGwsqC06ZQ8zA4ERFRm6DRCvjnrxfw2a/n8fuVOTjbyrBsTF8M7uJl3OKozWBwIiIis5dTWIq3dyThxPU7urb+fm54d2x/uDvYGLEyamsYnIiIyKwlZmRh4X9PIL+0HAAglQBTh3TH5IggWEg5NhO1LAYnIiIyS+WVGnyYcAb/Sb6ka/NwsMGKmHD08elgxMqoLWNwIiIis3P9XhFmb0/C+ez7urZhgUoseaIvFDacNoUMh8GJiIjMyv/OXcfS3SdRUl4JALCykOJvw3tifFgXTptCBsfgREREZqGkvBIr9qZi++mrujY/F3usih2AIA8no9VF7QuDExERmbz03HzM2paEzLuFurYne/hh7qhHYGvNP2XUevhtIyIikyUIAjb/dgXv7TuFco0WAGBjZYF50X0wtoefkauj9ojBiYiITJKqtByLf0rB/rRburZuHk5YFROOTq4ORqyM2jMGJyIiMjmpN+9gzvbjuK0q0bW90DcAbw7rAWtLCyNWRu0dgxMREZkMrSBg/dE0rD10Dhqhat4Uhdwa74zpi8hApZGrI2JwIiIiE3GnqAzzdh7Hsau5urZHvF2xIiYcno62RqyM6A8MTkREZHRHrmRj3s5k3CtRAwAkAF4ZFIz/GxwMS6nUuMURPYDBiYiIjKZCo8XaQ+ew/liars3NXo7lT/ZHfz93I1ZGVDsGJyIiMopb+cWYsz0Jp7Pu6doGd/bE0rH94GIrM2JlRHVjcCIiola39+JNLPlfCgrVFQAAS6kEbwzrgZf6BULKaVPIhDE4ERFRqymr0ODv+09h829XdG3eTnZYGROOUKWLESsjEofBiYiIWsXlOyrM2nYMl/JUurbHgn2wYHQf2MusjFgZkXgMTkREZFCCIGDr6atYuScVZZUaAIDc0gKzR/XGn3p2goSX5siMMDgREZHBFKkrsPSnk9h94YauLcDNEatiB6BLB0cjVkbUNAxORERkEOdu38OsbUm4mV+sa3vmkc74fyN6QW7FaVPIPDE4ERG1EnWlBnsu3MTBjCwUlKqhsJFhWKASo4K9ITOz+deqj+VA+i3cURWhg6M9hnftiFHB3rCykOI/yRn48OAZVGqrpk1xkFlh4egwjAr2NnLlRM3D4ERE1AoSMrIwf1cyCssqIJUAWgGQSoD9abewcl8qlo3pZzZzsdV6LDkqHEjPwoq9qfB1tsP57Hzd+j2ULlgREw5vJzvjFU3UQsxuHPtbt27hpZdegqurK2xsbNCjRw+cOHFCt1wQBCxcuBBeXl6wsbFBVFQUMjIyjFgxEbV3CRlZiNtyBEVlVWMW/X4SRvezqKwCcVuOICEjy0gVitfgsagr9ELTpAHdsP6lSIYmajPMKjjdv38fgwYNgpWVFX766SecP38e77//PpydnXXrrFq1CmvWrMG6deuQlJQEOzs7REdHo6yszIiVE1F7pa7UYP6uZACAUMc61e0LdiVD/ftTZ6ZIzLFUkwBY8/RAxA3rASsLs/pTQ1Qvs7pUt3LlSvj4+GD9+vW6Nn9/f93/FgQBq1evxvz58xETEwMA+Prrr+Hh4YFt27Zh/PjxrV4zEbVvey7cROHvZ2fqIwBQlVXgP8cz8GiAl+ELa4LEjCxRxwJUHU/1qOBEbYlZBacdO3YgOjoazzzzDBITE9GxY0dMnToVr7zyCgAgMzMT2dnZiIqK0m2jUCgQHh6Oo0eP1hmc1Go11Gq17rVKVTU4m1arhVarbdFj0Gq1EAShxfdLzce+MV3m3DcH0m/p7gMSY03iWaxJPGvYolpB9f1bj4f4GLuUdsucf29aW2M+I7MKTleuXMGnn36KmTNn4u2330ZycjJef/11WFtbY8KECcjOzgYAeHh46G3n4eGhW1ab+Ph4LFmypEZ7Xl5ei1/i02q1KCgogCAIkEp5+tqUsG9Mlzn3zR1VkejQ1JZohapjz83NNXYp7ZY5/960tsLCQtHrmlVw0mq16Nu3L5YvXw4AeOSRR3D27FmsW7cOEyZMaPJ+586di5kzZ+peq1Qq+Pj4wM3NDY6OLTtAm1arhUQigZubG7/IJoZ9Y7rMuW86ONpDkq1q8J6gakqFLQZ0cjdoTU117GousgpKRK0rlVQdu7u7aR5Le2DOvzetTS6Xi17XrIKTl5cXQkJC9NqCg4Pxww8/AAA8PT0BADk5OfDy+uMegZycHPTu3bvO/cpkMshkshrtUqnUIF82iURisH1T87BvTJe59o2fs73o0AQA0x7tjjGhfgarpzl2nrmmuzm8IVoBGNGto9n1V1tjrr83ra0xn49ZfZKDBg1CWlqaXlt6ejr8/Kr+kfH394enpyf279+vW65SqZCUlISIiIhWrZWI6MdTmfj6eLqodSUAHOVWGBlkugNEjgr2hoPcCg3NLGcOx0LUVGYVnN58800cO3YMy5cvx6VLl7Bx40Z8/vnnmDZtGoCqZB0XF4dly5Zhx44dOHPmDP7yl79AqVQiNjbWuMUTUbuhFQR8mHAGS/6XAs0Dp5vqChzV7UvH9DPpEcRllhZYNqYfAPM/FqKmMqvg1K9fP2zduhXffvstQkNDsXTpUqxevRovvviibp1Zs2ZhxowZePXVV9GvXz8UFRVh9+7djbp+SUTUVGUVGszeloQvj/5xdvylfgH4x1MRcJBbAai6/+fBnw5yK6x+eqBZjBweGajE6qcHtoljIWoKiSAI7fB5j/qpVCooFAoUFBQY5Obw3NxcuLu785qziWHfmC5z6Zu7xWV4Y8sRnMm6B6AqTMwe2RvjwwIAVA0guffiTRxIz0JBaTkUNtYY3lWJkUHmOVfd3os3sT/tj7nqRnTraJbH0laZy++NKWjM332zujmciMhUXb6jwvTvD+ueOrO1tsSqmHAMeWAwS5mlBcaE+pnszd+NUX0sj4f48I8ztSsMTkREzZR0NQd/+/GYbqRsdwcbfPzMIHTzcDJuYUTU4hiciIia4cdTmXh390lU/j7KZZCHE9Y8MwgeDjZGroyIDIHBiYioCbSCgI8Sz+rdBD40wAsrYsJha81/WonaKv52ExE1UlmFBgt2JWPPxZu6thf7BuBvI3rBQtrQKEdEZM4YnIiIGuFucRnithzB6QeenJsV1RvP9w0wcmVE1BoYnIiIRLpyR4Xpm3/FrfxiAFVPzq2MCcejDzw5R0RtG4MTEZEItT0599EzgxDEJ+eI2hUGJyKiBmw9lYllfHKOiMDgRERUJ60g4OPEs/jXA0/OPRrghZV8co6o3WrSMK/x8fEoKipq6VqIiExG9ZxzD4amF/oGYPW4gQxNRO1Yk4LTf/7zH/j4+GDevHnIy8ursfzcuXP497//3eziiIiM4W5xGV7ZmKgbbkAqAeaM7I3ZI3tzuAGidq5JwencuXP45ptvsHPnTnTq1AkzZszAhQsXkJeXhxs3buD999/Hyy+/3NK1EhEZ3JU7Kvz564O64QZsrCzw4dODONwAEQFo4j1OBw4cwKuvvoqsrCwAwNq1a/HJJ5/orTNp0qTmV0dE1IqSrubibz8e5ZNzRFSnJgWn6dOnw87ODlu2bEGnTp0AALm5uYiPj8cvv/yC5557Dl988UVL1klEZFAPPznXzcMJH/HJOSJ6SJOC09WrV/H+++/jqaee0mt/7LHHsHHjRrz88stYtGgRlixZ0iJFEhEZCp+cI6LGaNI9TsHBwUhJSal12QsvvIDZs2dj7dq1zSqMiMjQyio0mL2dT84RkXhN+pdh1qxZeP755+Hh4YF58+bB1tZWb7kgCCgpKWmRAomIDKG2OefeiuqNF3gTOBHVo0nB6bnnnsOZM2ewfPlyfPbZZxg9ejT69OmDDh064OzZs1izZg369OnT0rUSEbWIh+ecs7GywMqYcAwNVBq5MiIydU0+F71s2TJER0fjww8/xI4dO/DNN9/olvn7++PTTz9tkQKJiFrSw0/OudnL8dEzgxDs6WzkyojIHDTrIv6QIUMwZMgQaLVaXLlyBXl5eVAoFAgODoZEwkHiiMi0bDt9FUt/StF/cu7pgfBwtG1gSyKiKi1y96NUKkVAQAACAnhvABGZHq0gYO2hc/jiyEVd25AunlgVO4A3gRNRo/BfDCJq08oqNFj432T8fOGmru35sAC8FdWL06cQUaMxOBFRm3WvRI24Lb/i1K0/npz7fyN64cV+gUaujIjMFYMTEbVJmXdVmP79r7jJJ+eIqAUxOBFRm3P8Wi5m/ngUhWV8co6IWhaDExG1KdtPX8U7fHKOiAyEwYmI2oS6npxbGRMOO5mVESsjoraEwYmIzJ66UoMFu2o+Off/onrCUtqkKTmJiGrF4EREZo1PzhFRa2JwIiKzVduTcytiwhHJJ+eIyEAYnIjIZKkrNdhz4SYOpN/CHVUROjjaY3jXjhgV7I1Tt+7yyTkianUMTkRkkhIysjB/VzIKyyoglQBaAZDmqHAgPQvLdp9EuUaD3x+cQzd3BT56ZhCfnCMig2NwIiKTk5CRhbgtR3SvqwNS9c+ySo1uGZ+cI6LWxOBERCZFXanB/F3JAAChgXWtLKRYGRsOO2uGJiJqHXxOl4hMyp4LN1FYVtFgaAKACo0WB9OzDF4TEVE1BiciMikHM7IglYhbVyoBDjA4EVErYnAiIpNRpK5A5h2V7l6mhmgFoKC03LBFERE9gPc4EZFR3S4oQeKlLCRk3EbytVzdHHNiSCWAwsbagNUREeljcCKiViUIAi7m5CMhoyosXczJb/K+tAIwvCsHuySi1sPgREQGV6HRIvlaLhIybiMhIws5haW1ruflaIshXTyx6+x1lFZU1nuDuASAg9wKI4O8DVIzEVFtGJyIyCBUpeX45XI2Ei5l4dfL2Sgur6x1vRBPZwwN9MKwQCW6uisgkUgwqIsn4rYcgQS1D0lQfe/40jH9ILO0MNQhEBHVwOBERC3mZn4xEtKzkHApCyev34FGqBl7rCyk6O/nhshAJYYGeNU62ndkoBKrnx6IBbuSoXpw5PDffzrIrbB0TD/OSUdErY7BiYiaTCsIOHf7PhIzsnAwIwuX8lS1rqeQW2NIgCciA5UY6O8hapTvyEAl9s0Yg70Xb2J/2h9z1Y3o1hEjg7x5pomIjILBiYgaRV2pQdLVXCRkZOHQpdvIKyqrdT0fJztEdlUiMlCJ3t6usJQ2fvQTmaUFxoT64fEQH+Tm5sLd3R3SJuyHiKilMDgRUYPul6jxy6XbOJiRhaOZOSit0NRYRwKgh9IFkYFKRHZVorOrAyQSkSNZEhGZCbMOTitWrMDcuXPxxhtvYPXq1QCAsrIy/O1vf8N3330HtVqN6OhofPLJJ/Dw8DBusURm5urdQiReysLB9CycunW31kEpZZZSDOjkgchAJR4N8EIHe3nrF0pE1IrMNjglJyfjs88+Q8+ePfXa33zzTfz3v//F5s2boVAoMH36dDz11FP49ddfjVQpmTp1pQZ7LtzEgfQ/7qMZ3rUjRgWb53001cdzMCMLBaVqKGxkGBaobPB4NFoBp7PuIiEjC4kZt5F5t7DW9ZxtZRga4IXIQCXCO7nD1tps/xkhImo0s/wXr6ioCC+++CL++c9/YtmyZbr2goIC/Otf/8LGjRsxfPhwAMD69esRHByMY8eOYcCAAcYqmUxUQkYW5u9KRuGDT27lqHAgPQsr96VimZk9uVXr8UiA/Wm3aj2e0opKHMvMxcHf71e6X6Kudb/+rg66IQN6KF1hIXYyOSKiNsYsg9O0adPwxBNPICoqSi84paSkoKKiAlFRUbq2oKAg+Pr64ujRo3UGJ7VaDbX6jz8YKlXVk0FarRZarbZFa9dqtRAEocX3S42XkJGFmT8e072uvhRV/bOorAJxW47gg6cGmEV4Ens8ix8PQ6VWi8RLt5F0NRfqyprfRakE6NXRFZEBXhga6AU/F4cHlgrQNmJalJbA3xvTxb4xXewb8RrzGZldcPruu+9w8uRJJCcn11iWnZ0Na2trODk56bV7eHggOzu7zn3Gx8djyZIlNdrz8vJQVlb7E0NNpdVqUVBQAEEQ+HSQEZVXarFg1wkAtQ+wWN0uAbBg1wl8+1x/WFuabn+JPR4AWPS/lFqXyy2lCOvojAgfF/TzcYGT/PchAypLkZtb+0jfrYW/N6aLfWO62DfiFRbWfmtCbcwqON24cQNvvPEG9u7dC7m85W5CnTt3LmbOnKl7rVKp4OPjAzc3Nzg6OrbY+wBVX2SJRAI3Nzd+kY1o19nrKKpjJOsHCQCKyivx87V8DOniZfjCmujQ1duijudhHezlGNrFE0MDlejv52ay93Tx98Z0sW9MF/tGvMZkCrMKTikpKcjNzUWfPn10bRqNBocOHcLHH3+Mn3/+GeXl5cjPz9c765STkwNPT8869yuTySCTyWq0S6VSg3zZJBKJwfZN4iRcuq27B0iMjw+dx8eHzhu2qFbUycUe747tjxAvZ0jNZMgA/t6YLvaN6WLfiNOYz8esgtOIESNw5swZvbZJkyYhKCgIs2fPho+PD6ysrLB//36MGzcOAJCWlobr168jIiLCGCWTiamebPZ0HY/Xtxcd7G0QqnQxdhlERGbHrIKTg4MDQkND9drs7Ozg6uqqa58yZQpmzpwJFxcXODo6YsaMGYiIiOATde2Y2Mlm66NU2CLC33THAjuamYOsghJR60olgMLG2sAVERG1TWYVnMT4xz/+AalUinHjxukNgEnti5jJZhtj2qPdMSbUr4Wqa3k7z1zD/F01H5iojVYAhnc1/acEiYhMkdkHp4SEBL3Xcrkca9euxdq1a41TEBmF2MlmHeVWGNLFC4O7eGL5z7+hSF1R51NoQNVTdQ5yK4wM8jZI3S1lVLA3Vu5LRVFZ2zgeIiJTZfbBidovsZPNejvZITJQiWFd9SebtbW2RNyWI5Cg9kf4q2+ZXjqmn8k+bVZNZmmBZWP6tZnjISIyVQxOZFbul6hx6NJtJGRk4UhmDspqmWwWAHr+Ptns0EAvdOngWOtks5GBSqx+eiAW7EqG6qGRtrVC1ZmZpWY0cnhbOx4iIlPE4EQmT+xks+GdPDCskZPNRgYqsW/GGOy9eBP70/6Yq25Et44YGWR+c9U9eDwH0rNQUFoOhY01hndVmuXxEBGZGgYnMjmNmWz20YCq+dOaM9mszNICY0L98HiID3Jzc+Hu7m7WY55UH48p38xORGSuGJzIJJSUVyLpKiebJSIi08bgREZzp6gMhy7dxsGMLCRdzalzstne3h0QGeiFoQFKdHJ1qGVPRERErYPBiRpNXanBngs3cTAjCwWlaihsZBgWqMSo4PrvoREEAZfvqJCYUXVz95mse7U+/SW3ssBA/6r7lQYHeMHFtuZ0OERERMbA4ESNkpCRhfm7klH40FNb+9NuYeW+VCx76KmtSq0WqTfv4mB6FhIzsnAjv7jW/Xawk2NooBcif79fiTcxExGRKWJwItESMrIQt+WI7nX1023VP4vKKhC35QhWxPSHhVSKg+lZ+OXybajKKmrdX4CbI4YFKjE0UInuZjTZLBERtV8MTiSKulKjm9KjrpGpq9tnbz9e63ILiQR9fDtUhaUAL3g727d8oURERAbE4ESi7LlwE4V1nDmqj521JQZ18URkgBJDunjCkZPLEhGRGWNwIlEOZmTp7mkSw9vJDvOiH0E/P3dYWZjvmEhEREQPYnAiUfJL1aJDEwB4OtpiYGdPwxVERERkBAxOVK8KjRa7z9/Axex80dtIJYCCl+SIiKgNYnCiWhWpK/BDaia+Sc5ATmFpo7bVCsDwrpxIloiI2h4GJ9KTV1SKjScuYfPJKyhU698MbiGRQCPUf71OAsBBboWRQd4GrJKIiMg4GJwIAJB5V4WvktKx6+x1VGj0pz6JDPTCxAHdUFBarhvHqbb4VD0K09Ix/TiAJRERtUkMTu3cbzfvYMOxNCRk3NZrt7KQYkyoL/7Svys6d3DUta9+eiAW7EqG6qGRw7VC1ZmmpQ+NHE5ERNSWMDi1Q1pBQEJGFjYcS8OpW/f0ljnIrPD0I53xQt8AuDvY1Ng2MlCJfTPGYO/FmziQnoWC0nIobKwxvKsSI4Pqn6uOiIjI3DE4tSPqSg12nb2Gr5LSce1ekd4yDwcbvNgvEON6+8NeZlXvfmSWFhgT6ocxoX6GLJeIiMjkMDi1A6rScnz/22VsPHEJd4vVessC3BwxMbwbHgvx4UCVREREDWBwasNuF5TgP8kZ+CH1CkorNHrL+vq6YdKArhjU2RMSTq5LREQkCoNTG5Sem48Nx9Lx84UbqHxguG+pBIjq5o0J4V0RqnQxYoVERETmicGpjRAEAcev5WHDsTQcyczRWyazlCK2pz/+3D8QPs72RqqQiIjI/DE4mblKrRb7Lt7ChqQ0XHhoWhQnG2uMD+uC58IC4GIrM06BREREbQiDk5kqKa/E9tNX8e/kDNzKL9Zb1tHJDn/uF4iYnp1ga80uJiIiain8q2pm7pWo8d2JS9h08jLyS8v1loV4OmNCeFdEBXWEpZRPyBEREbU0BiczceN+Eb5OSsf2M1ehrtSfEmWgvwcmDuiG/n5ufEKOiIjIgBicTNzZrHv4Kikd+9Ju4oEH5GAplSA62AcTwruim4eT0eojIiJqTxicTJAgCPj1SjbWH0vHiet5estsrCwwrndnvNQvEF4KWyNVSERE1D4xOLUSdaUGey7cxIH0W7ijKkIHR3sM79oRo4L/mN+tQqPF7vM3sCEpDZfyVHrbu9rJ8GLfQDzzSGc42lgb4xCIiIjaPQanVpCQkYX5u5JRWFYBqQTQCoA0R4UD6VlYuS8V86P7IKewFP9JzkBuYanetn4u9pgY3g1PhPpyAl0iIiIjY3AysISMLMRtOaJ7XX2fUvXPwrIKzN6eVGO7Xh1dMHFAN0QGKiHlDd9EREQmgcHJgNSVGszflQwAEBpYt1pkoBcmDuiGR7w7GK4wIiIiahIGJwPac+EmCssqRK//RmQoJkcEGbAiIiIiag6OkmhABzOyIBV5lU0qAc7evm/YgoiIiKhZGJwMqKBUrTf2Un20AlDw0EjgREREZFoYnAxIYSNr1BknBYcZICIiMmkMTgY0LFDZqDNOw7sqDVsQERERNQuDkwGNCvaGg9wKDZ10kgBwlFthZJB3a5RFRERETcTgZEAySwssG9MPAOoMT9XtS8f04wCXREREJo7BycAiA5VY/fRAOMitAEB3z1P1Twe5FVY/PRCRgbxMR0REZOo4jlMriAxUYt+MMdh78Sb2p/0xV92Ibh0xMsibZ5qIiIjMBINTK5FZWmBMqB8eD/FBbm4u3N3dIZXyhB8REZE54V9uIiIiIpHMKjjFx8ejX79+cHBwgLu7O2JjY5GWlqa3TllZGaZNmwZXV1fY29tj3LhxyMnJMVLFRERE1JaYVXBKTEzEtGnTcOzYMezduxcVFRUYNWoUiouLdeu8+eab2LlzJzZv3ozExERkZWXhqaeeMmLVRERE1FaY1T1Ou3fv1nu9YcMGuLu7IyUlBY8++igKCgrwr3/9Cxs3bsTw4cMBAOvXr0dwcDCOHTuGAQMGGKNsIiIiaiPM6ozTwwoKCgAALi4uAICUlBRUVFQgKipKt05QUBB8fX1x9OhRo9RIREREbYdZnXF6kFarRVxcHAYNGoTQ0FAAQHZ2NqytreHk5KS3roeHB7Kzs+vcl1qthlqt1r1WqVS699BqtS1etyAILb5faj72jeli35gu9o3pYt+I15jPyGyD07Rp03D27FkcPny42fuKj4/HkiVLarTn5eWhrKys2ft/kFarRUFBAQRB4HAEJoZ9Y7rYN6aLfWO62DfiFRYWil7XLIPT9OnTsWvXLhw6dAje3n/M7+bp6Yny8nLk5+frnXXKycmBp6dnnfubO3cuZs6cqXutUqng4+MDNzc3ODo6tmjtWq0WEokEbm5u/CKbGPaN6WLfmC72jeli34gnl8tFr2tWwUkQBMyYMQNbt25FQkIC/P399ZaHhYXBysoK+/fvx7hx4wAAaWlpuH79OiIiIurcr0wmg0wmq9EulUoN8mWTSCQG2zc1D/vGdLFvTBf7xnSxb8RpzOdjVsFp2rRp2LhxI7Zv3w4HBwfdfUsKhQI2NjZQKBSYMmUKZs6cCRcXFzg6OmLGjBmIiIjgE3VERETUbGYVnD799FMAQGRkpF77+vXrMXHiRADAP/7xD0ilUowbNw5qtRrR0dH45JNPWrlSIiIiaovMKjgJgtDgOnK5HGvXrsXatWtboSIiIiJqT3jRk4iIiEgkBiciIiIikRiciIiIiERicCIiIiISicGJiIiISCQGJyIiIiKRGJyIiIiIRGJwIiIiIhKJwYmIiIhIJAYnIiIiIpEYnIiIiIhEYnAiIiIiEonBiYiIiEgkBiciIiIikRiciIiIiERicCIiIiISicGJiIiISCQGJyIiIiKRGJyIiIiIRGJwIiIiIhKJwYmIiIhIJAYnIiIiIpEYnIiIiIhEYnAiIiIiEonBiYiIiEgkBiciIiIikRiciIiIiERicCIiIiISicGJiIiISCQGJyIiIiKRGJyIiIiIRGJwIiIiIhKJwYmIiIhIJAYnIiIiIpEYnIiIiIhEYnAiIiIiEonBiYiIiEgkBiciIiIikRiciIiIiERicCIiIiISicGJiIiISCQGJyIiIiKRGJyIiIiIRGJwIiIiIhKJwYmIiIhIJAYnIiIiIpHabHBau3YtOnXqBLlcjvDwcBw/ftzYJREREZGZa5PBadOmTZg5cyYWLVqEkydPolevXoiOjkZubq6xSyMiIiIz1iaD0wcffIBXXnkFkyZNQkhICNatWwdbW1t8+eWXxi6NiIiIzFibC07l5eVISUlBVFSUrk0qlSIqKgpHjx41YmVERERk7iyNXUBLu3PnDjQaDTw8PPTaPTw8cPHixVq3UavVUKvVutcFBQUAgPz8fGi12hatT6vVQqVSwdraGlJpm8utZo19Y7rYN6aLfWO62DfiqVQqAIAgCA2u2+aCU1PEx8djyZIlNdr9/PyMUA0REREZQ2FhIRQKRb3rtLng1KFDB1hYWCAnJ0evPScnB56enrVuM3fuXMycOVP3WqvV4t69e3B1dYVEImnR+lQqFXx8fHDjxg04Ojq26L6pedg3pot9Y7rYN6aLfSOeIAgoLCyEUqlscN02F5ysra0RFhaG/fv3IzY2FkBVENq/fz+mT59e6zYymQwymUyvzcnJyaB1Ojo68otsotg3pot9Y7rYN6aLfSNOQ2eaqrW54AQAM2fOxIQJE9C3b1/0798fq1evRnFxMSZNmmTs0oiIiMiMtcng9NxzzyEvLw8LFy5EdnY2evfujd27d9e4YZyIiIioMdpkcAKA6dOn13lpzphkMhkWLVpU49IgGR/7xnSxb0wX+8Z0sW8MQyKIefaOiIiIiNreAJhEREREhsLgRERERCQSgxMRERGRSAxOrWjt2rXo1KkT5HI5wsPDcfz4cWOX1C4dOnQIY8eOhVKphEQiwbZt2/SWC4KAhQsXwsvLCzY2NoiKikJGRoZxim1H4uPj0a9fPzg4OMDd3R2xsbFIS0vTW6esrAzTpk2Dq6sr7O3tMW7cuBqD3VLL+/TTT9GzZ0/deEARERH46aefdMvZL6ZjxYoVkEgkiIuL07Wxf1oWg1Mr2bRpE2bOnIlFixbh5MmT6NWrF6Kjo5Gbm2vs0tqd4uJi9OrVC2vXrq11+apVq7BmzRqsW7cOSUlJsLOzQ3R0NMrKylq50vYlMTER06ZNw7Fjx7B3715UVFRg1KhRKC4u1q3z5ptvYufOndi8eTMSExORlZWFp556yohVtw/e3t5YsWIFUlJScOLECQwfPhwxMTE4d+4cAPaLqUhOTsZnn32Gnj176rWzf1qYQK2if//+wrRp03SvNRqNoFQqhfj4eCNWRQCErVu36l5rtVrB09NTeO+993Rt+fn5gkwmE7799lsjVNh+5ebmCgCExMREQRCq+sHKykrYvHmzbp0LFy4IAISjR48aq8x2y9nZWfjiiy/YLyaisLBQCAwMFPbu3SsMHTpUeOONNwRB4O+NIfCMUysoLy9HSkoKoqKidG1SqRRRUVE4evSoESujh2VmZiI7O1uvrxQKBcLDw9lXraygoAAA4OLiAgBISUlBRUWFXt8EBQXB19eXfdOKNBoNvvvuOxQXFyMiIoL9YiKmTZuGJ554Qq8fAP7eGEKbHQDTlNy5cwcajabGyOUeHh64ePGikaqi2mRnZwNArX1VvYwMT6vVIi4uDoMGDUJoaCiAqr6xtrauMY8k+6Z1nDlzBhERESgrK4O9vT22bt2KkJAQpKamsl+M7LvvvsPJkyeRnJxcYxl/b1oegxMRmZxp06bh7NmzOHz4sLFLod9169YNqampKCgowJYtWzBhwgQkJiYau6x278aNG3jjjTewd+9eyOVyY5fTLvBSXSvo0KEDLCwsajzFkJOTA09PTyNVRbWp7g/2lfFMnz4du3btwsGDB+Ht7a1r9/T0RHl5OfLz8/XWZ9+0DmtrawQEBCAsLAzx8fHo1asXPvzwQ/aLkaWkpCA3Nxd9+vSBpaUlLC0tkZiYiDVr1sDS0hIeHh7snxbG4NQKrK2tERYWhv379+vatFot9u/fj4iICCNWRg/z9/eHp6enXl+pVCokJSWxrwxMEARMnz4dW7duxYEDB+Dv76+3PCwsDFZWVnp9k5aWhuvXr7NvjECr1UKtVrNfjGzEiBE4c+YMUlNTdf/17dsXL774ou5/s39aFi/VtZKZM2diwoQJ6Nu3L/r374/Vq1ejuLgYkyZNMnZp7U5RUREuXbqke52ZmYnU1FS4uLjA19cXcXFxWLZsGQIDA+Hv748FCxZAqVQiNjbWeEW3A9OmTcPGjRuxfft2ODg46O6/UCgUsLGxgUKhwJQpUzBz5ky4uLjA0dERM2bMQEREBAYMGGDk6tu2uXPnYvTo0fD19UVhYSE2btyIhIQE/Pzzz+wXI3NwcNDdB1jNzs4Orq6uunb2Twsz9mN97clHH30k+Pr6CtbW1kL//v2FY8eOGbukdungwYMCgBr/TZgwQRCEqiEJFixYIHh4eAgymUwYMWKEkJaWZtyi24Ha+gSAsH79et06paWlwtSpUwVnZ2fB1tZW+NOf/iTcvn3beEW3E5MnTxb8/PwEa2trwc3NTRgxYoSwZ88e3XL2i2l5cDgCQWD/tDSJIAiCkTIbERERkVnhPU5EREREIjE4EREREYnE4EREREQkEoMTERERkUgMTkREREQiMTgRERERicTgRERERCQSgxMRERGRSAxORNTmfP/993BxcUFRUZGuTSKRYPr06Q1uu2HDBkgkEly9etUgtc2ZMwfh4eEG2TcRGR6DExG1KRqNBosWLcKMGTNgb29v7HJqiIuLw6lTp7Bjxw5jl0JETcDgRERtys6dO5GWloZXX321Sdv/+c9/RmlpKfz8/Fq4siqenp6IiYnB3//+d4Psn4gMi8GJiMxKcXFxvcvXr1+PQYMGoWPHjk3av4WFBeRyOSQSSZO2F+PZZ5/F4cOHceXKFYO9BxEZBoMTEZmsxYsXQyKR4Pz583jhhRfg7OyMwYMH17l+WVkZdu/ejaioqDrX+eabb9CtWzfI5XKEhYXh0KFDestru8epU6dOGDNmDA4fPoz+/ftDLpejc+fO+Prrr/W2raiowJIlSxAYGAi5XA5XV1cMHjwYe/fu1Vuvur7t27eL/SiIyEQwOBGRyXvmmWdQUlKC5cuX45VXXqlzvZSUFJSXl6NPnz61Lk9MTERcXBxeeuklvPPOO7h79y4ee+wxnD17tsEaLl26hKeffhojR47E+++/D2dnZ0ycOBHnzp3TrbN48WIsWbIEw4YNw8cff4x58+bB19cXJ0+e1NuXQqFAly5d8Ouvv4r8BIjIVFgauwAioob06tULGzdubHC9ixcvAgD8/f1rXX727FmcOHECYWFhAIDx48ejW7duWLhwIX788cd6952WloZDhw5hyJAhAKout/n4+GD9+vW6+5X++9//4vHHH8fnn3/eYK2dO3fG+fPnG1yPiEwLzzgRkcl77bXXRK139+5dAICzs3OtyyMiInShCQB8fX0RExODn3/+GRqNpt59h4SE6EITALi5uaFbt2569yk5OTnh3LlzyMjIaLBWZ2dn3Llzp8H1iMi0MDgRkcmr6wxSXQRBqLU9MDCwRlvXrl1RUlKCvLy8evfp6+tbo83Z2Rn379/XvX7nnXeQn5+Prl27okePHnjrrbdw+vTpOms05A3oRGQYDE5EZPJsbGxErefq6goAemGmpVhYWNTa/mBIe/TRR3H58mV8+eWXCA0NxRdffIE+ffrgiy++qLHd/fv30aFDhxavk4gMi8GJiNqMoKAgAEBmZmaty2u7hJaeng5bW1u4ubm1SA0uLi6YNGkSvv32W9y4cQM9e/bE4sWLa6yXmZmJ4ODgFnlPImo9DE5E1GaEhYXB2toaJ06cqHX50aNH9Z5wu3HjBrZv345Ro0bVeUapMarvsapmb2+PgIAAqNVqvfaCggJcvnwZAwcObPZ7ElHr4lN1RNRmyOVyjBo1Cvv27cM777xTY3loaCiio6Px+uuvQyaT4ZNPPgEALFmypEXePyQkBJGRkQgLC4OLiwtOnDiBLVu21Jgjb9++fRAEATExMS3yvkTUehiciKhNmTx5MsaNG4cbN27Ax8dHb9nQoUMRERGBJUuW4Pr16wgJCcGGDRvQs2fPFnnv119/HTt27MCePXugVqvh5+eHZcuW4a233tJbb/PmzRg8eDC6dOnSIu9LRK1HItT1+AkRkRnSaDQICQnBs88+i6VLlxq7nBqys7Ph7++P7777jmeciMwQgxMRtTmbNm3CX//6V1y/fh329vbGLkfPnDlzcODAARw/ftzYpRBREzA4EREREYnEp+qIiIiIRGJwIiIiIhKJwYmIiIhIJAYnIiIiIpEYnIiIiIhEYnAiIiIiEonBiYiIiEgkBiciIiIikRiciIiIiERicCIiIiISicGJiIiISKT/D9QaBAOh/aXVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved:\n",
      "  - acceptance_rate_vs_threshold.png\n",
      "  - speedup_vs_gamma.png\n"
     ]
    }
   ],
   "source": [
    "# Plot acceptance rate vs relaxation threshold\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(relaxation_thresholds, [r * 100 for r in acceptance_rates], \n",
    "         marker='o', linewidth=2, markersize=8, color='#2E86AB')\n",
    "plt.xlabel('r (bins)', fontsize=12)\n",
    "plt.ylabel(r'$\\alpha$', fontsize=12)\n",
    "plt.title(r'$\\alpha$ vs $r$ (relaxation threshold)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 105)\n",
    "plt.tight_layout()\n",
    "plt.savefig('acceptance_rate_vs_threshold.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot speedup vs gamma\n",
    "fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "color1 = '#A23B72'\n",
    "ax1.set_xlabel(r'$\\gamma$ (draft tokens per round)', fontsize=12)\n",
    "ax1.set_ylabel('speedup wrt target', fontsize=12, color=color1)\n",
    "ax1.plot(gamma_values, speedups, marker='o', linewidth=2, markersize=8, \n",
    "         color=color1, label='Speedup')\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "ax1.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5, label='Baseline (no speedup)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(gamma_values)\n",
    "ax1.set_ylim(0.0, 1.05)\n",
    "\n",
    "# acceptance rate on secondary y-axis\n",
    "ax2 = ax1.twinx()\n",
    "color2 = '#F18F01'\n",
    "ax2.set_ylabel(r'$\\alpha$', fontsize=12, color=color2)\n",
    "ax2.plot(gamma_values, [r * 100 for r in acceptance_rates_gamma], \n",
    "         marker='s', linewidth=2, markersize=8, color=color2, \n",
    "         linestyle='--', label='Acceptance Rate')\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "ax2.set_ylim(0, 105)\n",
    "ax2.legend(loc='lower left')\n",
    "plt.title(r'speedup, $\\alpha$ vs $\\gamma$' + f' (with r={cfg.relaxed_acceptance_r})', fontsize=12)\n",
    "fig.tight_layout()\n",
    "plt.savefig('speedup_vs_gamma.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plots saved:\")\n",
    "print(\"  - acceptance_rate_vs_threshold.png\")\n",
    "print(\"  - speedup_vs_gamma.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBERO Rollout Benchmark (v0, broken for target, always has 0% SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LIBERO rollout benchmark...\n",
      "Configuration: RolloutBenchmarkConfig(task_ids=[0, 1, 2], num_trials_per_task=1, eval_modes=['target'], gamma=7, relaxed_acceptance_r=7, temperature=0.0, task_suite_name='libero_90', num_steps_wait=10, max_episode_steps=None, seed=7)\n",
      "================================================================================\n",
      "LIBERO ROLLOUT BENCHMARK\n",
      "================================================================================\n",
      "Task suite: libero_90\n",
      "Tasks to evaluate: 3 tasks\n",
      "Trials per task: 1\n",
      "Total rollouts: 3\n",
      "Modes to compare: ['target']\n",
      "Gamma: 7, Relaxed r: 7\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Evaluating: TARGET\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1fb2504e6d4f32822fb0a75ab7452e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target tasks:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ROLLOUT BENCHMARK RESULTS\n",
      "================================================================================\n",
      "\n",
      "TARGET:\n",
      "  Success Rate: 0/3 (0.0%)\n",
      "  Throughput: 5.29 Hz (188.9 ms/action)\n",
      "  Total steps: 1200\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PER-TASK SUCCESS RATE BREAKDOWN\n",
      "================================================================================\n",
      "\n",
      "TARGET:\n",
      "  Task  0:   0.0% (0/1) - close the top drawer of the cabinet\n",
      "  Task  1:   0.0% (0/1) - close the top drawer of the cabinet and put the bl...\n",
      "  Task  2:   0.0% (0/1) - put the black bowl in the top drawer of the cabine...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LIBERO Rollout Benchmark\n",
    "# Evaluates speculative decoding through complete rollouts with success rate tracking\n",
    "\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from libero.libero import benchmark\n",
    "\n",
    "from experiments.robot.libero.libero_utils import (\n",
    "    get_libero_dummy_action,\n",
    "    get_libero_env,\n",
    "    get_libero_image,\n",
    "    quat2axisangle,\n",
    ")\n",
    "from experiments.robot.robot_utils import (\n",
    "    invert_gripper_action,\n",
    "    normalize_gripper_action,\n",
    "    set_seed_everywhere,\n",
    ")\n",
    "\n",
    "\n",
    "# Configuration\n",
    "@dataclass\n",
    "class RolloutBenchmarkConfig:\n",
    "    \"\"\"Configuration for LIBERO rollout benchmarking.\"\"\"\n",
    "    \n",
    "    task_ids: Optional[List[int]] = field(default_factory=lambda: [0, 1, 2])  # None = all tasks or list with task ids\n",
    "    #task 2:put_the_black_bowl_in_the_top_drawer_of_the_cabine, had 100% success rate with openvla\n",
    "    \n",
    "    # Number of rollouts (trials) per task\n",
    "    num_trials_per_task: int = 1  # Increase for more reliable success rate estimates\n",
    "    \n",
    "    # what to compare: [\"specdec\"], [\"target\"], [\"specdec\", \"target\"], [\"specdec\", \"target\", \"draft\"]\n",
    "    eval_modes: List[str] = field(default_factory=lambda: [\"target\"])\n",
    "    \n",
    "    # Speculative decoding parameters (for specdec mode)\n",
    "    gamma: int = 7\n",
    "    relaxed_acceptance_r: int = 7\n",
    "    temperature: float = 0.0\n",
    "    \n",
    "    # Environment parameters\n",
    "    task_suite_name: str = \"libero_90\"\n",
    "    num_steps_wait: int = 10  # Steps to wait for sim stabilization\n",
    "    max_episode_steps: Optional[int] = None  # None = use default for suite\n",
    "    \n",
    "    # Random seed\n",
    "    seed: int = 7\n",
    "    \n",
    "rollout_cfg = RolloutBenchmarkConfig()\n",
    "\n",
    "def get_max_steps(task_suite_name: str) -> int:\n",
    "    \"\"\"Get maximum episode steps for a task suite.\"\"\"\n",
    "    max_steps_dict = {\n",
    "        \"libero_spatial\": 220,\n",
    "        \"libero_object\": 280,\n",
    "        \"libero_goal\": 300,\n",
    "        \"libero_10\": 520,\n",
    "        \"libero_90\": 400,\n",
    "    }\n",
    "    return max_steps_dict.get(task_suite_name, 400)\n",
    "\n",
    "@dataclass\n",
    "class RolloutResults:\n",
    "    \"\"\"Results from rollout evaluation.\"\"\"\n",
    "    mode: str\n",
    "    task_id: int\n",
    "    task_description: str\n",
    "    trial_idx: int\n",
    "    success: bool\n",
    "    num_steps: int\n",
    "    inference_times: List[float]\n",
    "    # Specdec-specific\n",
    "    acceptance_rate: Optional[float] = None\n",
    "    tokens_per_target_forward: Optional[float] = None\n",
    "    total_tokens_generated: int = 0\n",
    "    total_draft_tokens_proposed: int = 0\n",
    "    total_draft_tokens_accepted: int = 0\n",
    "    total_target_forward_passes: int = 0\n",
    "    total_draft_forward_passes: int = 0\n",
    "\n",
    "def run_single_rollout(\n",
    "    env,\n",
    "    task_description: str,\n",
    "    initial_state,\n",
    "    mode: str,\n",
    "    rollout_cfg: RolloutBenchmarkConfig,\n",
    ") -> RolloutResults:\n",
    "    env.reset()\n",
    "    obs = env.set_init_state(initial_state)\n",
    "    \n",
    "    max_steps = rollout_cfg.max_episode_steps or get_max_steps(rollout_cfg.task_suite_name)\n",
    "    resize_size = 224\n",
    "    \n",
    "    t = 0\n",
    "    inference_times = []\n",
    "    \n",
    "    # For specdec stats\n",
    "    total_tokens_generated = 0\n",
    "    total_draft_tokens_proposed = 0\n",
    "    total_draft_tokens_accepted = 0\n",
    "    total_target_forward_passes = 0\n",
    "    total_draft_forward_passes = 0\n",
    "    \n",
    "    while t < max_steps + rollout_cfg.num_steps_wait:\n",
    "        # Wait for sim stabilization\n",
    "        if t < rollout_cfg.num_steps_wait:\n",
    "            obs, reward, done, info = env.step(get_libero_dummy_action(\"openvla\"))\n",
    "            t += 1\n",
    "            continue\n",
    "        \n",
    "        # Get image observation\n",
    "        img = get_libero_image(obs, resize_size)\n",
    "        observation = {\n",
    "            \"full_image\": img,\n",
    "            \"state\": np.concatenate(\n",
    "                (obs[\"robot0_eef_pos\"], quat2axisangle(obs[\"robot0_eef_quat\"]), obs[\"robot0_gripper_qpos\"])\n",
    "            ),\n",
    "        }\n",
    "        \n",
    "        # Get action based on mode\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if mode == \"specdec\":\n",
    "            pil_img = prepare_image(observation[\"full_image\"], center_crop=cfg.center_crop)\n",
    "            action, stats = specdec_decoder.predict_action_speculative(\n",
    "                pil_img, task_description, unnorm_key_target\n",
    "            )\n",
    "            total_tokens_generated += stats.total_tokens_generated\n",
    "            total_draft_tokens_proposed += stats.total_draft_tokens_proposed\n",
    "            total_draft_tokens_accepted += stats.total_draft_tokens_accepted\n",
    "            total_target_forward_passes += stats.total_target_forward_passes\n",
    "            total_draft_forward_passes += stats.total_draft_forward_passes\n",
    "            \n",
    "        elif mode == \"target\":\n",
    "            action = run_target_inference_obs(observation, task_description)\n",
    "            \n",
    "        elif mode == \"draft\":\n",
    "            action = run_draft_inference_obs(observation, task_description)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        inference_times.append(time.time() - start_time)\n",
    "        \n",
    "        # Post-process action\n",
    "        action = normalize_gripper_action(action, binarize=True)\n",
    "        action = invert_gripper_action(action)\n",
    "        \n",
    "        # Step environment\n",
    "        obs, reward, done, info = env.step(action.tolist())\n",
    "        \n",
    "        if done:\n",
    "            # Success!\n",
    "            acceptance_rate = None\n",
    "            tokens_per_fwd = None\n",
    "            if mode == \"specdec\" and total_draft_tokens_proposed > 0:\n",
    "                acceptance_rate = total_draft_tokens_accepted / total_draft_tokens_proposed\n",
    "            if mode == \"specdec\" and total_target_forward_passes > 0:\n",
    "                tokens_per_fwd = total_tokens_generated / total_target_forward_passes\n",
    "            \n",
    "            return RolloutResults(\n",
    "                mode=mode,\n",
    "                task_id=-1,  # Will be set by caller\n",
    "                task_description=task_description,\n",
    "                trial_idx=-1,  # Will be set by caller\n",
    "                success=True,\n",
    "                num_steps=t - rollout_cfg.num_steps_wait + 1,\n",
    "                inference_times=inference_times,\n",
    "                acceptance_rate=acceptance_rate,\n",
    "                tokens_per_target_forward=tokens_per_fwd,\n",
    "                total_tokens_generated=total_tokens_generated,\n",
    "                total_draft_tokens_proposed=total_draft_tokens_proposed,\n",
    "                total_draft_tokens_accepted=total_draft_tokens_accepted,\n",
    "                total_target_forward_passes=total_target_forward_passes,\n",
    "                total_draft_forward_passes=total_draft_forward_passes,\n",
    "            )\n",
    "        \n",
    "        t += 1\n",
    "    \n",
    "    # ended without success\n",
    "    acceptance_rate = None\n",
    "    tokens_per_fwd = None\n",
    "    if mode == \"specdec\" and total_draft_tokens_proposed > 0:\n",
    "        acceptance_rate = total_draft_tokens_accepted / total_draft_tokens_proposed\n",
    "    if mode == \"specdec\" and total_target_forward_passes > 0:\n",
    "        tokens_per_fwd = total_tokens_generated / total_target_forward_passes\n",
    "    \n",
    "    return RolloutResults(\n",
    "        mode=mode,\n",
    "        task_id=-1,\n",
    "        task_description=task_description,\n",
    "        trial_idx=-1,\n",
    "        success=False,\n",
    "        num_steps=max_steps,\n",
    "        inference_times=inference_times,\n",
    "        acceptance_rate=acceptance_rate,\n",
    "        tokens_per_target_forward=tokens_per_fwd,\n",
    "        total_tokens_generated=total_tokens_generated,\n",
    "        total_draft_tokens_proposed=total_draft_tokens_proposed,\n",
    "        total_draft_tokens_accepted=total_draft_tokens_accepted,\n",
    "        total_target_forward_passes=total_target_forward_passes,\n",
    "        total_draft_forward_passes=total_draft_forward_passes,\n",
    "    )\n",
    "\n",
    "def run_target_inference_obs(observation, task_description):\n",
    "    \"\"\"Run target model inference on observation.\"\"\"\n",
    "    from experiments.robot.openvla_utils import get_vla_action\n",
    "    return get_vla_action(\n",
    "        target_model,\n",
    "        target_processor,\n",
    "        str(cfg.target_checkpoint),\n",
    "        observation,\n",
    "        task_description,\n",
    "        unnorm_key_target,\n",
    "        center_crop=cfg.center_crop,\n",
    "    )\n",
    "\n",
    "def run_draft_inference_obs(observation, task_description):\n",
    "    \"\"\"Run draft model inference on observation.\"\"\"\n",
    "    from experiments.robot.openvla_utils import get_prismatic_vla_action\n",
    "    return get_prismatic_vla_action(\n",
    "        draft_model,\n",
    "        None,\n",
    "        str(cfg.draft_checkpoint),\n",
    "        observation,\n",
    "        task_description,\n",
    "        unnorm_key_draft,\n",
    "        center_crop=cfg.center_crop,\n",
    "    )\n",
    "\n",
    "# Main benchmark function\n",
    "def run_libero_rollout_benchmark(rollout_cfg: RolloutBenchmarkConfig) -> dict:\n",
    "    \"\"\"\n",
    "    Run comprehensive LIBERO rollout benchmark.\n",
    "    \n",
    "    Returns dict with results for each mode.\n",
    "    \"\"\"\n",
    "    set_seed_everywhere(rollout_cfg.seed)\n",
    "    \n",
    "    global specdec_decoder\n",
    "    specdec_decoder = VLASpeculativeDecoderBatchedLM(\n",
    "        target_model=target_model,\n",
    "        draft_model=draft_model,\n",
    "        target_processor=target_processor,\n",
    "        gamma=rollout_cfg.gamma,\n",
    "        temperature=rollout_cfg.temperature,\n",
    "        relaxed_acceptance_r=rollout_cfg.relaxed_acceptance_r,\n",
    "    )\n",
    "    \n",
    "    # Load task suite\n",
    "    task_suite = benchmark.get_benchmark_dict()[rollout_cfg.task_suite_name]()\n",
    "    \n",
    "    # Determine task IDs\n",
    "    task_ids = list(range(task_suite.n_tasks)) if rollout_cfg.task_ids is None else rollout_cfg.task_ids\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"LIBERO ROLLOUT BENCHMARK\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Task suite: {rollout_cfg.task_suite_name}\")\n",
    "    print(f\"Tasks to evaluate: {len(task_ids)} tasks\")\n",
    "    print(f\"Trials per task: {rollout_cfg.num_trials_per_task}\")\n",
    "    print(f\"Total rollouts: {len(task_ids) * rollout_cfg.num_trials_per_task}\")\n",
    "    print(f\"Modes to compare: {rollout_cfg.eval_modes}\")\n",
    "    print(f\"Gamma: {rollout_cfg.gamma}, Relaxed r: {rollout_cfg.relaxed_acceptance_r}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_results = {mode: [] for mode in rollout_cfg.eval_modes}\n",
    "    \n",
    "    for mode in rollout_cfg.eval_modes:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Evaluating: {mode.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for task_id in tqdm(task_ids, desc=f\"{mode} tasks\"):\n",
    "            task = task_suite.get_task(task_id)\n",
    "            initial_states = task_suite.get_task_init_states(task_id)\n",
    "            env, task_description = get_libero_env(task, \"openvla\", resolution=224)\n",
    "            \n",
    "            num_trials = min(rollout_cfg.num_trials_per_task, len(initial_states))\n",
    "            \n",
    "            for trial_idx in range(num_trials):\n",
    "                result = run_single_rollout(\n",
    "                    env,\n",
    "                    task_description,\n",
    "                    initial_states[trial_idx],\n",
    "                    mode,\n",
    "                    rollout_cfg,\n",
    "                )\n",
    "                result.task_id = task_id\n",
    "                result.trial_idx = trial_idx\n",
    "                all_results[mode].append(result)\n",
    "            \n",
    "            env.close()\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Results analysis and printing\n",
    "\n",
    "def analyze_and_print_results(all_results: dict):\n",
    "    \"\"\"Analyze and print comprehensive results.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ROLLOUT BENCHMARK RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    summary = {}\n",
    "    \n",
    "    for mode, results in all_results.items():\n",
    "        if not results:\n",
    "            continue\n",
    "            \n",
    "        # Aggregate metrics\n",
    "        total_episodes = len(results)\n",
    "        total_successes = sum(1 for r in results if r.success)\n",
    "        total_steps = sum(r.num_steps for r in results)\n",
    "        total_inference_time = sum(sum(r.inference_times) for r in results)\n",
    "        \n",
    "        success_rate = total_successes / total_episodes * 100\n",
    "        avg_time_per_action = total_inference_time / total_steps * 1000  # ms\n",
    "        throughput_hz = total_steps / total_inference_time\n",
    "        \n",
    "        summary[mode] = {\n",
    "            \"success_rate\": success_rate,\n",
    "            \"throughput_hz\": throughput_hz,\n",
    "            \"avg_time_ms\": avg_time_per_action,\n",
    "            \"total_episodes\": total_episodes,\n",
    "            \"total_successes\": total_successes,\n",
    "            \"total_steps\": total_steps,\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{mode.upper()}:\")\n",
    "        print(f\"  Success Rate: {total_successes}/{total_episodes} ({success_rate:.1f}%)\")\n",
    "        print(f\"  Throughput: {throughput_hz:.2f} Hz ({avg_time_per_action:.1f} ms/action)\")\n",
    "        print(f\"  Total steps: {total_steps}\")\n",
    "        \n",
    "        # Specdec-specific metrics\n",
    "        if mode == \"specdec\":\n",
    "            acceptance_rates = [r.acceptance_rate for r in results if r.acceptance_rate is not None]\n",
    "            tokens_per_fwd = [r.tokens_per_target_forward for r in results if r.tokens_per_target_forward is not None]\n",
    "            \n",
    "            total_tokens = sum(r.total_tokens_generated for r in results)\n",
    "            total_proposed = sum(r.total_draft_tokens_proposed for r in results)\n",
    "            total_accepted = sum(r.total_draft_tokens_accepted for r in results)\n",
    "            total_target_fwd = sum(r.total_target_forward_passes for r in results)\n",
    "            total_draft_fwd = sum(r.total_draft_forward_passes for r in results)\n",
    "            \n",
    "            overall_acceptance = total_accepted / total_proposed if total_proposed > 0 else 0\n",
    "            overall_tokens_per_fwd = total_tokens / total_target_fwd if total_target_fwd > 0 else 0\n",
    "            \n",
    "            summary[mode][\"acceptance_rate\"] = overall_acceptance\n",
    "            summary[mode][\"tokens_per_target_forward\"] = overall_tokens_per_fwd\n",
    "            \n",
    "            print(f\"  Overall Acceptance Rate: {overall_acceptance:.2%}\")\n",
    "            print(f\"  Tokens per Target Forward: {overall_tokens_per_fwd:.2f}\")\n",
    "            print(f\"  Total Target Forward Passes: {total_target_fwd}\")\n",
    "            print(f\"  Total Draft Forward Passes: {total_draft_fwd}\")\n",
    "            \n",
    "            if acceptance_rates:\n",
    "                print(f\"  Per-episode Acceptance Rate: {np.mean(acceptance_rates):.2%}  {np.std(acceptance_rates):.2%}\")\n",
    "    \n",
    "    # Compute speedups\n",
    "    if \"target\" in summary and \"specdec\" in summary:\n",
    "        speedup = summary[\"specdec\"][\"throughput_hz\"] / summary[\"target\"][\"throughput_hz\"]\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SPEEDUP (SpecDec vs Target): {speedup:.2f}x\")\n",
    "        \n",
    "        # Check accuracy preservation\n",
    "        sr_diff = summary[\"specdec\"][\"success_rate\"] - summary[\"target\"][\"success_rate\"]\n",
    "        print(f\"Success Rate Difference: {sr_diff:+.1f}% (specdec - target)\")\n",
    "        \n",
    "        if abs(sr_diff) < 5:\n",
    "            print(\" Accuracy preserved (difference < 5%)\")\n",
    "        elif sr_diff < 0:\n",
    "            print(\" Warning: SpecDec success rate is lower\")\n",
    "        else:\n",
    "            print(\" SpecDec success rate is higher\")\n",
    "    \n",
    "    if \"draft\" in summary:\n",
    "        draft_speedup = summary[\"draft\"][\"throughput_hz\"] / summary.get(\"target\", summary[\"draft\"])[\"throughput_hz\"]\n",
    "        print(f\"SPEEDUP (Draft vs Target): {draft_speedup:.2f}x\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Run the benchmark\n",
    "print(\"Starting LIBERO rollout benchmark...\")\n",
    "print(f\"Configuration: {rollout_cfg}\")\n",
    "\n",
    "rollout_results = run_libero_rollout_benchmark(rollout_cfg)\n",
    "summary = analyze_and_print_results(rollout_results)\n",
    "\n",
    "\n",
    "def print_per_task_breakdown(all_results: dict):\n",
    "    \"\"\"Print success rate breakdown by task.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PER-TASK SUCCESS RATE BREAKDOWN\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for mode in all_results.keys():\n",
    "        results = all_results[mode]\n",
    "        if not results: continue\n",
    "        \n",
    "        # Group by task_id\n",
    "        tasks = {}\n",
    "        for r in results:\n",
    "            if r.task_id not in tasks:\n",
    "                tasks[r.task_id] = {\"successes\": 0, \"total\": 0, \"description\": r.task_description}\n",
    "            tasks[r.task_id][\"total\"] += 1\n",
    "            if r.success:\n",
    "                tasks[r.task_id][\"successes\"] += 1\n",
    "        \n",
    "        print(f\"\\n{mode.upper()}:\")\n",
    "        for task_id in sorted(tasks.keys()):\n",
    "            t = tasks[task_id]\n",
    "            sr = t[\"successes\"] / t[\"total\"] * 100\n",
    "            desc = t[\"description\"][:50] + \"...\" if len(t[\"description\"]) > 50 else t[\"description\"]\n",
    "            print(f\"  Task {task_id:2d}: {sr:5.1f}% ({t['successes']}/{t['total']}) - {desc}\")\n",
    "\n",
    "# Uncomment to see per-task breakdown:\n",
    "print_per_task_breakdown(rollout_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvla1311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
