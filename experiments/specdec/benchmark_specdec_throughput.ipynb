{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 09:20:14.966751: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-15 09:20:15.134131: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-15 09:20:15.134192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-15 09:20:15.139250: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-15 09:20:15.196036: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-15 09:20:17.391236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "/mnt/scratch/aagouzoul/miniconda3/envs/mvla1311/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2025-12-15 09:20:33.999072: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2348] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 15 09:20:42 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 NVL                On  |   00000000:81:00.0 Off |                    0 |\n",
      "| N/A   43C    P0             73W /  400W |       4MiB /  95830MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Benchmarks inference throughput on a single LIBERO task observation\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['PRISMATIC_DATA_ROOT'] = ''\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "import draccus\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "# sys.path.insert(0, str(Path(__file__).resolve().parents[3]))\n",
    "# sys.path.insert(0, str(Path().resolve().parents[1]))\n",
    "sys.path.append(\"../..\")\n",
    "from libero.libero import benchmark\n",
    "\n",
    "from experiments.robot.libero.libero_utils import get_libero_env, get_libero_image, quat2axisangle\n",
    "from experiments.robot.openvla_utils import get_processor, get_vla, get_prismatic_vla\n",
    "from experiments.robot.robot_utils import get_image_resize_size, set_seed_everywhere\n",
    "\n",
    "assert torch.cuda.is_available(), \"ERROR: CUDA not available!\"\n",
    "\n",
    "os.system(\"nvidia-smi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Speculative Decoding Throughput Benchmark\n",
      "================================================================================\n",
      "Target: /pub/scratch/aagouzoul/ovla/openvla-mini/ft_experiments_logs/openvla-7b+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug_step-75000_l1-loss-0.0012_tokacc-0.955\n",
      "Draft: Stanford-ILIAD/minivla-libero90-prismatic\n",
      "Gamma: 7\n",
      "Iterations: 1\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class BenchmarkConfig:\n",
    "    # fmt: off\n",
    "    # target_checkpoint: Union[str, Path] = \"/pub/scratch/aagouzoul/ovla/openvla-mini/ft_experiments_logs/openvla-7b+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug\"\n",
    "    target_checkpoint: Union[str, Path] = \"/pub/scratch/aagouzoul/ovla/openvla-mini/ft_experiments_logs/openvla-7b+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug_step-75000_l1-loss-0.0012_tokacc-0.955\"\n",
    "    draft_checkpoint: Union[str, Path] = \"Stanford-ILIAD/minivla-libero90-prismatic\"\n",
    "    hf_token: str = Path(\"/pub/scratch/aagouzoul/ovla/openvla-mini/.hf_token\")\n",
    "    load_in_8bit: bool = False\n",
    "    load_in_4bit: bool = False\n",
    "    center_crop: bool = True\n",
    "    \n",
    "    # Speculative decoding parameters\n",
    "    gamma: int = 7\n",
    "    temperature: float = 0.0\n",
    "    \n",
    "    # Benchmark parameters\n",
    "    task_suite_name: str = \"libero_90\"\n",
    "    task_id: int = 0\n",
    "    num_iterations: int = 1\n",
    "    warmup_iterations: int = 3\n",
    "    seed: int = 42\n",
    "    \n",
    "    # fmt: on\n",
    "    \n",
    "cfg = BenchmarkConfig()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Speculative Decoding Throughput Benchmark\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Target: {cfg.target_checkpoint}\")\n",
    "print(f\"Draft: {cfg.draft_checkpoint}\")\n",
    "print(f\"Gamma: {cfg.gamma}\")\n",
    "print(f\"Iterations: {cfg.num_iterations}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "set_seed_everywhere(cfg.seed)\n",
    "\n",
    "# Create config objects for model loading\n",
    "class TargetConfig:\n",
    "    def __init__(self, c):\n",
    "        self.pretrained_checkpoint = c.target_checkpoint\n",
    "        self.load_in_8bit = c.load_in_8bit\n",
    "        self.load_in_4bit = c.load_in_4bit\n",
    "        self.hf_token = c.hf_token\n",
    "\n",
    "class DraftConfig:\n",
    "    def __init__(self, c):\n",
    "        self.pretrained_checkpoint = c.draft_checkpoint\n",
    "        self.model_family = \"prismatic\"\n",
    "        self.hf_token = c.hf_token\n",
    "        self.center_crop = c.center_crop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/4] Loading TARGET model (OpenVLA)...\n",
      "[*] Instantiating Pretrained VLA model\n",
      "[*] Loading in BF16 with Flash-Attention Enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698e137504cd4ccfbb20ca8d2b3fa339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n[1/4] Loading TARGET model (OpenVLA)...\")\n",
    "target_cfg = TargetConfig(cfg)\n",
    "target_model = get_vla(target_cfg)\n",
    "target_processor = get_processor(target_cfg)\n",
    "\n",
    "print(\"\\n[2/4] Loading DRAFT model (MiniVLA)...\")\n",
    "draft_cfg = DraftConfig(cfg)\n",
    "draft_model = get_prismatic_vla(draft_cfg)\n",
    "\n",
    "# Set unnorm key\n",
    "unnorm_key_target = cfg.task_suite_name\n",
    "if unnorm_key_target not in target_model.norm_stats:\n",
    "    if f\"{unnorm_key_target}_no_noops\" in target_model.norm_stats:\n",
    "        unnorm_key_target = f\"{unnorm_key_target}_no_noops\"\n",
    "    elif f\"{unnorm_key_target.replace('_no_noops', '')}\" in target_model.norm_stats:\n",
    "        unnorm_key_target = f\"{unnorm_key_target}\"\n",
    "    else:\n",
    "        unnorm_key_target = list(target_model.norm_stats.keys())[0]\n",
    "        \n",
    "unnorm_key_draft = cfg.task_suite_name\n",
    "if unnorm_key_draft not in draft_model.norm_stats:\n",
    "    if f\"{unnorm_key_draft}_no_noops\" in draft_model.norm_stats:\n",
    "        unnorm_key_draft = f\"{unnorm_key_draft}_no_noops\"\n",
    "    elif f\"{unnorm_key_draft.replace('_no_noops', '')}\" in draft_model.norm_stats:\n",
    "        unnorm_key_draft = f\"{unnorm_key_draft}\"\n",
    "    else:\n",
    "        unnorm_key_draft = list(draft_model.norm_stats.keys())[0]\n",
    "\n",
    "print(f\"Using unnorm_keys: {unnorm_key_target} (target), {unnorm_key_draft} (draft)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/4] Loading LIBERO task: libero_90 (task 0)...\n",
      "Task: close the top drawer of the cabinet\n",
      "Image shape: (224, 224, 3)\n",
      "\u001b[38;2;255;165;0m[SRP] -> \u001b[0m call params: get_vla_action(target_model..., target_processor..., cfg.target_checkpoint=/pub/scratch/aagouzoul/ovla/openvla-mini/ft_experiments_logs/openvla-7b+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug+libero_90_no_noops+b32+lr-0.0005+lora-r32+dropout-0.0--image_aug_step-75000_l1-loss-0.0012_tokacc-0.955, observation=..., task_description=close the top drawer of the cabinet, unnorm_key=libero_90_no_noops, center_crop=True)\n",
      "\u001b[38;2;255;165;0m[SRP] -> \u001b[0m call params: get_prismatic_vla_action(draft_model..., processor=None, cfg.draft_checkpoint=Stanford-ILIAD/minivla-libero90-prismatic, observation..., task_description=close the top drawer of the cabinet, unnorm_key=libero_90, center_crop=True)\n",
      "\n",
      "Running warmup (3 iterations each)...\n",
      "  Warming up TARGET...\n",
      "  Warming up DRAFT...\n",
      "\n",
      "Benchmarking TARGET (1 iterations)...\n",
      "\n",
      "Benchmarking DRAFT (1 iterations)...\n"
     ]
    }
   ],
   "source": [
    "# Load LIBERO task and get observation\n",
    "print(f\"\\n[4/4] Loading LIBERO task: {cfg.task_suite_name} (task {cfg.task_id})...\")\n",
    "benchmark_dict = benchmark.get_benchmark_dict()\n",
    "task_suite = benchmark_dict[cfg.task_suite_name]()\n",
    "task = task_suite.get_task(cfg.task_id)\n",
    "env, task_description = get_libero_env(task, \"openvla\", resolution=224)\n",
    "\n",
    "initial_states = task_suite.get_task_init_states(cfg.task_id)\n",
    "env.reset()\n",
    "obs = env.set_init_state(initial_states[0])\n",
    "\n",
    "# Prepare observation\n",
    "img = get_libero_image(obs, 224)\n",
    "observation = {\n",
    "    \"full_image\": img,\n",
    "    \"state\": np.concatenate(\n",
    "        (obs[\"robot0_eef_pos\"], quat2axisangle(obs[\"robot0_eef_quat\"]), obs[\"robot0_gripper_qpos\"])\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"Task: {task_description}\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"call params: get_vla_action(target_model..., target_processor..., cfg.target_checkpoint={cfg.target_checkpoint}, observation=..., task_description={task_description}, unnorm_key={unnorm_key_target}, center_crop={cfg.center_crop})\")\n",
    "print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"call params: get_prismatic_vla_action(draft_model..., processor=None, cfg.draft_checkpoint={cfg.draft_checkpoint}, observation..., task_description={task_description}, unnorm_key={unnorm_key_draft}, center_crop={cfg.center_crop})\")\n",
    "\n",
    "def run_target_inference():\n",
    "    from experiments.robot.openvla_utils import get_vla_action\n",
    "    return get_vla_action(\n",
    "        target_model,\n",
    "        target_processor,\n",
    "        str(cfg.target_checkpoint),\n",
    "        observation,\n",
    "        task_description,\n",
    "        unnorm_key_target,\n",
    "        center_crop=cfg.center_crop,\n",
    "    )\n",
    "\n",
    "def run_draft_inference():\n",
    "    from experiments.robot.openvla_utils import get_prismatic_vla_action    \n",
    "    return get_prismatic_vla_action(\n",
    "        draft_model,\n",
    "        None,\n",
    "        str(cfg.draft_checkpoint),\n",
    "        observation,\n",
    "        task_description,\n",
    "        unnorm_key_draft,\n",
    "        center_crop=cfg.center_crop,\n",
    "    )\n",
    "\n",
    "def timed_cuda(fn):\n",
    "    \"\"\"Time a function using CUDA events for accurate GPU timing.\"\"\"\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end) / 1000  # Convert to seconds\n",
    "\n",
    "print(f\"\\nRunning warmup ({cfg.warmup_iterations} iterations each)...\")\n",
    "\n",
    "print(\"  Warming up TARGET...\")\n",
    "for _ in range(cfg.warmup_iterations):\n",
    "    run_target_inference()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(\"  Warming up DRAFT...\")\n",
    "for _ in range(cfg.warmup_iterations):\n",
    "    run_draft_inference()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "# Benchmark TARGET\n",
    "print(f\"\\nBenchmarking TARGET ({cfg.num_iterations} iterations)...\")\n",
    "target_times = []\n",
    "for i in range(cfg.num_iterations):\n",
    "    result, dt = timed_cuda(run_target_inference)\n",
    "    target_times.append(dt)\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Progress: {i+1}/{cfg.num_iterations}, last: {dt*1000:.1f}ms\")\n",
    "\n",
    "# Benchmark DRAFT\n",
    "print(f\"\\nBenchmarking DRAFT ({cfg.num_iterations} iterations)...\")\n",
    "draft_times = []\n",
    "for i in range(cfg.num_iterations):\n",
    "    result, dt = timed_cuda(run_draft_inference)\n",
    "    draft_times.append(dt)\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Progress: {i+1}/{cfg.num_iterations}, last: {dt*1000:.1f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load vla_speculative_decoding.py\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "# ============================================================================\n",
    "# Cache utilities for KV cache pruning\n",
    "# ============================================================================\n",
    "def prune_cache(\n",
    "    cache: Union[Tuple[Tuple[torch.Tensor, torch.Tensor]], DynamicCache, None],\n",
    "    num_tokens_to_discard: int,\n",
    ") -> Union[Tuple[Tuple[torch.Tensor, torch.Tensor]], DynamicCache, None]:\n",
    "    \"\"\"Prune the KV cache by removing tokens from the end.\"\"\"\n",
    "    if cache is None or num_tokens_to_discard <= 0:\n",
    "        return cache\n",
    "    \n",
    "    if isinstance(cache, DynamicCache):\n",
    "        for layer in range(len(cache)):\n",
    "            cache.key_cache[layer] = cache.key_cache[layer][:, :, :-num_tokens_to_discard, :]\n",
    "            cache.value_cache[layer] = cache.value_cache[layer][:, :, :-num_tokens_to_discard, :]\n",
    "        cache._seen_tokens -= num_tokens_to_discard\n",
    "        return cache\n",
    "    \n",
    "    elif isinstance(cache, tuple):\n",
    "        new_cache = []\n",
    "        for layer_cache in cache:\n",
    "            if layer_cache is None:\n",
    "                new_cache.append(None)\n",
    "                continue\n",
    "            layer = []\n",
    "            for tensor in layer_cache:\n",
    "                new_tensor = tensor[:, :, :-num_tokens_to_discard, :]\n",
    "                layer.append(new_tensor)\n",
    "            new_cache.append(tuple(layer))\n",
    "        return tuple(new_cache)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported cache type: {type(cache)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Image preprocessing utilities\n",
    "# ============================================================================\n",
    "def apply_center_crop(im: np.ndarray, t_h: int, t_w: int) -> np.ndarray:\n",
    "    \"\"\"Center crop an image to target dimensions.\"\"\"\n",
    "    assert im.shape[-3] >= t_h and im.shape[-2] >= t_w\n",
    "    crop_h = int((im.shape[-3] - t_h) / 2)\n",
    "    crop_w = int((im.shape[-2] - t_w) / 2)\n",
    "    return im[..., crop_h : crop_h + t_h, crop_w : crop_w + t_w, :]\n",
    "\n",
    "def prepare_image(full_image: Union[np.ndarray, List[np.ndarray]], center_crop: bool = False) -> Image.Image:\n",
    "    \"\"\"Convert numpy image to PIL Image with optional center crop.\"\"\"\n",
    "    if isinstance(full_image, list):\n",
    "        full_image = full_image[0]\n",
    "    \n",
    "    image = Image.fromarray(full_image).convert(\"RGB\")\n",
    "    \n",
    "    if center_crop:\n",
    "        temp_image = np.array(image)\n",
    "        crop_scale = 0.9\n",
    "        sqrt_crop_scale = math.sqrt(crop_scale)\n",
    "        temp_image_cropped = apply_center_crop(\n",
    "            temp_image,\n",
    "            t_h=int(sqrt_crop_scale * temp_image.shape[0]),\n",
    "            t_w=int(sqrt_crop_scale * temp_image.shape[1]),\n",
    "        )\n",
    "        image = Image.fromarray(temp_image_cropped)\n",
    "        image = image.resize((224, 224), Image.Resampling.BILINEAR)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# ============================================================================\n",
    "# Speculative decoding core implementation\n",
    "# ============================================================================\n",
    "@dataclass\n",
    "class SpeculativeDecodingStats:\n",
    "    \"\"\"Statistics from speculative decoding run.\"\"\"\n",
    "    total_tokens_generated: int = 0\n",
    "    total_draft_tokens_proposed: int = 0\n",
    "    total_draft_tokens_accepted: int = 0\n",
    "    total_target_forward_passes: int = 0\n",
    "    total_draft_forward_passes: int = 0\n",
    "    \n",
    "    @property\n",
    "    def acceptance_rate(self) -> float:\n",
    "        if self.total_draft_tokens_proposed == 0:\n",
    "            return 0.0\n",
    "        return self.total_draft_tokens_accepted / self.total_draft_tokens_proposed\n",
    "    \n",
    "    @property\n",
    "    def tokens_per_target_forward(self) -> float:\n",
    "        if self.total_target_forward_passes == 0:\n",
    "            return 0.0\n",
    "        return self.total_tokens_generated / self.total_target_forward_passes\n",
    "\n",
    "def max_fn(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Normalize max(0, x) to create a valid probability distribution.\"\"\"\n",
    "    x_max = torch.where(x > 0, x, torch.zeros_like(x))\n",
    "    x_max_sum = torch.sum(x_max, dim=-1, keepdim=True)\n",
    "    # Avoid division by zero\n",
    "    return x_max / (x_max_sum + 1e-10)\n",
    "\n",
    "class VLASpeculativeDecoder:\n",
    "    \"\"\"\n",
    "    Speculative decoding for VLA models.\n",
    "    \n",
    "    Uses a draft model (MiniVLA) to propose action tokens and a target model \n",
    "    (OpenVLA) to verify them.\n",
    "    \n",
    "    IMPORTANT: For speculative decoding to work correctly, both models should\n",
    "    share the same tokenizer/vocabulary. If they don't, action token remapping\n",
    "    is attempted but this only works for action tokens (last 256 tokens of vocab).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model,\n",
    "        draft_model,\n",
    "        target_processor=None,\n",
    "        gamma: int = 4,  # Number of draft tokens to propose at once\n",
    "        use_cache: bool = True,\n",
    "        temperature: float = 0.0,  # 0 = greedy/argmax\n",
    "        n_action_bins: int = 256,  # Number of action bins (tokens)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the speculative decoder.\n",
    "        \n",
    "        Args:\n",
    "            target_model: OpenVLA model (larger, slower, more accurate)\n",
    "            draft_model: MiniVLA model (smaller, faster)\n",
    "            target_processor: HuggingFace processor for target model\n",
    "            gamma: Number of tokens to speculate at each step\n",
    "            use_cache: Whether to use KV caching\n",
    "            temperature: Sampling temperature (0 = greedy/argmax)\n",
    "            n_action_bins: Number of action token bins (typically 256)\n",
    "        \"\"\"\n",
    "        self.target = target_model\n",
    "        self.draft = draft_model\n",
    "        self.target_processor = target_processor\n",
    "        self.gamma = gamma\n",
    "        self.use_cache = use_cache\n",
    "        self.temperature = temperature\n",
    "        self.n_action_bins = n_action_bins\n",
    "        \n",
    "        # Get device\n",
    "        self.device = next(target_model.parameters()).device\n",
    "        \n",
    "        # Stats tracking\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "        \n",
    "        # Check vocabulary compatibility and setup token mapping\n",
    "        self._setup_token_mapping()\n",
    "    \n",
    "    def _setup_token_mapping(self):\n",
    "        \"\"\"Setup token mapping between draft and target vocabularies.\"\"\"\n",
    "        # Get vocabulary sizes\n",
    "        # Target model (OpenVLA/HF style) - use ACTUAL embedding dimension, not vocab_size attribute\n",
    "        # The embedding may be padded to \"multiple of\" for efficiency\n",
    "        if hasattr(self.target, 'language_model') and hasattr(self.target.language_model, 'model'):\n",
    "            # Actual embedding dimension (includes padding)\n",
    "            self.target_logit_dim = self.target.language_model.model.embed_tokens.weight.shape[0]\n",
    "        elif hasattr(self.target, 'get_output_embeddings'):\n",
    "            self.target_logit_dim = self.target.get_output_embeddings().weight.shape[0]\n",
    "        else:\n",
    "            self.target_logit_dim = self.target.config.vocab_size\n",
    "        \n",
    "        # Also get the \"logical\" vocab size (without padding) for action token calculation\n",
    "        if hasattr(self.target, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.vocab_size\n",
    "        elif hasattr(self.target, 'config') and hasattr(self.target.config, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.config.vocab_size\n",
    "        else:\n",
    "            self.target_vocab_size = self.target_logit_dim\n",
    "        \n",
    "        # Draft model (Prismatic style)\n",
    "        if hasattr(self.draft, 'llm_backbone'):\n",
    "            draft_tokenizer = self.draft.llm_backbone.tokenizer\n",
    "            # Qwen2 uses len(tokenizer) for full vocab including added tokens\n",
    "            self.draft_vocab_size = len(draft_tokenizer) if hasattr(draft_tokenizer, '__len__') else draft_tokenizer.vocab_size\n",
    "            # Get actual logit dimension from draft model\n",
    "            if hasattr(self.draft.llm_backbone, 'llm') and hasattr(self.draft.llm_backbone.llm, 'lm_head'):\n",
    "                self.draft_logit_dim = self.draft.llm_backbone.llm.lm_head.weight.shape[0]\n",
    "            else:\n",
    "                self.draft_logit_dim = self.draft_vocab_size\n",
    "        else:\n",
    "            self.draft_vocab_size = self.draft.config.vocab_size\n",
    "            self.draft_logit_dim = self.draft_vocab_size\n",
    "        \n",
    "        # Check if vocabularies match\n",
    "        self.vocab_compatible = (self.target_logit_dim == self.draft_logit_dim)\n",
    "        \n",
    "        # Compute action token ranges\n",
    "        # Action tokens are the LAST n_action_bins tokens before padding\n",
    "        # For target: use vocab_size (not padded logit_dim)\n",
    "        self.target_action_start = self.target_vocab_size - self.n_action_bins\n",
    "        self.draft_action_start = self.draft_vocab_size - self.n_action_bins\n",
    "        \n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Target vocab_size: {self.target_vocab_size}, logit_dim: {self.target_logit_dim}, action tokens: [{self.target_action_start}, {self.target_vocab_size})\")\n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Draft vocab_size: {self.draft_vocab_size}, logit_dim: {self.draft_logit_dim}, action tokens: [{self.draft_action_start}, {self.draft_vocab_size})\")\n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Vocabularies compatible: {self.vocab_compatible}\")\n",
    "        \n",
    "        if not self.vocab_compatible:\n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"WARNING: Vocabulary mismatch! Token remapping will be used for action tokens only.\")\n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"This may affect acceptance rates. Consider using models with matching tokenizers.\")\n",
    "    \n",
    "    def _draft_token_to_target(self, draft_token_id: int) -> int:\n",
    "        \"\"\"Map a draft token ID to target vocabulary.\"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return draft_token_id\n",
    "        \n",
    "        # Check if this is an action token (from end of draft vocabulary)\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            # Map to corresponding action token in target vocabulary\n",
    "            action_bin = draft_token_id - self.draft_action_start\n",
    "            target_token = self.target_action_start + action_bin\n",
    "            return target_token\n",
    "        else:\n",
    "            # Non-action token - this shouldn't happen during action generation\n",
    "            # Return as-is but clamp to valid range\n",
    "            return min(draft_token_id, self.target_vocab_size - 1)\n",
    "    \n",
    "    def _target_token_to_draft(self, target_token_id: int) -> int:\n",
    "        \"\"\"Map a target token ID to draft vocabulary.\"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return target_token_id\n",
    "        \n",
    "        # Check if this is an action token\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            action_bin = target_token_id - self.target_action_start\n",
    "            draft_token = self.draft_action_start + action_bin\n",
    "            return draft_token\n",
    "        else:\n",
    "            return min(target_token_id, self.draft_vocab_size - 1)\n",
    "    \n",
    "    def _remap_logits_draft_to_target(self, draft_logits: torch.Tensor, target_logit_dim: int = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Remap draft logits to target vocabulary space.\n",
    "        Only remaps action tokens; non-action tokens get -inf.\n",
    "        \n",
    "        Args:\n",
    "            draft_logits: Logits from draft model\n",
    "            target_logit_dim: Actual dimension of target logits (may differ from vocab_size due to padding)\n",
    "        \"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return draft_logits\n",
    "        \n",
    "        # Use provided dimension or fall back to stored logit_dim\n",
    "        if target_logit_dim is None:\n",
    "            target_logit_dim = self.target_logit_dim\n",
    "        \n",
    "        # Create target-sized logits filled with -inf (use actual logit dimension, not vocab_size)\n",
    "        target_logits = torch.full(\n",
    "            (draft_logits.shape[0], target_logit_dim),\n",
    "            float('-inf'),\n",
    "            device=draft_logits.device,\n",
    "            dtype=draft_logits.dtype\n",
    "        )\n",
    "        \n",
    "        # Copy action token logits from draft to corresponding target positions\n",
    "        # Draft action tokens: [draft_action_start, draft_vocab_size)\n",
    "        # Target action tokens: [target_action_start, target_vocab_size)\n",
    "        draft_action_logits = draft_logits[:, self.draft_action_start:self.draft_vocab_size]\n",
    "        target_logits[:, self.target_action_start:self.target_vocab_size] = draft_action_logits\n",
    "        \n",
    "        return target_logits\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        \"\"\"Reset statistics counters.\"\"\"\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "    \n",
    "    def _sample_token(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Sample a token from logits.\"\"\"\n",
    "        if self.temperature <= 0:\n",
    "            return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            probs = F.softmax(logits / self.temperature, dim=-1)\n",
    "            return torch.multinomial(probs.squeeze(0), num_samples=1).unsqueeze(0)\n",
    "    \n",
    "    def _get_probs(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert logits to probabilities.\"\"\"\n",
    "        if self.temperature <= 0:\n",
    "            # For greedy, use a very low temperature to approximate argmax\n",
    "            return F.softmax(logits / 0.01, dim=-1)\n",
    "        return F.softmax(logits / self.temperature, dim=-1)\n",
    "    \n",
    "    def _prepare_target_inputs(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Prepare inputs for the target (OpenVLA) model.\"\"\"\n",
    "        prompt = f\"In: What action should the robot take to {instruction.lower()}?\\nOut:\"\n",
    "        inputs = self.target_processor(prompt, image).to(self.device, dtype=torch.bfloat16)\n",
    "        return inputs\n",
    "    \n",
    "    def _prepare_draft_inputs(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Prepare inputs for the draft (MiniVLA) model.\"\"\"\n",
    "        # Get prompt using draft model's prompt builder\n",
    "        prompt_builder = self.draft.get_prompt_builder()\n",
    "        prompt_builder.add_turn(role=\"human\", message=f\"What action should the robot take to {instruction.lower()}?\")\n",
    "        prompt_text = prompt_builder.get_prompt()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenizer = self.draft.llm_backbone.tokenizer\n",
    "        input_ids = tokenizer(prompt_text, truncation=True, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        \n",
    "        # Handle special token for Llama tokenizer\n",
    "        from transformers import LlamaTokenizerFast\n",
    "        if isinstance(tokenizer, LlamaTokenizerFast):\n",
    "            if not torch.all(input_ids[:, -1] == 29871):\n",
    "                input_ids = torch.cat(\n",
    "                    (input_ids, torch.tensor([[29871]], device=self.device)),\n",
    "                    dim=1\n",
    "                )\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Process image\n",
    "        image_transform = self.draft.vision_backbone.get_image_transform()\n",
    "        pixel_values = image_transform(image)\n",
    "        if isinstance(pixel_values, torch.Tensor):\n",
    "            pixel_values = pixel_values[None, ...].to(self.device)\n",
    "        elif isinstance(pixel_values, dict):\n",
    "            pixel_values = {k: v[None, ...].to(self.device) for k, v in pixel_values.items()}\n",
    "        \n",
    "        return input_ids, attention_mask, pixel_values\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict_action_speculative(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "        unnorm_key_target: str,\n",
    "    ) -> Tuple[np.ndarray, SpeculativeDecodingStats]:\n",
    "        \"\"\"\n",
    "        Generate action using speculative decoding.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image observation\n",
    "            instruction: Task instruction string\n",
    "            unnorm_key_target: Key for action un-normalization statistics of the target model\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (unnormalized action array, decoding statistics)\n",
    "        \"\"\"\n",
    "        # Reset per-call stats\n",
    "        call_stats = SpeculativeDecodingStats()\n",
    "        \n",
    "        action_dim = self.target.get_action_dim(unnorm_key_target)\n",
    "        \n",
    "        \n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Target vocab size: {self.target.vocab_size}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Target embedding size: {self.target.language_model.model.embed_tokens.weight.shape[0]}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Draft tokenizer vocab size: {self.draft.llm_backbone.tokenizer.vocab_size}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Action dim: {action_dim}\")\n",
    "        \n",
    "        # Prepare inputs for both models\n",
    "        target_inputs = self._prepare_target_inputs(image, instruction)\n",
    "        draft_input_ids, draft_attention_mask, draft_pixel_values = self._prepare_draft_inputs(image, instruction)\n",
    "        \n",
    "        # Cast draft inputs to appropriate dtype\n",
    "        autocast_dtype = self.draft.llm_backbone.half_precision_dtype\n",
    "        \n",
    "        # Initialize caches\n",
    "        target_cache = None\n",
    "        draft_cache = None\n",
    "        \n",
    "        generated_token_ids = []\n",
    "        \n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16, enabled=True):\n",
    "            # === Initial forward pass to get first token and cache ===\n",
    "            \n",
    "            # Target model initial forward\n",
    "            target_out = self.target(\n",
    "                **target_inputs,\n",
    "                past_key_values=None,\n",
    "                use_cache=self.use_cache,\n",
    "            )\n",
    "            target_cache = target_out.past_key_values\n",
    "            target_logits = target_out.logits[:, -1, :]\n",
    "            call_stats.total_target_forward_passes += 1\n",
    "            \n",
    "            # Draft model initial forward\n",
    "            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                draft_out = self.draft(\n",
    "                    input_ids=draft_input_ids,\n",
    "                    attention_mask=draft_attention_mask,\n",
    "                    pixel_values=draft_pixel_values,\n",
    "                    past_key_values=None,\n",
    "                    use_cache=self.use_cache,\n",
    "                )\n",
    "            draft_cache = draft_out.past_key_values\n",
    "            draft_logits = draft_out.logits[:, -1, :]\n",
    "            call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            # Sample first token from target (in target vocab space)\n",
    "            first_token_target = self._sample_token(target_logits)\n",
    "            target_token_id = int(first_token_target.item())\n",
    "            generated_token_ids.append(target_token_id)  # Store in target vocab space\n",
    "            call_stats.total_tokens_generated += 1\n",
    "            \n",
    "            # Update target cache with target token\n",
    "            target_step = self.target(\n",
    "                input_ids=first_token_target,\n",
    "                past_key_values=target_cache,\n",
    "                use_cache=self.use_cache,\n",
    "            )\n",
    "            target_cache = target_step.past_key_values\n",
    "            target_logits = target_step.logits[:, -1, :]\n",
    "            call_stats.total_target_forward_passes += 1\n",
    "            \n",
    "            # Map token to draft vocab space for draft model\n",
    "            first_token_draft_id = self._target_token_to_draft(target_token_id)\n",
    "            first_token_draft = torch.tensor([[first_token_draft_id]], device=self.device)\n",
    "            \n",
    "            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                draft_step = self.draft(\n",
    "                    input_ids=first_token_draft,\n",
    "                    past_key_values=draft_cache,\n",
    "                    use_cache=self.use_cache,\n",
    "                )\n",
    "            draft_cache = draft_step.past_key_values\n",
    "            draft_logits = draft_step.logits[:, -1, :]\n",
    "            call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            # === Main speculative decoding loop ===\n",
    "            while len(generated_token_ids) < action_dim:\n",
    "                # Determine how many tokens to speculate\n",
    "                gamma = min(self.gamma, action_dim - len(generated_token_ids))\n",
    "                \n",
    "                # Generate gamma draft tokens\n",
    "                draft_tokens = []\n",
    "                draft_probs_list = []\n",
    "                \n",
    "                current_draft_cache = draft_cache\n",
    "                current_draft_logits = draft_logits\n",
    "                \n",
    "                for _ in range(gamma):\n",
    "                    draft_probs = self._get_probs(current_draft_logits)\n",
    "                    draft_token = self._sample_token(current_draft_logits)\n",
    "                    \n",
    "                    draft_tokens.append(draft_token)\n",
    "                    draft_probs_list.append(draft_probs)\n",
    "                    \n",
    "                    # Advance draft model\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=draft_token.to(self.device),\n",
    "                            past_key_values=current_draft_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                    current_draft_cache = draft_step.past_key_values\n",
    "                    current_draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                \n",
    "                call_stats.total_draft_tokens_proposed += gamma\n",
    "                \n",
    "                # Map draft tokens to target vocab space for verification\n",
    "                draft_token_ids_target = []\n",
    "                for dt in draft_tokens:\n",
    "                    draft_id = dt.item()\n",
    "                    target_id = self._draft_token_to_target(draft_id)\n",
    "                    draft_token_ids_target.append(target_id)\n",
    "                \n",
    "                # Verify with target model - run all gamma tokens through\n",
    "                target_cache_for_verify = target_cache\n",
    "                target_logits_list = []\n",
    "                \n",
    "                for i in range(gamma):\n",
    "                    target_token_input = torch.tensor([[draft_token_ids_target[i]]], device=self.device)\n",
    "                    target_step = self.target(\n",
    "                        input_ids=target_token_input,\n",
    "                        past_key_values=target_cache_for_verify,\n",
    "                        use_cache=self.use_cache,\n",
    "                    )\n",
    "                    target_cache_for_verify = target_step.past_key_values\n",
    "                    target_logits_list.append(target_step.logits[:, -1:, :])\n",
    "                \n",
    "                call_stats.total_target_forward_passes += gamma\n",
    "                \n",
    "                # Stack target logits\n",
    "                target_logits_batch = torch.cat(target_logits_list, dim=1)  # [1, gamma, actual_vocab_dim]\n",
    "                target_probs_batch = self._get_probs(target_logits_batch)\n",
    "                \n",
    "                # Get actual target logit dimension from the output\n",
    "                actual_target_logit_dim = target_logits_batch.shape[-1]\n",
    "                \n",
    "                # Remap draft probs to target vocab space for comparison\n",
    "                # Use actual target logit dimension to ensure tensor size match\n",
    "                draft_probs_remapped = [self._remap_logits_draft_to_target(dp, actual_target_logit_dim) for dp in draft_probs_list]\n",
    "                draft_probs_remapped = [self._get_probs(dp) for dp in draft_probs_remapped]\n",
    "                \n",
    "                # Rejection sampling loop\n",
    "                n_accepted = 0\n",
    "                for i in range(gamma):\n",
    "                    draft_token_id_draft = draft_tokens[i].item()  # In draft vocab\n",
    "                    draft_token_id_target = draft_token_ids_target[i]  # Mapped to target vocab\n",
    "                    \n",
    "                    draft_prob_remapped = draft_probs_remapped[i]\n",
    "                    target_prob = target_probs_batch[:, i, :]\n",
    "                    \n",
    "                    # Get probability of the token under both models (in target vocab space)\n",
    "                    p_target = target_prob[0, draft_token_id_target].item()\n",
    "                    p_draft = draft_prob_remapped[0, draft_token_id_target].item()\n",
    "                    \n",
    "                    # Rejection sampling\n",
    "                    if p_draft > 0:\n",
    "                        acceptance_prob = min(1.0, p_target / p_draft)\n",
    "                    else:\n",
    "                        acceptance_prob = 1.0 if p_target > 0 else 0.0\n",
    "                    \n",
    "                    if torch.rand(1).item() < acceptance_prob:\n",
    "                        # Accept this token (store in target vocab space)\n",
    "                        print(f\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Accepted token: {draft_token_id_target} since p_target={p_target:.4f} > p_draft={p_draft:.4f}\")\n",
    "                        generated_token_ids.append(draft_token_id_target)\n",
    "                        n_accepted += 1\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        call_stats.total_draft_tokens_accepted += 1\n",
    "                        \n",
    "                        if len(generated_token_ids) >= action_dim:\n",
    "                            print(f\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Generated {len(generated_token_ids)} tokens, breaking\")\n",
    "                            break\n",
    "                    else:\n",
    "                        # Reject - sample from adjusted distribution\n",
    "                        adjusted_probs = max_fn(target_prob - draft_prob_remapped)\n",
    "                        if adjusted_probs.sum() > 0:\n",
    "                            corrected_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
    "                        else:\n",
    "                            corrected_token = self._sample_token(target_prob.unsqueeze(0))\n",
    "                        \n",
    "                        print(f\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Rejected token: {draft_token_id_target} since p_target={p_target:.4f} < p_draft={p_draft:.4f}, corrected token: {corrected_token.item()}\")\n",
    "                        print(f\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Absolute id distance between rejected and corrected token: {abs(draft_token_id_target - corrected_token.item())}\")\n",
    "                        \n",
    "                        # Store corrected token (already in target vocab space)\n",
    "                        generated_token_ids.append(int(corrected_token.item()))\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        n_accepted = i  # Number of accepted tokens (before this rejection)\n",
    "                        break\n",
    "                                    \n",
    "                # Update caches after acceptance/rejection\n",
    "                if n_accepted == gamma and len(generated_token_ids) < action_dim:\n",
    "                    # All accepted - need to sample one more from target\n",
    "                    target_cache = target_cache_for_verify\n",
    "                    target_logits = target_logits_list[-1].squeeze(1)\n",
    "                    \n",
    "                    # Sample additional token from target (in target vocab space)\n",
    "                    bonus_token_target = self._sample_token(target_logits)\n",
    "                    bonus_token_id_target = int(bonus_token_target.item())\n",
    "                    generated_token_ids.append(bonus_token_id_target)\n",
    "                    call_stats.total_tokens_generated += 1\n",
    "                    \n",
    "                    # Update target cache\n",
    "                    target_step = self.target(\n",
    "                        input_ids=bonus_token_target,\n",
    "                        past_key_values=target_cache,\n",
    "                        use_cache=self.use_cache,\n",
    "                    )\n",
    "                    target_cache = target_step.past_key_values\n",
    "                    target_logits = target_step.logits[:, -1, :]\n",
    "                    call_stats.total_target_forward_passes += 1\n",
    "                    \n",
    "                    # Map bonus token to draft vocab and update draft cache\n",
    "                    bonus_token_id_draft = self._target_token_to_draft(bonus_token_id_target)\n",
    "                    bonus_token_draft = torch.tensor([[bonus_token_id_draft]], device=self.device)\n",
    "                    \n",
    "                    draft_cache = current_draft_cache\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=bonus_token_draft,\n",
    "                            past_key_values=draft_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                    draft_cache = draft_step.past_key_values\n",
    "                    draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                    \n",
    "                else:\n",
    "                    # Some tokens rejected - prune caches\n",
    "                    tokens_to_discard = gamma - n_accepted\n",
    "                    if tokens_to_discard > 0 and self.use_cache:\n",
    "                        # We need to prune and resync\n",
    "                        # Use the cache state after the accepted tokens\n",
    "                        target_cache = target_cache_for_verify\n",
    "                        if tokens_to_discard > 0:\n",
    "                            target_cache = prune_cache(target_cache, tokens_to_discard)\n",
    "                        \n",
    "                        # Rebuild draft cache\n",
    "                        draft_cache = prune_cache(current_draft_cache, gamma - n_accepted)\n",
    "                    \n",
    "                    # Get logits for next round\n",
    "                    if len(generated_token_ids) < action_dim:\n",
    "                        # Last token is in target vocab space\n",
    "                        last_token_id_target = generated_token_ids[-1]\n",
    "                        last_token_target = torch.tensor([[last_token_id_target]], device=self.device)\n",
    "                        \n",
    "                        target_step = self.target(\n",
    "                            input_ids=last_token_target,\n",
    "                            past_key_values=target_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                        target_cache = target_step.past_key_values\n",
    "                        target_logits = target_step.logits[:, -1, :]\n",
    "                        call_stats.total_target_forward_passes += 1\n",
    "                        \n",
    "                        # Map to draft vocab for draft model\n",
    "                        last_token_id_draft = self._target_token_to_draft(last_token_id_target)\n",
    "                        last_token_draft = torch.tensor([[last_token_id_draft]], device=self.device)\n",
    "                        \n",
    "                        with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                            draft_step = self.draft(\n",
    "                                input_ids=last_token_draft,\n",
    "                                past_key_values=draft_cache,\n",
    "                                use_cache=self.use_cache,\n",
    "                            )\n",
    "                        draft_cache = draft_step.past_key_values\n",
    "                        draft_logits = draft_step.logits[:, -1, :]\n",
    "                        call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Call stats: {call_stats}\")\n",
    "            \n",
    "        # Decode tokens to actions\n",
    "        predicted_action_token_ids = np.array(generated_token_ids[:action_dim], dtype=np.int64)\n",
    "        \n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Predicted action token ids: {predicted_action_token_ids}\")\n",
    "        \n",
    "        # Use target model's decoding (vocab_size - token_id approach)\n",
    "        vocab_size = self.target.vocab_size\n",
    "        discretized_actions = vocab_size - predicted_action_token_ids\n",
    "        discretized_actions = np.clip(discretized_actions - 1, a_min=0, a_max=self.target.bin_centers.shape[0] - 1)\n",
    "        normalized_actions = self.target.bin_centers[discretized_actions]\n",
    "        \n",
    "        # Un-normalize actions\n",
    "        action_norm_stats = self.target.get_action_stats(unnorm_key_target)\n",
    "        mask = action_norm_stats.get(\"mask\", np.ones_like(action_norm_stats[\"q01\"], dtype=bool))\n",
    "        action_high, action_low = np.array(action_norm_stats[\"q99\"]), np.array(action_norm_stats[\"q01\"])\n",
    "        actions = np.where(\n",
    "            mask,\n",
    "            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,\n",
    "            normalized_actions,\n",
    "        )\n",
    "        \n",
    "        # Update global stats\n",
    "        self.stats.total_tokens_generated += call_stats.total_tokens_generated\n",
    "        self.stats.total_draft_tokens_proposed += call_stats.total_draft_tokens_proposed\n",
    "        self.stats.total_draft_tokens_accepted += call_stats.total_draft_tokens_accepted\n",
    "        self.stats.total_target_forward_passes += call_stats.total_target_forward_passes\n",
    "        self.stats.total_draft_forward_passes += call_stats.total_draft_forward_passes\n",
    "        \n",
    "        return actions, call_stats\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Simplified speculative action prediction (standalone function)\n",
    "# ============================================================================\n",
    "\n",
    "@torch.inference_mode()\n",
    "def speculative_predict_action(\n",
    "    target_vla,\n",
    "    draft_vla,\n",
    "    target_processor,\n",
    "    observation: Dict,\n",
    "    instruction: str,\n",
    "    unnorm_key_target: str,\n",
    "    center_crop: bool = False,\n",
    "    gamma: int = 4,\n",
    "    temperature: float = 0.0,\n",
    ") -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Speculative decoding for VLA action prediction.\n",
    "    \n",
    "    Args:\n",
    "        target_vla: OpenVLA model (loaded via get_vla)\n",
    "        draft_vla: MiniVLA model (loaded via get_prismatic_vla)\n",
    "        target_processor: HuggingFace processor for target\n",
    "        observation: Dict with 'full_image' key\n",
    "        instruction: Task instruction string\n",
    "        unnorm_key_target: Key for un-normalization of the target model\n",
    "        center_crop: Whether to center crop the image\n",
    "        gamma: Number of tokens to speculate\n",
    "        temperature: Sampling temperature\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (action array, acceptance rate)\n",
    "    \"\"\"\n",
    "    # Prepare image\n",
    "    image = prepare_image(observation[\"full_image\"], center_crop=center_crop)\n",
    "    \n",
    "    # Create decoder\n",
    "    decoder = VLASpeculativeDecoder(\n",
    "        target_model=target_vla,\n",
    "        draft_model=draft_vla,\n",
    "        target_processor=target_processor,\n",
    "        gamma=gamma,\n",
    "        use_cache=True,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    # Run speculative decoding\n",
    "    action, stats = decoder.predict_action_speculative(image, instruction, unnorm_key_target)\n",
    "    \n",
    "    return action, stats.acceptance_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLASpeculativeDecoderD:\n",
    "    \"\"\"\n",
    "    Speculative decoding for VLA models.\n",
    "    \n",
    "    Uses a draft model (MiniVLA) to propose action tokens and a target model \n",
    "    (OpenVLA) to verify them.\n",
    "    \n",
    "    IMPORTANT: For speculative decoding to work correctly, both models should\n",
    "    share the same tokenizer/vocabulary. If they don't, action token remapping\n",
    "    is attempted but this only works for action tokens (last 256 tokens of vocab).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model,\n",
    "        draft_model,\n",
    "        target_processor=None,\n",
    "        gamma: int = 4,  # Number of draft tokens to propose at once\n",
    "        use_cache: bool = True,\n",
    "        temperature: float = 0.0,  # 0 = greedy/argmax\n",
    "        n_action_bins: int = 256,  # Number of action bins (tokens)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the speculative decoder.\n",
    "        \n",
    "        Args:\n",
    "            target_model: OpenVLA model (larger, slower, more accurate)\n",
    "            draft_model: MiniVLA model (smaller, faster)\n",
    "            target_processor: HuggingFace processor for target model\n",
    "            gamma: Number of tokens to speculate at each step\n",
    "            use_cache: Whether to use KV caching\n",
    "            temperature: Sampling temperature (0 = greedy/argmax)\n",
    "            n_action_bins: Number of action token bins (typically 256)\n",
    "        \"\"\"\n",
    "        self.target = target_model\n",
    "        self.draft = draft_model\n",
    "        self.target_processor = target_processor\n",
    "        self.gamma = gamma\n",
    "        self.use_cache = use_cache\n",
    "        self.temperature = temperature\n",
    "        self.n_action_bins = n_action_bins\n",
    "        \n",
    "        # Get device\n",
    "        self.device = next(target_model.parameters()).device\n",
    "        \n",
    "        # Stats tracking\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "        \n",
    "        # Check vocabulary compatibility and setup token mapping\n",
    "        self._setup_token_mapping()\n",
    "    \n",
    "    def _setup_token_mapping(self):\n",
    "        \"\"\"Setup token mapping between draft and target vocabularies.\"\"\"\n",
    "        # Get vocabulary sizes\n",
    "        # Target model (OpenVLA/HF style) - use ACTUAL embedding dimension, not vocab_size attribute\n",
    "        # The embedding may be padded to \"multiple of\" for efficiency\n",
    "        if hasattr(self.target, 'language_model') and hasattr(self.target.language_model, 'model'):\n",
    "            # Actual embedding dimension (includes padding)\n",
    "            self.target_logit_dim = self.target.language_model.model.embed_tokens.weight.shape[0]\n",
    "        elif hasattr(self.target, 'get_output_embeddings'):\n",
    "            self.target_logit_dim = self.target.get_output_embeddings().weight.shape[0]\n",
    "        else:\n",
    "            self.target_logit_dim = self.target.config.vocab_size\n",
    "        \n",
    "        # Also get the \"logical\" vocab size (without padding) for action token calculation\n",
    "        if hasattr(self.target, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.vocab_size\n",
    "        elif hasattr(self.target, 'config') and hasattr(self.target.config, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.config.vocab_size\n",
    "        else:\n",
    "            self.target_vocab_size = self.target_logit_dim\n",
    "        \n",
    "        # Draft model (Prismatic style)\n",
    "        if hasattr(self.draft, 'llm_backbone'):\n",
    "            draft_tokenizer = self.draft.llm_backbone.tokenizer\n",
    "            # Qwen2 uses len(tokenizer) for full vocab including added tokens\n",
    "            self.draft_vocab_size = len(draft_tokenizer) if hasattr(draft_tokenizer, '__len__') else draft_tokenizer.vocab_size\n",
    "            # Get actual logit dimension from draft model\n",
    "            if hasattr(self.draft.llm_backbone, 'llm') and hasattr(self.draft.llm_backbone.llm, 'lm_head'):\n",
    "                self.draft_logit_dim = self.draft.llm_backbone.llm.lm_head.weight.shape[0]\n",
    "            else:\n",
    "                self.draft_logit_dim = self.draft_vocab_size\n",
    "        else:\n",
    "            self.draft_vocab_size = self.draft.config.vocab_size\n",
    "            self.draft_logit_dim = self.draft_vocab_size\n",
    "        \n",
    "        # Check if vocabularies match\n",
    "        self.vocab_compatible = (self.target_logit_dim == self.draft_logit_dim)\n",
    "        \n",
    "        # Compute action token ranges\n",
    "        # Action tokens are the LAST n_action_bins tokens before padding\n",
    "        # For target: use vocab_size (not padded logit_dim)\n",
    "        self.target_action_start = self.target_vocab_size - self.n_action_bins\n",
    "        self.draft_action_start = self.draft_vocab_size - self.n_action_bins\n",
    "        \n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Target vocab_size: {self.target_vocab_size}, logit_dim: {self.target_logit_dim}, action tokens: [{self.target_action_start}, {self.target_vocab_size})\")\n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Draft vocab_size: {self.draft_vocab_size}, logit_dim: {self.draft_logit_dim}, action tokens: [{self.draft_action_start}, {self.draft_vocab_size})\")\n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Vocabularies compatible: {self.vocab_compatible}\")\n",
    "        \n",
    "        if not self.vocab_compatible:\n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"WARNING: Vocabulary mismatch! Token remapping will be used for action tokens only.\")\n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"This may affect acceptance rates. Consider using models with matching tokenizers.\")\n",
    "        \n",
    "        # DEBUG: Verify mapping with example tokens\n",
    "        print(\"\\033[38;2;100;200;255m[DEBUG] Token mapping verification examples:\\033[0m\")\n",
    "        for bin_idx in [0, 127, 255]:  # First, middle, last action bins\n",
    "            draft_tok = self.draft_action_start + bin_idx\n",
    "            target_tok = self._draft_token_to_target(draft_tok)\n",
    "            draft_bin = self._get_action_bin_from_draft_token(draft_tok)\n",
    "            target_bin = self._get_action_bin_from_target_token(target_tok)\n",
    "            print(f\"  Bin {bin_idx}: draft_token={draft_tok}  target_token={target_tok} | draft_bin={draft_bin}, target_bin={target_bin} | {'' if draft_bin == target_bin else ''}\")\n",
    "    \n",
    "    def _draft_token_to_target(self, draft_token_id: int) -> int:\n",
    "        \"\"\"Map a draft token ID to target vocabulary.\"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return draft_token_id\n",
    "        \n",
    "        # Check if this is an action token (from end of draft vocabulary)\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            # Map to corresponding action token in target vocabulary\n",
    "            action_bin = draft_token_id - self.draft_action_start\n",
    "            target_token = self.target_action_start + action_bin\n",
    "            return target_token\n",
    "        else:\n",
    "            # Non-action token - this shouldn't happen during action generation\n",
    "            # Return as-is but clamp to valid range\n",
    "            return min(draft_token_id, self.target_vocab_size - 1)\n",
    "    \n",
    "    def _target_token_to_draft(self, target_token_id: int) -> int:\n",
    "        \"\"\"Map a target token ID to draft vocabulary.\"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return target_token_id\n",
    "        \n",
    "        # Check if this is an action token\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            action_bin = target_token_id - self.target_action_start\n",
    "            draft_token = self.draft_action_start + action_bin\n",
    "            return draft_token\n",
    "        else:\n",
    "            return min(target_token_id, self.draft_vocab_size - 1)\n",
    "    \n",
    "    def _remap_logits_draft_to_target(self, draft_logits: torch.Tensor, target_logit_dim: int = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Remap draft logits to target vocabulary space.\n",
    "        Only remaps action tokens; non-action tokens get -inf.\n",
    "        \n",
    "        Args:\n",
    "            draft_logits: Logits from draft model\n",
    "            target_logit_dim: Actual dimension of target logits (may differ from vocab_size due to padding)\n",
    "        \"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return draft_logits\n",
    "        \n",
    "        # Use provided dimension or fall back to stored logit_dim\n",
    "        if target_logit_dim is None:\n",
    "            target_logit_dim = self.target_logit_dim\n",
    "        \n",
    "        # Create target-sized logits filled with -inf (use actual logit dimension, not vocab_size)\n",
    "        target_logits = torch.full(\n",
    "            (draft_logits.shape[0], target_logit_dim),\n",
    "            float('-inf'),\n",
    "            device=draft_logits.device,\n",
    "            dtype=draft_logits.dtype\n",
    "        )\n",
    "        \n",
    "        # Copy action token logits from draft to corresponding target positions\n",
    "        # Draft action tokens: [draft_action_start, draft_vocab_size)\n",
    "        # Target action tokens: [target_action_start, target_vocab_size)\n",
    "        draft_action_logits = draft_logits[:, self.draft_action_start:self.draft_vocab_size]\n",
    "        target_logits[:, self.target_action_start:self.target_vocab_size] = draft_action_logits\n",
    "        \n",
    "        return target_logits\n",
    "    \n",
    "    # =========================================================================\n",
    "    # DEBUG: Token mapping verification methods\n",
    "    # =========================================================================\n",
    "    \n",
    "    def _get_action_bin_from_draft_token(self, draft_token_id: int) -> int:\n",
    "        \"\"\"Get action bin index from draft token ID.\"\"\"\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            return draft_token_id - self.draft_action_start\n",
    "        return -1  # Not an action token\n",
    "    \n",
    "    def _get_action_bin_from_target_token(self, target_token_id: int) -> int:\n",
    "        \"\"\"Get action bin index from target token ID.\"\"\"\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            return target_token_id - self.target_action_start\n",
    "        return -1  # Not an action token\n",
    "    \n",
    "    def _get_continuous_action_from_bin(self, bin_idx: int) -> float:\n",
    "        \"\"\"Convert action bin index to continuous action value using target's bin centers.\"\"\"\n",
    "        if 0 <= bin_idx < len(self.target.bin_centers):\n",
    "            return self.target.bin_centers[bin_idx]\n",
    "        return float('nan')\n",
    "    \n",
    "    def _debug_token_mapping(self, draft_token_id: int, target_token_id: int, prefix: str = \"\"):\n",
    "        \"\"\"Debug print showing token mapping verification.\"\"\"\n",
    "        draft_bin = self._get_action_bin_from_draft_token(draft_token_id)\n",
    "        target_bin = self._get_action_bin_from_target_token(target_token_id)\n",
    "        \n",
    "        draft_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "        target_action = self._get_continuous_action_from_bin(target_bin)\n",
    "        \n",
    "        match_status = \" MATCH\" if draft_bin == target_bin else \" MISMATCH\"\n",
    "        \n",
    "        print(f\"\\033[38;2;100;200;255m[DEBUG TOKEN MAP] {prefix}\\033[0m\")\n",
    "        print(f\"  Draft token:  {draft_token_id}  bin {draft_bin}  action {draft_action:.4f}\")\n",
    "        print(f\"  Target token: {target_token_id}  bin {target_bin}  action {target_action:.4f}\")\n",
    "        print(f\"  Bins match: {match_status}\")\n",
    "        \n",
    "        return draft_bin == target_bin\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        \"\"\"Reset statistics counters.\"\"\"\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "    \n",
    "    def _sample_token(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Sample a token from logits.\"\"\"\n",
    "        if self.temperature <= 0:\n",
    "            return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            probs = F.softmax(logits / self.temperature, dim=-1)\n",
    "            return torch.multinomial(probs.squeeze(0), num_samples=1).unsqueeze(0)\n",
    "    \n",
    "    def _get_probs(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert logits to probabilities.\"\"\"\n",
    "        if self.temperature <= 0:\n",
    "            # For greedy, use a very low temperature to approximate argmax\n",
    "            return F.softmax(logits / 0.01, dim=-1)\n",
    "        return F.softmax(logits / self.temperature, dim=-1)\n",
    "    \n",
    "    def _prepare_target_inputs(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Prepare inputs for the target (OpenVLA) model.\"\"\"\n",
    "        prompt = f\"In: What action should the robot take to {instruction.lower()}?\\nOut:\"\n",
    "        inputs = self.target_processor(prompt, image).to(self.device, dtype=torch.bfloat16)\n",
    "        return inputs\n",
    "    \n",
    "    def _prepare_draft_inputs(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Prepare inputs for the draft (MiniVLA) model.\"\"\"\n",
    "        # Get prompt using draft model's prompt builder\n",
    "        prompt_builder = self.draft.get_prompt_builder()\n",
    "        prompt_builder.add_turn(role=\"human\", message=f\"What action should the robot take to {instruction.lower()}?\")\n",
    "        prompt_text = prompt_builder.get_prompt()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenizer = self.draft.llm_backbone.tokenizer\n",
    "        input_ids = tokenizer(prompt_text, truncation=True, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        \n",
    "        # Handle special token for Llama tokenizer\n",
    "        from transformers import LlamaTokenizerFast\n",
    "        if isinstance(tokenizer, LlamaTokenizerFast):\n",
    "            if not torch.all(input_ids[:, -1] == 29871):\n",
    "                input_ids = torch.cat(\n",
    "                    (input_ids, torch.tensor([[29871]], device=self.device)),\n",
    "                    dim=1\n",
    "                )\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Process image\n",
    "        image_transform = self.draft.vision_backbone.get_image_transform()\n",
    "        pixel_values = image_transform(image)\n",
    "        if isinstance(pixel_values, torch.Tensor):\n",
    "            pixel_values = pixel_values[None, ...].to(self.device)\n",
    "        elif isinstance(pixel_values, dict):\n",
    "            pixel_values = {k: v[None, ...].to(self.device) for k, v in pixel_values.items()}\n",
    "        \n",
    "        return input_ids, attention_mask, pixel_values\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict_action_speculative(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "        unnorm_key_target: str,\n",
    "    ) -> Tuple[np.ndarray, SpeculativeDecodingStats]:\n",
    "        \"\"\"\n",
    "        Generate action using speculative decoding.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image observation\n",
    "            instruction: Task instruction string\n",
    "            unnorm_key_target: Key for action un-normalization statistics of the target model\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (unnormalized action array, decoding statistics)\n",
    "        \"\"\"\n",
    "        # Reset per-call stats\n",
    "        call_stats = SpeculativeDecodingStats()\n",
    "        \n",
    "        action_dim = self.target.get_action_dim(unnorm_key_target)\n",
    "        \n",
    "        \n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Target vocab size: {self.target.vocab_size}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Target embedding size: {self.target.language_model.model.embed_tokens.weight.shape[0]}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Draft tokenizer vocab size: {self.draft.llm_backbone.tokenizer.vocab_size}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Action dim: {action_dim}\")\n",
    "        \n",
    "        # Prepare inputs for both models\n",
    "        target_inputs = self._prepare_target_inputs(image, instruction)\n",
    "        draft_input_ids, draft_attention_mask, draft_pixel_values = self._prepare_draft_inputs(image, instruction)\n",
    "        \n",
    "        # Cast draft inputs to appropriate dtype\n",
    "        autocast_dtype = self.draft.llm_backbone.half_precision_dtype\n",
    "        \n",
    "        # Initialize caches\n",
    "        target_cache = None\n",
    "        draft_cache = None\n",
    "        \n",
    "        generated_token_ids = []\n",
    "        \n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16, enabled=True):\n",
    "            # === Initial forward pass to get first token and cache ===\n",
    "            \n",
    "            # Target model initial forward\n",
    "            target_out = self.target(\n",
    "                **target_inputs,\n",
    "                past_key_values=None,\n",
    "                use_cache=self.use_cache,\n",
    "            )\n",
    "            target_cache = target_out.past_key_values\n",
    "            target_logits = target_out.logits[:, -1, :]\n",
    "            call_stats.total_target_forward_passes += 1\n",
    "            \n",
    "            # Draft model initial forward\n",
    "            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                draft_out = self.draft(\n",
    "                    input_ids=draft_input_ids,\n",
    "                    attention_mask=draft_attention_mask,\n",
    "                    pixel_values=draft_pixel_values,\n",
    "                    past_key_values=None,\n",
    "                    use_cache=self.use_cache,\n",
    "                )\n",
    "            draft_cache = draft_out.past_key_values\n",
    "            draft_logits = draft_out.logits[:, -1, :]\n",
    "            call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            # Sample first token from target (in target vocab space)\n",
    "            first_token_target = self._sample_token(target_logits)\n",
    "            target_token_id = int(first_token_target.item())\n",
    "            generated_token_ids.append(target_token_id)  # Store in target vocab space\n",
    "            call_stats.total_tokens_generated += 1\n",
    "            \n",
    "            # DEBUG: Show first token info\n",
    "            first_target_bin = self._get_action_bin_from_target_token(target_token_id)\n",
    "            first_target_action = self._get_continuous_action_from_bin(first_target_bin)\n",
    "            print(f\"\\033[38;2;100;200;255m[DEBUG] First token sampled from target:\\033[0m\")\n",
    "            print(f\"  target_token={target_token_id}  bin={first_target_bin}  action={first_target_action:.4f}\")\n",
    "            \n",
    "            # Update target cache with target token\n",
    "            target_step = self.target(\n",
    "                input_ids=first_token_target,\n",
    "                past_key_values=target_cache,\n",
    "                use_cache=self.use_cache,\n",
    "            )\n",
    "            target_cache = target_step.past_key_values\n",
    "            target_logits = target_step.logits[:, -1, :]\n",
    "            call_stats.total_target_forward_passes += 1\n",
    "            \n",
    "            # Map token to draft vocab space for draft model\n",
    "            first_token_draft_id = self._target_token_to_draft(target_token_id)\n",
    "            first_token_draft = torch.tensor([[first_token_draft_id]], device=self.device)\n",
    "            \n",
    "            # DEBUG: Verify reverse mapping\n",
    "            first_draft_bin = self._get_action_bin_from_draft_token(first_token_draft_id)\n",
    "            print(f\"  Mapped to draft: draft_token={first_token_draft_id}  bin={first_draft_bin} {'' if first_target_bin == first_draft_bin else ' MISMATCH!'}\")\n",
    "            \n",
    "            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                draft_step = self.draft(\n",
    "                    input_ids=first_token_draft,\n",
    "                    past_key_values=draft_cache,\n",
    "                    use_cache=self.use_cache,\n",
    "                )\n",
    "            draft_cache = draft_step.past_key_values\n",
    "            draft_logits = draft_step.logits[:, -1, :]\n",
    "            call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            # === Main speculative decoding loop ===\n",
    "            while len(generated_token_ids) < action_dim:\n",
    "                # Determine how many tokens to speculate\n",
    "                gamma = min(self.gamma, action_dim - len(generated_token_ids))\n",
    "                \n",
    "                # Generate gamma draft tokens\n",
    "                draft_tokens = []\n",
    "                draft_probs_list = []\n",
    "                \n",
    "                current_draft_cache = draft_cache\n",
    "                current_draft_logits = draft_logits\n",
    "                \n",
    "                for _ in range(gamma):\n",
    "                    draft_probs = self._get_probs(current_draft_logits)\n",
    "                    draft_token = self._sample_token(current_draft_logits)\n",
    "                    \n",
    "                    draft_tokens.append(draft_token)\n",
    "                    draft_probs_list.append(draft_probs)\n",
    "                    \n",
    "                    # Advance draft model\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=draft_token.to(self.device),\n",
    "                            past_key_values=current_draft_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                    current_draft_cache = draft_step.past_key_values\n",
    "                    current_draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                \n",
    "                call_stats.total_draft_tokens_proposed += gamma\n",
    "                \n",
    "                # Map draft tokens to target vocab space for verification\n",
    "                draft_token_ids_target = []\n",
    "                print(f\"\\033[38;2;100;200;255m[DEBUG] Mapping {gamma} draft tokens to target vocab:\\033[0m\")\n",
    "                for idx, dt in enumerate(draft_tokens):\n",
    "                    draft_id = dt.item()\n",
    "                    target_id = self._draft_token_to_target(draft_id)\n",
    "                    draft_token_ids_target.append(target_id)\n",
    "                    \n",
    "                    # Verify mapping preserves action bin\n",
    "                    draft_bin = self._get_action_bin_from_draft_token(draft_id)\n",
    "                    target_bin = self._get_action_bin_from_target_token(target_id)\n",
    "                    draft_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "                    target_action = self._get_continuous_action_from_bin(target_bin)\n",
    "                    match = \"\" if draft_bin == target_bin else \" MISMATCH!\"\n",
    "                    print(f\"  [{idx}] draft_tok={draft_id}  target_tok={target_id} | bin: {draft_bin}{target_bin} | action: {draft_action:.4f}{target_action:.4f} {match}\")\n",
    "                \n",
    "                # Verify with target model - run all gamma tokens through\n",
    "                target_cache_for_verify = target_cache\n",
    "                target_logits_list = []\n",
    "                \n",
    "                for i in range(gamma):\n",
    "                    target_token_input = torch.tensor([[draft_token_ids_target[i]]], device=self.device)\n",
    "                    target_step = self.target(\n",
    "                        input_ids=target_token_input,\n",
    "                        past_key_values=target_cache_for_verify,\n",
    "                        use_cache=self.use_cache,\n",
    "                    )\n",
    "                    target_cache_for_verify = target_step.past_key_values\n",
    "                    target_logits_list.append(target_step.logits[:, -1:, :])\n",
    "                \n",
    "                call_stats.total_target_forward_passes += gamma\n",
    "                \n",
    "                # Stack target logits\n",
    "                target_logits_batch = torch.cat(target_logits_list, dim=1)  # [1, gamma, actual_vocab_dim]\n",
    "                target_probs_batch = self._get_probs(target_logits_batch)\n",
    "                \n",
    "                # Get actual target logit dimension from the output\n",
    "                actual_target_logit_dim = target_logits_batch.shape[-1]\n",
    "                \n",
    "                # Remap draft probs to target vocab space for comparison\n",
    "                # Use actual target logit dimension to ensure tensor size match\n",
    "                draft_probs_remapped = [self._remap_logits_draft_to_target(dp, actual_target_logit_dim) for dp in draft_probs_list]\n",
    "                draft_probs_remapped = [self._get_probs(dp) for dp in draft_probs_remapped]\n",
    "                \n",
    "                # Rejection sampling loop\n",
    "                n_accepted = 0\n",
    "                for i in range(gamma):\n",
    "                    draft_token_id_draft = draft_tokens[i].item()  # In draft vocab\n",
    "                    draft_token_id_target = draft_token_ids_target[i]  # Mapped to target vocab\n",
    "                    \n",
    "                    draft_prob_remapped = draft_probs_remapped[i]\n",
    "                    target_prob = target_probs_batch[:, i, :]\n",
    "                    \n",
    "                    # Get probability of the token under both models (in target vocab space)\n",
    "                    p_target = target_prob[0, draft_token_id_target].item()\n",
    "                    p_draft = draft_prob_remapped[0, draft_token_id_target].item()\n",
    "                    \n",
    "                    # Rejection sampling\n",
    "                    if p_draft > 0:\n",
    "                        acceptance_prob = min(1.0, p_target / p_draft)\n",
    "                    else:\n",
    "                        acceptance_prob = 1.0 if p_target > 0 else 0.0\n",
    "                    \n",
    "                    if torch.rand(1).item() < acceptance_prob:\n",
    "                        # Accept this token (store in target vocab space)\n",
    "                        accepted_bin = self._get_action_bin_from_target_token(draft_token_id_target)\n",
    "                        accepted_action = self._get_continuous_action_from_bin(accepted_bin)\n",
    "                        print(f\"\\033[38;2;0;255;0m[ACCEPT]\\033[0m token[{i}]: target_tok={draft_token_id_target}  bin={accepted_bin}  action={accepted_action:.4f} | p_target={p_target:.4f}, p_draft={p_draft:.4f}, accept_prob={acceptance_prob:.4f}\")\n",
    "                        generated_token_ids.append(draft_token_id_target)\n",
    "                        n_accepted += 1\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        call_stats.total_draft_tokens_accepted += 1\n",
    "                        \n",
    "                        if len(generated_token_ids) >= action_dim:\n",
    "                            print(f\"\\033[38;2;255;165;0m[SRP] -> \\033[0m Generated {len(generated_token_ids)}/{action_dim} tokens, done.\")\n",
    "                            break\n",
    "                    else:\n",
    "                        # Reject - sample from adjusted distribution\n",
    "                        adjusted_probs = max_fn(target_prob - draft_prob_remapped)\n",
    "                        if adjusted_probs.sum() > 0:\n",
    "                            corrected_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
    "                        else:\n",
    "                            corrected_token = self._sample_token(target_prob.unsqueeze(0))\n",
    "                        \n",
    "                        corrected_token_id = int(corrected_token.item())\n",
    "                        rejected_bin = self._get_action_bin_from_target_token(draft_token_id_target)\n",
    "                        corrected_bin = self._get_action_bin_from_target_token(corrected_token_id)\n",
    "                        rejected_action = self._get_continuous_action_from_bin(rejected_bin)\n",
    "                        corrected_action = self._get_continuous_action_from_bin(corrected_bin)\n",
    "                        bin_diff = abs(rejected_bin - corrected_bin) if rejected_bin >= 0 and corrected_bin >= 0 else -1\n",
    "                        \n",
    "                        print(f\"\\033[38;2;255;100;100m[REJECT]\\033[0m token[{i}]: p_target={p_target:.4f} < p_draft={p_draft:.4f}, accept_prob={acceptance_prob:.4f}\")\n",
    "                        print(f\"  Rejected:  target_tok={draft_token_id_target}  bin={rejected_bin}  action={rejected_action:.4f}\")\n",
    "                        print(f\"  Corrected: target_tok={corrected_token_id}  bin={corrected_bin}  action={corrected_action:.4f}\")\n",
    "                        print(f\"  Bin difference: {bin_diff} | Action difference: {abs(rejected_action - corrected_action):.4f}\")\n",
    "                        \n",
    "                        # Store corrected token (already in target vocab space)\n",
    "                        generated_token_ids.append(corrected_token_id)\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        n_accepted = i  # Number of accepted tokens (before this rejection)\n",
    "                        break\n",
    "                                    \n",
    "                # Update caches after acceptance/rejection\n",
    "                if n_accepted == gamma and len(generated_token_ids) < action_dim:\n",
    "                    # All accepted - need to sample one more from target\n",
    "                    print(f\"\\033[38;2;0;255;0m[ALL ACCEPTED]\\033[0m All {gamma} draft tokens accepted! Sampling bonus token from target...\")\n",
    "                    target_cache = target_cache_for_verify\n",
    "                    target_logits = target_logits_list[-1].squeeze(1)\n",
    "                    \n",
    "                    # Sample additional token from target (in target vocab space)\n",
    "                    bonus_token_target = self._sample_token(target_logits)\n",
    "                    bonus_token_id_target = int(bonus_token_target.item())\n",
    "                    bonus_bin = self._get_action_bin_from_target_token(bonus_token_id_target)\n",
    "                    bonus_action = self._get_continuous_action_from_bin(bonus_bin)\n",
    "                    print(f\"  Bonus token: target_tok={bonus_token_id_target}  bin={bonus_bin}  action={bonus_action:.4f}\")\n",
    "                    generated_token_ids.append(bonus_token_id_target)\n",
    "                    call_stats.total_tokens_generated += 1\n",
    "                    \n",
    "                    # Update target cache\n",
    "                    target_step = self.target(\n",
    "                        input_ids=bonus_token_target,\n",
    "                        past_key_values=target_cache,\n",
    "                        use_cache=self.use_cache,\n",
    "                    )\n",
    "                    target_cache = target_step.past_key_values\n",
    "                    target_logits = target_step.logits[:, -1, :]\n",
    "                    call_stats.total_target_forward_passes += 1\n",
    "                    \n",
    "                    # Map bonus token to draft vocab and update draft cache\n",
    "                    bonus_token_id_draft = self._target_token_to_draft(bonus_token_id_target)\n",
    "                    bonus_draft_bin = self._get_action_bin_from_draft_token(bonus_token_id_draft)\n",
    "                    print(f\"  Mapped to draft: draft_tok={bonus_token_id_draft}  bin={bonus_draft_bin} {'' if bonus_bin == bonus_draft_bin else ' MISMATCH!'}\")\n",
    "                    bonus_token_draft = torch.tensor([[bonus_token_id_draft]], device=self.device)\n",
    "                    \n",
    "                    draft_cache = current_draft_cache\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=bonus_token_draft,\n",
    "                            past_key_values=draft_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                    draft_cache = draft_step.past_key_values\n",
    "                    draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                    \n",
    "                else:\n",
    "                    # Some tokens rejected - prune caches\n",
    "                    tokens_to_discard = gamma - n_accepted\n",
    "                    if tokens_to_discard > 0 and self.use_cache:\n",
    "                        # We need to prune and resync\n",
    "                        # Use the cache state after the accepted tokens\n",
    "                        target_cache = target_cache_for_verify\n",
    "                        if tokens_to_discard > 0:\n",
    "                            target_cache = prune_cache(target_cache, tokens_to_discard)\n",
    "                        \n",
    "                        # Rebuild draft cache\n",
    "                        draft_cache = prune_cache(current_draft_cache, gamma - n_accepted)\n",
    "                    \n",
    "                    # Get logits for next round\n",
    "                    if len(generated_token_ids) < action_dim:\n",
    "                        # Last token is in target vocab space\n",
    "                        last_token_id_target = generated_token_ids[-1]\n",
    "                        last_token_target = torch.tensor([[last_token_id_target]], device=self.device)\n",
    "                        \n",
    "                        target_step = self.target(\n",
    "                            input_ids=last_token_target,\n",
    "                            past_key_values=target_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                        target_cache = target_step.past_key_values\n",
    "                        target_logits = target_step.logits[:, -1, :]\n",
    "                        call_stats.total_target_forward_passes += 1\n",
    "                        \n",
    "                        # Map to draft vocab for draft model\n",
    "                        last_token_id_draft = self._target_token_to_draft(last_token_id_target)\n",
    "                        last_token_draft = torch.tensor([[last_token_id_draft]], device=self.device)\n",
    "                        \n",
    "                        with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                            draft_step = self.draft(\n",
    "                                input_ids=last_token_draft,\n",
    "                                past_key_values=draft_cache,\n",
    "                                use_cache=self.use_cache,\n",
    "                            )\n",
    "                        draft_cache = draft_step.past_key_values\n",
    "                        draft_logits = draft_step.logits[:, -1, :]\n",
    "                        call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Call stats: {call_stats}\")\n",
    "            \n",
    "        # Decode tokens to actions\n",
    "        predicted_action_token_ids = np.array(generated_token_ids[:action_dim], dtype=np.int64)\n",
    "        \n",
    "        # DEBUG: Show all generated tokens with their action values\n",
    "        print(f\"\\033[38;2;100;200;255m[DEBUG] Final generated tokens summary:\\033[0m\")\n",
    "        for dim_idx, tok_id in enumerate(predicted_action_token_ids):\n",
    "            bin_idx = self._get_action_bin_from_target_token(int(tok_id))\n",
    "            action_val = self._get_continuous_action_from_bin(bin_idx)\n",
    "            print(f\"  dim[{dim_idx}]: target_tok={tok_id}  bin={bin_idx}  normalized_action={action_val:.4f}\")\n",
    "        \n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Predicted action token ids: {predicted_action_token_ids}\")\n",
    "        \n",
    "        # Use target model's decoding (vocab_size - token_id approach)\n",
    "        vocab_size = self.target.vocab_size\n",
    "        discretized_actions = vocab_size - predicted_action_token_ids\n",
    "        discretized_actions = np.clip(discretized_actions - 1, a_min=0, a_max=self.target.bin_centers.shape[0] - 1)\n",
    "        normalized_actions = self.target.bin_centers[discretized_actions]\n",
    "        \n",
    "        # Un-normalize actions\n",
    "        action_norm_stats = self.target.get_action_stats(unnorm_key_target)\n",
    "        mask = action_norm_stats.get(\"mask\", np.ones_like(action_norm_stats[\"q01\"], dtype=bool))\n",
    "        action_high, action_low = np.array(action_norm_stats[\"q99\"]), np.array(action_norm_stats[\"q01\"])\n",
    "        actions = np.where(\n",
    "            mask,\n",
    "            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,\n",
    "            normalized_actions,\n",
    "        )\n",
    "        \n",
    "        # Update global stats\n",
    "        self.stats.total_tokens_generated += call_stats.total_tokens_generated\n",
    "        self.stats.total_draft_tokens_proposed += call_stats.total_draft_tokens_proposed\n",
    "        self.stats.total_draft_tokens_accepted += call_stats.total_draft_tokens_accepted\n",
    "        self.stats.total_target_forward_passes += call_stats.total_target_forward_passes\n",
    "        self.stats.total_draft_forward_passes += call_stats.total_draft_forward_passes\n",
    "        \n",
    "        return actions, call_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLASpeculativeDecoderDD:\n",
    "    \"\"\"\n",
    "    Speculative decoding for VLA models.\n",
    "    \n",
    "    Uses a draft model (MiniVLA) to propose action tokens and a target model \n",
    "    (OpenVLA) to verify them.\n",
    "    \n",
    "    IMPORTANT: For speculative decoding to work correctly, both models should\n",
    "    share the same tokenizer/vocabulary. If they don't, action token remapping\n",
    "    is attempted but this only works for action tokens (last 256 tokens of vocab).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model,\n",
    "        draft_model,\n",
    "        target_processor=None,\n",
    "        gamma: int = 4,  # Number of draft tokens to propose at once\n",
    "        use_cache: bool = True,\n",
    "        temperature: float = 0.0,  # 0 = greedy/argmax\n",
    "        n_action_bins: int = 256,  # Number of action bins (tokens)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the speculative decoder.\n",
    "        \n",
    "        Args:\n",
    "            target_model: OpenVLA model (larger, slower, more accurate)\n",
    "            draft_model: MiniVLA model (smaller, faster)\n",
    "            target_processor: HuggingFace processor for target model\n",
    "            gamma: Number of tokens to speculate at each step\n",
    "            use_cache: Whether to use KV caching\n",
    "            temperature: Sampling temperature (0 = greedy/argmax)\n",
    "            n_action_bins: Number of action token bins (typically 256)\n",
    "        \"\"\"\n",
    "        self.target = target_model\n",
    "        self.draft = draft_model\n",
    "        self.target_processor = target_processor\n",
    "        self.gamma = gamma\n",
    "        self.use_cache = use_cache\n",
    "        self.temperature = temperature\n",
    "        self.n_action_bins = n_action_bins\n",
    "        \n",
    "        # Get device\n",
    "        self.device = next(target_model.parameters()).device\n",
    "        \n",
    "        # Stats tracking\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "        \n",
    "        # Check vocabulary compatibility and setup token mapping\n",
    "        self._setup_token_mapping()\n",
    "    \n",
    "    def _setup_token_mapping(self):\n",
    "        \"\"\"Setup token mapping between draft and target vocabularies.\"\"\"\n",
    "        # Get vocabulary sizes\n",
    "        # Target model (OpenVLA/HF style) - use ACTUAL embedding dimension, not vocab_size attribute\n",
    "        # The embedding may be padded to \"multiple of\" for efficiency\n",
    "        if hasattr(self.target, 'language_model') and hasattr(self.target.language_model, 'model'):\n",
    "            # Actual embedding dimension (includes padding)\n",
    "            self.target_logit_dim = self.target.language_model.model.embed_tokens.weight.shape[0]\n",
    "        elif hasattr(self.target, 'get_output_embeddings'):\n",
    "            self.target_logit_dim = self.target.get_output_embeddings().weight.shape[0]\n",
    "        else:\n",
    "            self.target_logit_dim = self.target.config.vocab_size\n",
    "        \n",
    "        # Also get the \"logical\" vocab size (without padding) for action token calculation\n",
    "        if hasattr(self.target, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.vocab_size\n",
    "        elif hasattr(self.target, 'config') and hasattr(self.target.config, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.config.vocab_size\n",
    "        else:\n",
    "            self.target_vocab_size = self.target_logit_dim\n",
    "        \n",
    "        # Draft model (Prismatic style)\n",
    "        if hasattr(self.draft, 'llm_backbone'):\n",
    "            draft_tokenizer = self.draft.llm_backbone.tokenizer\n",
    "            # Qwen2 uses len(tokenizer) for full vocab including added tokens\n",
    "            self.draft_vocab_size = len(draft_tokenizer) if hasattr(draft_tokenizer, '__len__') else draft_tokenizer.vocab_size\n",
    "            # Get actual logit dimension from draft model\n",
    "            if hasattr(self.draft.llm_backbone, 'llm') and hasattr(self.draft.llm_backbone.llm, 'lm_head'):\n",
    "                self.draft_logit_dim = self.draft.llm_backbone.llm.lm_head.weight.shape[0]\n",
    "            else:\n",
    "                self.draft_logit_dim = self.draft_vocab_size\n",
    "        else:\n",
    "            self.draft_vocab_size = self.draft.config.vocab_size\n",
    "            self.draft_logit_dim = self.draft_vocab_size\n",
    "        \n",
    "        # Check if vocabularies match\n",
    "        self.vocab_compatible = (self.target_logit_dim == self.draft_logit_dim)\n",
    "        \n",
    "        # Compute action token ranges\n",
    "        # Action tokens are the LAST n_action_bins tokens before padding\n",
    "        # For target: use vocab_size (not padded logit_dim)\n",
    "        self.target_action_start = self.target_vocab_size - self.n_action_bins\n",
    "        self.draft_action_start = self.draft_vocab_size - self.n_action_bins\n",
    "        \n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Target vocab_size: {self.target_vocab_size}, logit_dim: {self.target_logit_dim}, action tokens: [{self.target_action_start}, {self.target_vocab_size})\")\n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Draft vocab_size: {self.draft_vocab_size}, logit_dim: {self.draft_logit_dim}, action tokens: [{self.draft_action_start}, {self.draft_vocab_size})\")\n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Vocabularies compatible: {self.vocab_compatible}\")\n",
    "        \n",
    "        if not self.vocab_compatible:\n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"WARNING: Vocabulary mismatch! Token remapping will be used for action tokens only.\")\n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"This may affect acceptance rates. Consider using models with matching tokenizers.\")\n",
    "        \n",
    "        # DEBUG: Verify mapping with example tokens\n",
    "        print(\"\\033[38;2;100;200;255m[DEBUG] Token mapping verification examples:\\033[0m\")\n",
    "        for bin_idx in [0, 127, 255]:  # First, middle, last action bins\n",
    "            draft_tok = self.draft_action_start + bin_idx\n",
    "            target_tok = self._draft_token_to_target(draft_tok)\n",
    "            draft_bin = self._get_action_bin_from_draft_token(draft_tok)\n",
    "            target_bin = self._get_action_bin_from_target_token(target_tok)\n",
    "            print(f\"  Bin {bin_idx}: draft_token={draft_tok}  target_token={target_tok} | draft_bin={draft_bin}, target_bin={target_bin} | {'' if draft_bin == target_bin else ''}\")\n",
    "    \n",
    "    def _draft_token_to_target(self, draft_token_id: int) -> int:\n",
    "        \"\"\"Map a draft token ID to target vocabulary.\"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return draft_token_id\n",
    "        \n",
    "        # Check if this is an action token (from end of draft vocabulary)\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            # Map to corresponding action token in target vocabulary\n",
    "            action_bin = draft_token_id - self.draft_action_start\n",
    "            target_token = self.target_action_start + action_bin\n",
    "            return target_token\n",
    "        else:\n",
    "            # Non-action token - this shouldn't happen during action generation\n",
    "            # Return as-is but clamp to valid range\n",
    "            return min(draft_token_id, self.target_vocab_size - 1)\n",
    "    \n",
    "    def _target_token_to_draft(self, target_token_id: int) -> int:\n",
    "        \"\"\"Map a target token ID to draft vocabulary.\"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return target_token_id\n",
    "        \n",
    "        # Check if this is an action token\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            action_bin = target_token_id - self.target_action_start\n",
    "            draft_token = self.draft_action_start + action_bin\n",
    "            return draft_token\n",
    "        else:\n",
    "            return min(target_token_id, self.draft_vocab_size - 1)\n",
    "    \n",
    "    def _remap_logits_draft_to_target(self, draft_logits: torch.Tensor, target_logit_dim: int = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Remap draft logits to target vocabulary space.\n",
    "        Only remaps action tokens; non-action tokens get -inf.\n",
    "        \n",
    "        Args:\n",
    "            draft_logits: Logits from draft model\n",
    "            target_logit_dim: Actual dimension of target logits (may differ from vocab_size due to padding)\n",
    "        \"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return draft_logits\n",
    "        \n",
    "        # Use provided dimension or fall back to stored logit_dim\n",
    "        if target_logit_dim is None:\n",
    "            target_logit_dim = self.target_logit_dim\n",
    "        \n",
    "        # Create target-sized logits filled with -inf (use actual logit dimension, not vocab_size)\n",
    "        target_logits = torch.full(\n",
    "            (draft_logits.shape[0], target_logit_dim),\n",
    "            float('-inf'),\n",
    "            device=draft_logits.device,\n",
    "            dtype=draft_logits.dtype\n",
    "        )\n",
    "        \n",
    "        # Copy action token logits from draft to corresponding target positions\n",
    "        # Draft action tokens: [draft_action_start, draft_vocab_size)\n",
    "        # Target action tokens: [target_action_start, target_vocab_size)\n",
    "        draft_action_logits = draft_logits[:, self.draft_action_start:self.draft_vocab_size]\n",
    "        target_logits[:, self.target_action_start:self.target_vocab_size] = draft_action_logits\n",
    "        \n",
    "        return target_logits\n",
    "    \n",
    "    # =========================================================================\n",
    "    # DEBUG: Token mapping verification methods\n",
    "    # =========================================================================\n",
    "    \n",
    "    def _get_action_bin_from_draft_token(self, draft_token_id: int) -> int:\n",
    "        \"\"\"Get action bin index from draft token ID.\"\"\"\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            return draft_token_id - self.draft_action_start\n",
    "        return -1  # Not an action token\n",
    "    \n",
    "    def _get_action_bin_from_target_token(self, target_token_id: int) -> int:\n",
    "        \"\"\"Get action bin index from target token ID.\"\"\"\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            return target_token_id - self.target_action_start\n",
    "        return -1  # Not an action token\n",
    "    \n",
    "    def _get_continuous_action_from_bin(self, bin_idx: int) -> float:\n",
    "        \"\"\"Convert action bin index to continuous action value using target's bin centers.\"\"\"\n",
    "        if 0 <= bin_idx < len(self.target.bin_centers):\n",
    "            return self.target.bin_centers[bin_idx]\n",
    "        return float('nan')\n",
    "    \n",
    "    def _debug_token_mapping(self, draft_token_id: int, target_token_id: int, prefix: str = \"\"):\n",
    "        \"\"\"Debug print showing token mapping verification.\"\"\"\n",
    "        draft_bin = self._get_action_bin_from_draft_token(draft_token_id)\n",
    "        target_bin = self._get_action_bin_from_target_token(target_token_id)\n",
    "        \n",
    "        draft_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "        target_action = self._get_continuous_action_from_bin(target_bin)\n",
    "        \n",
    "        match_status = \" MATCH\" if draft_bin == target_bin else \" MISMATCH\"\n",
    "        \n",
    "        print(f\"\\033[38;2;100;200;255m[DEBUG TOKEN MAP] {prefix}\\033[0m\")\n",
    "        print(f\"  Draft token:  {draft_token_id}  bin {draft_bin}  action {draft_action:.4f}\")\n",
    "        print(f\"  Target token: {target_token_id}  bin {target_bin}  action {target_action:.4f}\")\n",
    "        print(f\"  Bins match: {match_status}\")\n",
    "        \n",
    "        return draft_bin == target_bin\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        \"\"\"Reset statistics counters.\"\"\"\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "    \n",
    "    def _sample_token(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Sample a token from logits.\"\"\"\n",
    "        if self.temperature <= 0:\n",
    "            return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            probs = F.softmax(logits / self.temperature, dim=-1)\n",
    "            return torch.multinomial(probs.squeeze(0), num_samples=1).unsqueeze(0)\n",
    "    \n",
    "    def _get_probs(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert logits to probabilities.\"\"\"\n",
    "        if self.temperature <= 0:\n",
    "            # For greedy, use a very low temperature to approximate argmax\n",
    "            return F.softmax(logits / 0.01, dim=-1)\n",
    "        return F.softmax(logits / self.temperature, dim=-1)\n",
    "    \n",
    "    def _prepare_target_inputs(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Prepare inputs for the target (OpenVLA) model.\"\"\"\n",
    "        prompt = f\"In: What action should the robot take to {instruction.lower()}?\\nOut:\"\n",
    "        inputs = self.target_processor(prompt, image).to(self.device, dtype=torch.bfloat16)\n",
    "        \n",
    "        # IMPORTANT: Add the special empty token (29871) to input_ids if not present https://archive.is/20240927102623/https://medium.com/@manyi.yim/in-depth-understanding-of-llama-tokenizer-d91777025dab#selection-1151.5-1151.109\n",
    "        # This is what OpenVLA's predict_action does - the action tokens come AFTER this token\n",
    "            #if not torch.all(input_ids[:, -1] == 29871):\n",
    "            # input_ids = torch.cat((input_ids, torch.Tensor([29871]).long()), dim=1)\n",
    "            # generated_ids = self.generate(input_ids, max_new_tokens=action_dim, ...)\n",
    "        if not torch.all(inputs[\"input_ids\"][:, -1] == 29871):\n",
    "            inputs[\"input_ids\"] = torch.cat(\n",
    "                (inputs[\"input_ids\"], torch.tensor([[29871]], device=self.device)),\n",
    "                dim=1\n",
    "            )\n",
    "            # Also extend attention mask\n",
    "            if \"attention_mask\" in inputs:\n",
    "                inputs[\"attention_mask\"] = torch.cat(\n",
    "                    (inputs[\"attention_mask\"], torch.ones((1, 1), device=self.device, dtype=inputs[\"attention_mask\"].dtype)),\n",
    "                    dim=1\n",
    "                )\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def _prepare_draft_inputs(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Prepare inputs for the draft (MiniVLA) model.\"\"\"\n",
    "        # Get prompt using draft model's prompt builder\n",
    "        prompt_builder = self.draft.get_prompt_builder()\n",
    "        prompt_builder.add_turn(role=\"human\", message=f\"What action should the robot take to {instruction.lower()}?\")\n",
    "        prompt_text = prompt_builder.get_prompt()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenizer = self.draft.llm_backbone.tokenizer\n",
    "        input_ids = tokenizer(prompt_text, truncation=True, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        \n",
    "        # Handle special token for Llama tokenizer\n",
    "        from transformers import LlamaTokenizerFast\n",
    "        if isinstance(tokenizer, LlamaTokenizerFast):\n",
    "            if not torch.all(input_ids[:, -1] == 29871):\n",
    "                input_ids = torch.cat(\n",
    "                    (input_ids, torch.tensor([[29871]], device=self.device)),\n",
    "                    dim=1\n",
    "                )\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Process image\n",
    "        image_transform = self.draft.vision_backbone.get_image_transform()\n",
    "        pixel_values = image_transform(image)\n",
    "        if isinstance(pixel_values, torch.Tensor):\n",
    "            pixel_values = pixel_values[None, ...].to(self.device)\n",
    "        elif isinstance(pixel_values, dict):\n",
    "            pixel_values = {k: v[None, ...].to(self.device) for k, v in pixel_values.items()}\n",
    "        \n",
    "        return input_ids, attention_mask, pixel_values\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict_action_speculative(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "        unnorm_key_target: str,\n",
    "    ) -> Tuple[np.ndarray, SpeculativeDecodingStats]:\n",
    "        \"\"\"\n",
    "        Generate action using speculative decoding.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image observation\n",
    "            instruction: Task instruction string\n",
    "            unnorm_key_target: Key for action un-normalization statistics of the target model\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (unnormalized action array, decoding statistics)\n",
    "        \"\"\"\n",
    "        # Reset per-call stats\n",
    "        call_stats = SpeculativeDecodingStats()\n",
    "        \n",
    "        action_dim = self.target.get_action_dim(unnorm_key_target)\n",
    "        \n",
    "        \n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Target vocab size: {self.target.vocab_size}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Target embedding size: {self.target.language_model.model.embed_tokens.weight.shape[0]}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Draft tokenizer vocab size: {self.draft.llm_backbone.tokenizer.vocab_size}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Action dim: {action_dim}\")\n",
    "        \n",
    "        # Prepare inputs for both models\n",
    "        target_inputs = self._prepare_target_inputs(image, instruction)\n",
    "        draft_input_ids, draft_attention_mask, draft_pixel_values = self._prepare_draft_inputs(image, instruction)\n",
    "        \n",
    "        # Cast draft inputs to appropriate dtype\n",
    "        autocast_dtype = self.draft.llm_backbone.half_precision_dtype\n",
    "        \n",
    "        # Initialize caches\n",
    "        target_cache = None\n",
    "        draft_cache = None\n",
    "        \n",
    "        generated_token_ids = []\n",
    "        \n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16, enabled=True):\n",
    "            # === Initial forward pass to get KV cache and initial logits ===\n",
    "            # NOTE: After adding token 29871 to input, the model should output action tokens directly\n",
    "            \n",
    "            # Target model initial forward (prefill)\n",
    "            target_out = self.target(\n",
    "                **target_inputs,\n",
    "                past_key_values=None,\n",
    "                use_cache=self.use_cache,\n",
    "            )\n",
    "            target_cache = target_out.past_key_values\n",
    "            target_logits = target_out.logits[:, -1, :]\n",
    "            call_stats.total_target_forward_passes += 1\n",
    "            \n",
    "            # DEBUG: Check what the target model wants to output first\n",
    "            top_target_token = torch.argmax(target_logits, dim=-1).item()\n",
    "            top_target_bin = self._get_action_bin_from_target_token(top_target_token)\n",
    "            print(f\"\\033[38;2;100;200;255m[DEBUG] After prefill, target top token:\\033[0m\")\n",
    "            print(f\"  top_token={top_target_token}  bin={top_target_bin}  is_action_token={top_target_bin >= 0}\")\n",
    "            \n",
    "            # Draft model initial forward (prefill)\n",
    "            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                draft_out = self.draft(\n",
    "                    input_ids=draft_input_ids,\n",
    "                    attention_mask=draft_attention_mask,\n",
    "                    pixel_values=draft_pixel_values,\n",
    "                    past_key_values=None,\n",
    "                    use_cache=self.use_cache,\n",
    "                )\n",
    "            draft_cache = draft_out.past_key_values\n",
    "            draft_logits = draft_out.logits[:, -1, :]\n",
    "            call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            # DEBUG: Check what the draft model wants to output first\n",
    "            top_draft_token = torch.argmax(draft_logits, dim=-1).item()\n",
    "            top_draft_bin = self._get_action_bin_from_draft_token(top_draft_token)\n",
    "            print(f\"\\033[38;2;100;200;255m[DEBUG] After prefill, draft top token:\\033[0m\")\n",
    "            print(f\"  top_token={top_draft_token}  bin={top_draft_bin}  is_action_token={top_draft_bin >= 0}\")\n",
    "            \n",
    "            # === Main speculative decoding loop ===\n",
    "            # We start directly with drafting - no need to sample a first token separately\n",
    "            while len(generated_token_ids) < action_dim:\n",
    "                # Determine how many tokens to speculate\n",
    "                gamma = min(self.gamma, action_dim - len(generated_token_ids))\n",
    "                \n",
    "                # Generate gamma draft tokens\n",
    "                draft_tokens = []\n",
    "                draft_probs_list = []\n",
    "                \n",
    "                current_draft_cache = draft_cache\n",
    "                current_draft_logits = draft_logits\n",
    "                \n",
    "                for _ in range(gamma):\n",
    "                    draft_probs = self._get_probs(current_draft_logits)\n",
    "                    draft_token = self._sample_token(current_draft_logits)\n",
    "                    \n",
    "                    draft_tokens.append(draft_token)\n",
    "                    draft_probs_list.append(draft_probs)\n",
    "                    \n",
    "                    # Advance draft model\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=draft_token.to(self.device),\n",
    "                            past_key_values=current_draft_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                    current_draft_cache = draft_step.past_key_values\n",
    "                    current_draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                \n",
    "                call_stats.total_draft_tokens_proposed += gamma\n",
    "                \n",
    "                # Map draft tokens to target vocab space for verification\n",
    "                draft_token_ids_target = []\n",
    "                print(f\"\\033[38;2;100;200;255m[DEBUG] Mapping {gamma} draft tokens to target vocab:\\033[0m\")\n",
    "                for idx, dt in enumerate(draft_tokens):\n",
    "                    draft_id = dt.item()\n",
    "                    target_id = self._draft_token_to_target(draft_id)\n",
    "                    draft_token_ids_target.append(target_id)\n",
    "                    \n",
    "                    # Verify mapping preserves action bin\n",
    "                    draft_bin = self._get_action_bin_from_draft_token(draft_id)\n",
    "                    target_bin = self._get_action_bin_from_target_token(target_id)\n",
    "                    draft_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "                    target_action = self._get_continuous_action_from_bin(target_bin)\n",
    "                    match = \"\" if draft_bin == target_bin else \" MISMATCH!\"\n",
    "                    print(f\"  [{idx}] draft_tok={draft_id}  target_tok={target_id} | bin: {draft_bin}{target_bin} | action: {draft_action:.4f}{target_action:.4f} {match}\")\n",
    "                \n",
    "                # Verify with target model - run all gamma tokens through\n",
    "                target_cache_for_verify = target_cache\n",
    "                target_logits_list = []\n",
    "                \n",
    "                for i in range(gamma):\n",
    "                    target_token_input = torch.tensor([[draft_token_ids_target[i]]], device=self.device)\n",
    "                    target_step = self.target(\n",
    "                        input_ids=target_token_input,\n",
    "                        past_key_values=target_cache_for_verify,\n",
    "                        use_cache=self.use_cache,\n",
    "                    )\n",
    "                    target_cache_for_verify = target_step.past_key_values\n",
    "                    target_logits_list.append(target_step.logits[:, -1:, :])\n",
    "                \n",
    "                call_stats.total_target_forward_passes += gamma\n",
    "                \n",
    "                # Stack target logits\n",
    "                target_logits_batch = torch.cat(target_logits_list, dim=1)  # [1, gamma, actual_vocab_dim]\n",
    "                target_probs_batch = self._get_probs(target_logits_batch)\n",
    "                                \n",
    "                # Get actual target logit dimension from the output\n",
    "                actual_target_logit_dim = target_logits_batch.shape[-1]\n",
    "                \n",
    "                # Remap draft probs to target vocab space for comparison\n",
    "                # Use actual target logit dimension to ensure tensor size match\n",
    "                draft_probs_remapped = [self._remap_logits_draft_to_target(dp, actual_target_logit_dim) for dp in draft_probs_list]\n",
    "                draft_probs_remapped = [self._get_probs(dp) for dp in draft_probs_remapped]\n",
    "                \n",
    "                print(f\"\\033[38;2;100;200;255m[SRP] -> \\033[0m\", f\"[DEBUG] Target probs batch: {target_probs_batch}\")\n",
    "                print(f\"\\033[38;2;100;200;255m[SRP] -> \\033[0m\", f\"[DEBUG] Draft probs list: {draft_probs_list}\")\n",
    "\n",
    "                \n",
    "                # Rejection sampling loop\n",
    "                n_accepted = 0\n",
    "                for i in range(gamma):\n",
    "                    draft_token_id_draft = draft_tokens[i].item()  # In draft vocab\n",
    "                    draft_token_id_target = draft_token_ids_target[i]  # Mapped to target vocab\n",
    "                    \n",
    "                    draft_prob_remapped = draft_probs_remapped[i]\n",
    "                    target_prob = target_probs_batch[:, i, :]\n",
    "                    \n",
    "                    # Get probability of the token under both models (in target vocab space)\n",
    "                    p_target = target_prob[0, draft_token_id_target].item()\n",
    "                    p_draft = draft_prob_remapped[0, draft_token_id_target].item()\n",
    "                    \n",
    "                    # Rejection sampling\n",
    "                    if p_draft > 0:\n",
    "                        acceptance_prob = min(1.0, p_target / p_draft)\n",
    "                    else:\n",
    "                        acceptance_prob = 1.0 if p_target > 0 else 0.0\n",
    "                    \n",
    "                    if torch.rand(1).item() < acceptance_prob:\n",
    "                        # Accept this token (store in target vocab space)\n",
    "                        accepted_bin = self._get_action_bin_from_target_token(draft_token_id_target)\n",
    "                        accepted_action = self._get_continuous_action_from_bin(accepted_bin)\n",
    "                        print(f\"\\033[38;2;0;255;0m[ACCEPT]\\033[0m token[{i}]: target_tok={draft_token_id_target}  bin={accepted_bin}  action={accepted_action:.4f} | p_target={p_target:.4f}, p_draft={p_draft:.4f}, accept_prob={acceptance_prob:.4f}\")\n",
    "                        generated_token_ids.append(draft_token_id_target)\n",
    "                        n_accepted += 1\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        call_stats.total_draft_tokens_accepted += 1\n",
    "                        \n",
    "                        if len(generated_token_ids) >= action_dim:\n",
    "                            print(f\"\\033[38;2;255;165;0m[SRP] -> \\033[0m Generated {len(generated_token_ids)}/{action_dim} tokens, done.\")\n",
    "                            break\n",
    "                    else:\n",
    "                        # Reject - sample from adjusted distribution\n",
    "                        adjusted_probs = max_fn(target_prob - draft_prob_remapped)\n",
    "                        if adjusted_probs.sum() > 0:\n",
    "                            corrected_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
    "                        else:\n",
    "                            corrected_token = self._sample_token(target_prob.unsqueeze(0))\n",
    "                        \n",
    "                        corrected_token_id = int(corrected_token.item())\n",
    "                        rejected_bin = self._get_action_bin_from_target_token(draft_token_id_target)\n",
    "                        corrected_bin = self._get_action_bin_from_target_token(corrected_token_id)\n",
    "                        rejected_action = self._get_continuous_action_from_bin(rejected_bin)\n",
    "                        corrected_action = self._get_continuous_action_from_bin(corrected_bin)\n",
    "                        bin_diff = abs(rejected_bin - corrected_bin) if rejected_bin >= 0 and corrected_bin >= 0 else -1\n",
    "                        \n",
    "                        print(f\"\\033[38;2;255;100;100m[REJECT]\\033[0m token[{i}]: p_target={p_target:.4f} < p_draft={p_draft:.4f}, accept_prob={acceptance_prob:.4f}\")\n",
    "                        print(f\"  Rejected:  target_tok={draft_token_id_target}  bin={rejected_bin}  action={rejected_action:.4f}\")\n",
    "                        print(f\"  Corrected: target_tok={corrected_token_id}  bin={corrected_bin}  action={corrected_action:.4f}\")\n",
    "                        print(f\"  Bin difference: {bin_diff} | Action difference: {abs(rejected_action - corrected_action):.4f}\")\n",
    "                        \n",
    "                        # Store corrected token (already in target vocab space)\n",
    "                        generated_token_ids.append(corrected_token_id)\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        n_accepted = i  # Number of accepted tokens (before this rejection)\n",
    "                        break\n",
    "                                    \n",
    "                # Update caches after acceptance/rejection\n",
    "                if n_accepted == gamma and len(generated_token_ids) < action_dim:\n",
    "                    # All accepted - need to sample one more from target\n",
    "                    print(f\"\\033[38;2;0;255;0m[ALL ACCEPTED]\\033[0m All {gamma} draft tokens accepted! Sampling bonus token from target...\")\n",
    "                    target_cache = target_cache_for_verify\n",
    "                    target_logits = target_logits_list[-1].squeeze(1)\n",
    "                    \n",
    "                    # Sample additional token from target (in target vocab space)\n",
    "                    bonus_token_target = self._sample_token(target_logits)\n",
    "                    bonus_token_id_target = int(bonus_token_target.item())\n",
    "                    bonus_bin = self._get_action_bin_from_target_token(bonus_token_id_target)\n",
    "                    bonus_action = self._get_continuous_action_from_bin(bonus_bin)\n",
    "                    print(f\"  Bonus token: target_tok={bonus_token_id_target}  bin={bonus_bin}  action={bonus_action:.4f}\")\n",
    "                    generated_token_ids.append(bonus_token_id_target)\n",
    "                    call_stats.total_tokens_generated += 1\n",
    "                    \n",
    "                    # Update target cache\n",
    "                    target_step = self.target(\n",
    "                        input_ids=bonus_token_target,\n",
    "                        past_key_values=target_cache,\n",
    "                        use_cache=self.use_cache,\n",
    "                    )\n",
    "                    target_cache = target_step.past_key_values\n",
    "                    target_logits = target_step.logits[:, -1, :]\n",
    "                    call_stats.total_target_forward_passes += 1\n",
    "                    \n",
    "                    # Map bonus token to draft vocab and update draft cache\n",
    "                    bonus_token_id_draft = self._target_token_to_draft(bonus_token_id_target)\n",
    "                    bonus_draft_bin = self._get_action_bin_from_draft_token(bonus_token_id_draft)\n",
    "                    print(f\"  Mapped to draft: draft_tok={bonus_token_id_draft}  bin={bonus_draft_bin} {'' if bonus_bin == bonus_draft_bin else ' MISMATCH!'}\")\n",
    "                    bonus_token_draft = torch.tensor([[bonus_token_id_draft]], device=self.device)\n",
    "                    \n",
    "                    draft_cache = current_draft_cache\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=bonus_token_draft,\n",
    "                            past_key_values=draft_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                    draft_cache = draft_step.past_key_values\n",
    "                    draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                    \n",
    "                else:\n",
    "                    # Some tokens rejected - prune caches\n",
    "                    tokens_to_discard = gamma - n_accepted\n",
    "                    if tokens_to_discard > 0 and self.use_cache:\n",
    "                        # We need to prune and resync\n",
    "                        # Use the cache state after the accepted tokens\n",
    "                        target_cache = target_cache_for_verify\n",
    "                        if tokens_to_discard > 0:\n",
    "                            target_cache = prune_cache(target_cache, tokens_to_discard)\n",
    "                        \n",
    "                        # Rebuild draft cache\n",
    "                        draft_cache = prune_cache(current_draft_cache, gamma - n_accepted)\n",
    "                    \n",
    "                    # Get logits for next round\n",
    "                    if len(generated_token_ids) < action_dim:\n",
    "                        # Last token is in target vocab space\n",
    "                        last_token_id_target = generated_token_ids[-1]\n",
    "                        last_token_target = torch.tensor([[last_token_id_target]], device=self.device)\n",
    "                        \n",
    "                        target_step = self.target(\n",
    "                            input_ids=last_token_target,\n",
    "                            past_key_values=target_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                        target_cache = target_step.past_key_values\n",
    "                        target_logits = target_step.logits[:, -1, :]\n",
    "                        call_stats.total_target_forward_passes += 1\n",
    "                        \n",
    "                        # Map to draft vocab for draft model\n",
    "                        last_token_id_draft = self._target_token_to_draft(last_token_id_target)\n",
    "                        last_token_draft = torch.tensor([[last_token_id_draft]], device=self.device)\n",
    "                        \n",
    "                        with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                            draft_step = self.draft(\n",
    "                                input_ids=last_token_draft,\n",
    "                                past_key_values=draft_cache,\n",
    "                                use_cache=self.use_cache,\n",
    "                            )\n",
    "                        draft_cache = draft_step.past_key_values\n",
    "                        draft_logits = draft_step.logits[:, -1, :]\n",
    "                        call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Call stats: {call_stats}\")\n",
    "            \n",
    "        # Decode tokens to actions\n",
    "        predicted_action_token_ids = np.array(generated_token_ids[:action_dim], dtype=np.int64)\n",
    "        \n",
    "        # DEBUG: Show all generated tokens with their action values\n",
    "        print(f\"\\033[38;2;100;200;255m[DEBUG] Final generated tokens summary:\\033[0m\")\n",
    "        for dim_idx, tok_id in enumerate(predicted_action_token_ids):\n",
    "            bin_idx = self._get_action_bin_from_target_token(int(tok_id))\n",
    "            action_val = self._get_continuous_action_from_bin(bin_idx)\n",
    "            print(f\"  dim[{dim_idx}]: target_tok={tok_id}  bin={bin_idx}  normalized_action={action_val:.4f}\")\n",
    "        \n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Predicted action token ids: {predicted_action_token_ids}\")\n",
    "        \n",
    "        # Use target model's decoding (vocab_size - token_id approach)\n",
    "        vocab_size = self.target.vocab_size\n",
    "        discretized_actions = vocab_size - predicted_action_token_ids\n",
    "        discretized_actions = np.clip(discretized_actions - 1, a_min=0, a_max=self.target.bin_centers.shape[0] - 1)\n",
    "        normalized_actions = self.target.bin_centers[discretized_actions]\n",
    "        \n",
    "        # Un-normalize actions\n",
    "        action_norm_stats = self.target.get_action_stats(unnorm_key_target)\n",
    "        mask = action_norm_stats.get(\"mask\", np.ones_like(action_norm_stats[\"q01\"], dtype=bool))\n",
    "        action_high, action_low = np.array(action_norm_stats[\"q99\"]), np.array(action_norm_stats[\"q01\"])\n",
    "        actions = np.where(\n",
    "            mask,\n",
    "            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,\n",
    "            normalized_actions,\n",
    "        )\n",
    "        \n",
    "        # Update global stats\n",
    "        self.stats.total_tokens_generated += call_stats.total_tokens_generated\n",
    "        self.stats.total_draft_tokens_proposed += call_stats.total_draft_tokens_proposed\n",
    "        self.stats.total_draft_tokens_accepted += call_stats.total_draft_tokens_accepted\n",
    "        self.stats.total_target_forward_passes += call_stats.total_target_forward_passes\n",
    "        self.stats.total_draft_forward_passes += call_stats.total_draft_forward_passes\n",
    "        \n",
    "        return actions, call_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLASpeculativeDecoderDDD:\n",
    "    \"\"\"\n",
    "    Speculative decoding for VLA models.\n",
    "    \n",
    "    Uses a draft model (MiniVLA) to propose action tokens and a target model \n",
    "    (OpenVLA) to verify them.\n",
    "    \n",
    "    IMPORTANT: For speculative decoding to work correctly, both models should\n",
    "    share the same tokenizer/vocabulary. If they don't, action token remapping\n",
    "    is attempted but this only works for action tokens (last 256 tokens of vocab).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model,\n",
    "        draft_model,\n",
    "        target_processor=None,\n",
    "        gamma: int = 4,  # Number of draft tokens to propose at once\n",
    "        use_cache: bool = True,\n",
    "        temperature: float = 0.0,  # 0 = greedy/argmax\n",
    "        n_action_bins: int = 256,  # Number of action bins (tokens)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the speculative decoder.\n",
    "        \n",
    "        Args:\n",
    "            target_model: OpenVLA model (larger, slower, more accurate)\n",
    "            draft_model: MiniVLA model (smaller, faster)\n",
    "            target_processor: HuggingFace processor for target model\n",
    "            gamma: Number of tokens to speculate at each step\n",
    "            use_cache: Whether to use KV caching\n",
    "            temperature: Sampling temperature (0 = greedy/argmax)\n",
    "            n_action_bins: Number of action token bins (typically 256)\n",
    "        \"\"\"\n",
    "        self.target = target_model\n",
    "        self.draft = draft_model\n",
    "        self.target_processor = target_processor\n",
    "        self.gamma = gamma\n",
    "        self.use_cache = use_cache\n",
    "        self.temperature = temperature\n",
    "        self.n_action_bins = n_action_bins\n",
    "        \n",
    "        # Get device\n",
    "        self.device = next(target_model.parameters()).device\n",
    "        \n",
    "        # Stats tracking\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "        \n",
    "        # Check vocabulary compatibility and setup token mapping\n",
    "        self._setup_token_mapping()\n",
    "    \n",
    "    def _setup_token_mapping(self):\n",
    "        \"\"\"Setup token mapping between draft and target vocabularies.\"\"\"\n",
    "        # Get vocabulary sizes\n",
    "        # Target model (OpenVLA/HF style) - use ACTUAL embedding dimension, not vocab_size attribute\n",
    "        # The embedding may be padded to \"multiple of\" for efficiency\n",
    "        if hasattr(self.target, 'language_model') and hasattr(self.target.language_model, 'model'):\n",
    "            # Actual embedding dimension (includes padding)\n",
    "            self.target_logit_dim = self.target.language_model.model.embed_tokens.weight.shape[0]\n",
    "        elif hasattr(self.target, 'get_output_embeddings'):\n",
    "            self.target_logit_dim = self.target.get_output_embeddings().weight.shape[0]\n",
    "        else:\n",
    "            self.target_logit_dim = self.target.config.vocab_size\n",
    "        \n",
    "        # Also get the \"logical\" vocab size (without padding) for action token calculation\n",
    "        if hasattr(self.target, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.vocab_size\n",
    "        elif hasattr(self.target, 'config') and hasattr(self.target.config, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.config.vocab_size\n",
    "        else:\n",
    "            self.target_vocab_size = self.target_logit_dim\n",
    "        \n",
    "        # Draft model (Prismatic style)\n",
    "        if hasattr(self.draft, 'llm_backbone'):\n",
    "            draft_tokenizer = self.draft.llm_backbone.tokenizer\n",
    "            # Qwen2 uses len(tokenizer) for full vocab including added tokens\n",
    "            self.draft_vocab_size = len(draft_tokenizer) if hasattr(draft_tokenizer, '__len__') else draft_tokenizer.vocab_size\n",
    "            # Get actual logit dimension from draft model\n",
    "            if hasattr(self.draft.llm_backbone, 'llm') and hasattr(self.draft.llm_backbone.llm, 'lm_head'):\n",
    "                self.draft_logit_dim = self.draft.llm_backbone.llm.lm_head.weight.shape[0]\n",
    "            else:\n",
    "                self.draft_logit_dim = self.draft_vocab_size\n",
    "        else:\n",
    "            self.draft_vocab_size = self.draft.config.vocab_size\n",
    "            self.draft_logit_dim = self.draft_vocab_size\n",
    "        \n",
    "        # Check if vocabularies match\n",
    "        self.vocab_compatible = (self.target_logit_dim == self.draft_logit_dim)\n",
    "        \n",
    "        # Compute action token ranges\n",
    "        # Action tokens are the LAST n_action_bins tokens before padding\n",
    "        # For target: use vocab_size (not padded logit_dim)\n",
    "        self.target_action_start = self.target_vocab_size - self.n_action_bins\n",
    "        self.draft_action_start = self.draft_vocab_size - self.n_action_bins\n",
    "        \n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Target vocab_size: {self.target_vocab_size}, logit_dim: {self.target_logit_dim}, action tokens: [{self.target_action_start}, {self.target_vocab_size})\")\n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Draft vocab_size: {self.draft_vocab_size}, logit_dim: {self.draft_logit_dim}, action tokens: [{self.draft_action_start}, {self.draft_vocab_size})\")\n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Vocabularies compatible: {self.vocab_compatible}\")\n",
    "        \n",
    "        if not self.vocab_compatible:\n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"WARNING: Vocabulary mismatch! Token remapping will be used for action tokens only.\")\n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"This may affect acceptance rates. Consider using models with matching tokenizers.\")\n",
    "        \n",
    "        # DEBUG: Verify mapping with example tokens\n",
    "        print(\"\\033[38;2;100;200;255m[DEBUG] Token mapping verification examples:\\033[0m\")\n",
    "        for bin_idx in [0, 127, 255]:  # First, middle, last action bins\n",
    "            draft_tok = self.draft_action_start + bin_idx\n",
    "            target_tok = self._draft_token_to_target(draft_tok)\n",
    "            draft_bin = self._get_action_bin_from_draft_token(draft_tok)\n",
    "            target_bin = self._get_action_bin_from_target_token(target_tok)\n",
    "            print(f\"  Bin {bin_idx}: draft_token={draft_tok}  target_token={target_tok} | draft_bin={draft_bin}, target_bin={target_bin} | {'' if draft_bin == target_bin else ''}\")\n",
    "    \n",
    "    def _draft_token_to_target(self, draft_token_id: int) -> int:\n",
    "        \"\"\"Map a draft token ID to target vocabulary.\"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return draft_token_id\n",
    "        \n",
    "        # Check if this is an action token (from end of draft vocabulary)\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            # Map to corresponding action token in target vocabulary\n",
    "            action_bin = draft_token_id - self.draft_action_start\n",
    "            target_token = self.target_action_start + action_bin\n",
    "            return target_token\n",
    "        else:\n",
    "            # Non-action token - this shouldn't happen during action generation\n",
    "            # Return as-is but clamp to valid range\n",
    "            return min(draft_token_id, self.target_vocab_size - 1)\n",
    "    \n",
    "    def _target_token_to_draft(self, target_token_id: int) -> int:\n",
    "        \"\"\"Map a target token ID to draft vocabulary.\"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return target_token_id\n",
    "        \n",
    "        # Check if this is an action token\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            action_bin = target_token_id - self.target_action_start\n",
    "            draft_token = self.draft_action_start + action_bin\n",
    "            return draft_token\n",
    "        else:\n",
    "            return min(target_token_id, self.draft_vocab_size - 1)\n",
    "    \n",
    "    def _remap_logits_draft_to_target(self, draft_logits: torch.Tensor, target_logit_dim: int = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Remap draft logits to target vocabulary space.\n",
    "        Only remaps action tokens; non-action tokens get -inf.\n",
    "        \n",
    "        Args:\n",
    "            draft_logits: Logits from draft model\n",
    "            target_logit_dim: Actual dimension of target logits (may differ from vocab_size due to padding)\n",
    "        \"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return draft_logits\n",
    "        \n",
    "        # Use provided dimension or fall back to stored logit_dim\n",
    "        if target_logit_dim is None:\n",
    "            target_logit_dim = self.target_logit_dim\n",
    "        \n",
    "        # Create target-sized logits filled with -inf (use actual logit dimension, not vocab_size)\n",
    "        target_logits = torch.full(\n",
    "            (draft_logits.shape[0], target_logit_dim),\n",
    "            float('-inf'),\n",
    "            device=draft_logits.device,\n",
    "            dtype=draft_logits.dtype\n",
    "        )\n",
    "        \n",
    "        # Copy action token logits from draft to corresponding target positions\n",
    "        # Draft action tokens: [draft_action_start, draft_vocab_size)\n",
    "        # Target action tokens: [target_action_start, target_vocab_size)\n",
    "        draft_action_logits = draft_logits[:, self.draft_action_start:self.draft_vocab_size]\n",
    "        target_logits[:, self.target_action_start:self.target_vocab_size] = draft_action_logits\n",
    "        \n",
    "        return target_logits\n",
    "    \n",
    "    # =========================================================================\n",
    "    # DEBUG: Token mapping verification methods\n",
    "    # =========================================================================\n",
    "    \n",
    "    def _get_action_bin_from_draft_token(self, draft_token_id: int) -> int:\n",
    "        \"\"\"Get action bin index from draft token ID.\"\"\"\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            return draft_token_id - self.draft_action_start\n",
    "        return -1  # Not an action token\n",
    "    \n",
    "    def _get_action_bin_from_target_token(self, target_token_id: int) -> int:\n",
    "        \"\"\"Get action bin index from target token ID.\"\"\"\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            return target_token_id - self.target_action_start\n",
    "        return -1  # Not an action token\n",
    "    \n",
    "    def _get_continuous_action_from_bin(self, bin_idx: int) -> float:\n",
    "        \"\"\"Convert action bin index to continuous action value using target's bin centers.\"\"\"\n",
    "        if 0 <= bin_idx < len(self.target.bin_centers):\n",
    "            return self.target.bin_centers[bin_idx]\n",
    "        return float('nan')\n",
    "    \n",
    "    def _debug_token_mapping(self, draft_token_id: int, target_token_id: int, prefix: str = \"\"):\n",
    "        \"\"\"Debug print showing token mapping verification.\"\"\"\n",
    "        draft_bin = self._get_action_bin_from_draft_token(draft_token_id)\n",
    "        target_bin = self._get_action_bin_from_target_token(target_token_id)\n",
    "        \n",
    "        draft_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "        target_action = self._get_continuous_action_from_bin(target_bin)\n",
    "        \n",
    "        match_status = \" MATCH\" if draft_bin == target_bin else \" MISMATCH\"\n",
    "        \n",
    "        print(f\"\\033[38;2;100;200;255m[DEBUG TOKEN MAP] {prefix}\\033[0m\")\n",
    "        print(f\"  Draft token:  {draft_token_id}  bin {draft_bin}  action {draft_action:.4f}\")\n",
    "        print(f\"  Target token: {target_token_id}  bin {target_bin}  action {target_action:.4f}\")\n",
    "        print(f\"  Bins match: {match_status}\")\n",
    "        \n",
    "        return draft_bin == target_bin\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        \"\"\"Reset statistics counters.\"\"\"\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "    \n",
    "    def _sample_token(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Sample a token from logits.\"\"\"\n",
    "        if self.temperature <= 0:\n",
    "            return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            probs = F.softmax(logits / self.temperature, dim=-1)\n",
    "            return torch.multinomial(probs.squeeze(0), num_samples=1).unsqueeze(0)\n",
    "    \n",
    "    def _get_probs(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert logits to probabilities.\"\"\"\n",
    "        if self.temperature <= 0:\n",
    "            # For greedy, use a very low temperature to approximate argmax\n",
    "            return F.softmax(logits / 0.01, dim=-1)\n",
    "        return F.softmax(logits / self.temperature, dim=-1)\n",
    "    \n",
    "    def _prepare_target_inputs(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Prepare inputs for the target (OpenVLA) model.\"\"\"\n",
    "        prompt = f\"In: What action should the robot take to {instruction.lower()}?\\nOut:\"\n",
    "        inputs = self.target_processor(prompt, image).to(self.device, dtype=torch.bfloat16)\n",
    "        \n",
    "        # IMPORTANT: Add the special empty token (29871) to input_ids if not present\n",
    "        # This is what OpenVLA's predict_action does - the action tokens come AFTER this token\n",
    "        if not torch.all(inputs[\"input_ids\"][:, -1] == 29871):\n",
    "            inputs[\"input_ids\"] = torch.cat(\n",
    "                (inputs[\"input_ids\"], torch.tensor([[29871]], device=self.device)),\n",
    "                dim=1\n",
    "            )\n",
    "            # Also extend attention mask\n",
    "            if \"attention_mask\" in inputs:\n",
    "                inputs[\"attention_mask\"] = torch.cat(\n",
    "                    (inputs[\"attention_mask\"], torch.ones((1, 1), device=self.device, dtype=inputs[\"attention_mask\"].dtype)),\n",
    "                    dim=1\n",
    "                )\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def _prepare_draft_inputs(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Prepare inputs for the draft (MiniVLA) model.\"\"\"\n",
    "        # Get prompt using draft model's prompt builder\n",
    "        prompt_builder = self.draft.get_prompt_builder()\n",
    "        prompt_builder.add_turn(role=\"human\", message=f\"What action should the robot take to {instruction.lower()}?\")\n",
    "        prompt_text = prompt_builder.get_prompt()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenizer = self.draft.llm_backbone.tokenizer\n",
    "        input_ids = tokenizer(prompt_text, truncation=True, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        \n",
    "        # Handle special token for Llama tokenizer\n",
    "        from transformers import LlamaTokenizerFast\n",
    "        if isinstance(tokenizer, LlamaTokenizerFast):\n",
    "            if not torch.all(input_ids[:, -1] == 29871):\n",
    "                input_ids = torch.cat(\n",
    "                    (input_ids, torch.tensor([[29871]], device=self.device)),\n",
    "                    dim=1\n",
    "                )\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Process image\n",
    "        image_transform = self.draft.vision_backbone.get_image_transform()\n",
    "        pixel_values = image_transform(image)\n",
    "        if isinstance(pixel_values, torch.Tensor):\n",
    "            pixel_values = pixel_values[None, ...].to(self.device)\n",
    "        elif isinstance(pixel_values, dict):\n",
    "            pixel_values = {k: v[None, ...].to(self.device) for k, v in pixel_values.items()}\n",
    "        \n",
    "        return input_ids, attention_mask, pixel_values\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict_action_speculative(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "        unnorm_key_target: str,\n",
    "    ) -> Tuple[np.ndarray, SpeculativeDecodingStats]:\n",
    "        \"\"\"\n",
    "        Generate action using speculative decoding.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image observation\n",
    "            instruction: Task instruction string\n",
    "            unnorm_key_target: Key for action un-normalization statistics of the target model\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (unnormalized action array, decoding statistics)\n",
    "        \"\"\"\n",
    "        # Reset per-call stats\n",
    "        call_stats = SpeculativeDecodingStats()\n",
    "        \n",
    "        action_dim = self.target.get_action_dim(unnorm_key_target)\n",
    "        \n",
    "        \n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Target vocab size: {self.target.vocab_size}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Target embedding size: {self.target.language_model.model.embed_tokens.weight.shape[0]}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Draft tokenizer vocab size: {self.draft.llm_backbone.tokenizer.vocab_size}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Action dim: {action_dim}\")\n",
    "        \n",
    "        # Prepare inputs for both models\n",
    "        target_inputs = self._prepare_target_inputs(image, instruction)\n",
    "        draft_input_ids, draft_attention_mask, draft_pixel_values = self._prepare_draft_inputs(image, instruction)\n",
    "        \n",
    "        # Cast draft inputs to appropriate dtype\n",
    "        autocast_dtype = self.draft.llm_backbone.half_precision_dtype\n",
    "        \n",
    "        # Initialize caches\n",
    "        target_cache = None\n",
    "        draft_cache = None\n",
    "        \n",
    "        generated_token_ids = []\n",
    "        \n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16, enabled=True):\n",
    "            # === Initial forward pass to get KV cache and initial logits ===\n",
    "            # NOTE: After adding token 29871 to input, the model should output action tokens directly\n",
    "            \n",
    "            # Target model initial forward (prefill)\n",
    "            target_out = self.target(\n",
    "                **target_inputs,\n",
    "                past_key_values=None,\n",
    "                use_cache=self.use_cache,\n",
    "            )\n",
    "            target_cache = target_out.past_key_values\n",
    "            target_logits = target_out.logits[:, -1, :]\n",
    "            call_stats.total_target_forward_passes += 1\n",
    "            \n",
    "            # DEBUG: Check what the target model wants to output first\n",
    "            top_target_token = torch.argmax(target_logits, dim=-1).item()\n",
    "            top_target_bin = self._get_action_bin_from_target_token(top_target_token)\n",
    "            print(f\"\\033[38;2;100;200;255m[DEBUG] After prefill, target top token:\\033[0m\")\n",
    "            print(f\"  top_token={top_target_token}  bin={top_target_bin}  is_action_token={top_target_bin >= 0}\")\n",
    "            \n",
    "            # Draft model initial forward (prefill)\n",
    "            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                draft_out = self.draft(\n",
    "                    input_ids=draft_input_ids,\n",
    "                    attention_mask=draft_attention_mask,\n",
    "                    pixel_values=draft_pixel_values,\n",
    "                    past_key_values=None,\n",
    "                    use_cache=self.use_cache,\n",
    "                )\n",
    "            draft_cache = draft_out.past_key_values\n",
    "            draft_logits = draft_out.logits[:, -1, :]\n",
    "            call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            # DEBUG: Check what the draft model wants to output first\n",
    "            top_draft_token = torch.argmax(draft_logits, dim=-1).item()\n",
    "            top_draft_bin = self._get_action_bin_from_draft_token(top_draft_token)\n",
    "            print(f\"\\033[38;2;100;200;255m[DEBUG] After prefill, draft top token:\\033[0m\")\n",
    "            print(f\"  top_token={top_draft_token}  bin={top_draft_bin}  is_action_token={top_draft_bin >= 0}\")\n",
    "            \n",
    "            # === Main speculative decoding loop ===\n",
    "            # We start directly with drafting - no need to sample a first token separately\n",
    "            while len(generated_token_ids) < action_dim:\n",
    "                # Determine how many tokens to speculate\n",
    "                gamma = min(self.gamma, action_dim - len(generated_token_ids))\n",
    "                \n",
    "                # Generate gamma draft tokens\n",
    "                draft_tokens = []\n",
    "                draft_probs_list = []\n",
    "                \n",
    "                current_draft_cache = draft_cache\n",
    "                current_draft_logits = draft_logits\n",
    "                \n",
    "                for _ in range(gamma):\n",
    "                    draft_probs = self._get_probs(current_draft_logits)\n",
    "                    draft_token = self._sample_token(current_draft_logits)\n",
    "                    \n",
    "                    draft_tokens.append(draft_token)\n",
    "                    draft_probs_list.append(draft_probs)\n",
    "                    \n",
    "                    # Advance draft model\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=draft_token.to(self.device),\n",
    "                            past_key_values=current_draft_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                    current_draft_cache = draft_step.past_key_values\n",
    "                    current_draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                \n",
    "                call_stats.total_draft_tokens_proposed += gamma\n",
    "                \n",
    "                # Map draft tokens to target vocab space for verification\n",
    "                draft_token_ids_target = []\n",
    "                print(f\"\\033[38;2;100;200;255m[DEBUG] Mapping {gamma} draft tokens to target vocab:\\033[0m\")\n",
    "                for idx, dt in enumerate(draft_tokens):\n",
    "                    draft_id = dt.item()\n",
    "                    target_id = self._draft_token_to_target(draft_id)\n",
    "                    draft_token_ids_target.append(target_id)\n",
    "                    \n",
    "                    # Verify mapping preserves action bin\n",
    "                    draft_bin = self._get_action_bin_from_draft_token(draft_id)\n",
    "                    target_bin = self._get_action_bin_from_target_token(target_id)\n",
    "                    draft_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "                    target_action = self._get_continuous_action_from_bin(target_bin)\n",
    "                    match = \"\" if draft_bin == target_bin else \" MISMATCH!\"\n",
    "                    print(f\"  [{idx}] draft_tok={draft_id}  target_tok={target_id} | bin: {draft_bin}{target_bin} | action: {draft_action:.4f}{target_action:.4f} {match}\")\n",
    "                \n",
    "                # Verify with target model\n",
    "                # IMPORTANT: We need to collect logits BEFORE feeding each token\n",
    "                # - target_logits (from prefill or last step) is used to evaluate draft_token[0]\n",
    "                # - logits after feeding token[0] is used to evaluate draft_token[1]\n",
    "                # etc.\n",
    "                \n",
    "                target_cache_for_verify = target_cache\n",
    "                # Start with current target_logits for evaluating first draft token\n",
    "                target_logits_for_verification = [target_logits.unsqueeze(1)]  # [1, 1, vocab]\n",
    "                last_target_logits = None  # Will hold logits after feeding last token\n",
    "                \n",
    "                for i in range(gamma):\n",
    "                    target_token_input = torch.tensor([[draft_token_ids_target[i]]], device=self.device)\n",
    "                    target_step = self.target(\n",
    "                        input_ids=target_token_input,\n",
    "                        past_key_values=target_cache_for_verify,\n",
    "                        use_cache=self.use_cache,\n",
    "                    )\n",
    "                    target_cache_for_verify = target_step.past_key_values\n",
    "                    last_target_logits = target_step.logits[:, -1, :]  # Always save last logits\n",
    "                    # Store logits for evaluating the NEXT token (if there is one)\n",
    "                    if i < gamma - 1:\n",
    "                        target_logits_for_verification.append(target_step.logits[:, -1:, :])\n",
    "                \n",
    "                call_stats.total_target_forward_passes += gamma\n",
    "                \n",
    "                # Stack target logits - now target_logits_for_verification[i] evaluates draft_token[i]\n",
    "                target_logits_batch = torch.cat(target_logits_for_verification, dim=1)  # [1, gamma, actual_vocab_dim]\n",
    "                target_probs_batch = self._get_probs(target_logits_batch)\n",
    "                \n",
    "                # DEBUG: Show which token the target actually wants at each position\n",
    "                print(f\"\\033[38;2;100;200;255m[DEBUG] Target's preferred tokens at each position:\\033[0m\")\n",
    "                for i in range(gamma):\n",
    "                    top_tok = torch.argmax(target_logits_batch[0, i, :]).item()\n",
    "                    top_bin = self._get_action_bin_from_target_token(top_tok)\n",
    "                    draft_tok = draft_token_ids_target[i]\n",
    "                    draft_bin = self._get_action_bin_from_target_token(draft_tok)\n",
    "                    p_top = target_probs_batch[0, i, top_tok].item()\n",
    "                    p_draft = target_probs_batch[0, i, draft_tok].item()\n",
    "                    print(f\"  pos[{i}]: target_wants={top_tok}(bin={top_bin}, p={p_top:.4f}) | draft_proposed={draft_tok}(bin={draft_bin}, p={p_draft:.4f})\")\n",
    "                \n",
    "                # Get actual target logit dimension from the output\n",
    "                actual_target_logit_dim = target_logits_batch.shape[-1]\n",
    "                \n",
    "                # Remap draft probs to target vocab space for comparison\n",
    "                # Use actual target logit dimension to ensure tensor size match\n",
    "                draft_probs_remapped = [self._remap_logits_draft_to_target(dp, actual_target_logit_dim) for dp in draft_probs_list]\n",
    "                draft_probs_remapped = [self._get_probs(dp) for dp in draft_probs_remapped]\n",
    "                \n",
    "                # Rejection sampling loop\n",
    "                n_accepted = 0\n",
    "                for i in range(gamma):\n",
    "                    draft_token_id_draft = draft_tokens[i].item()  # In draft vocab\n",
    "                    draft_token_id_target = draft_token_ids_target[i]  # Mapped to target vocab\n",
    "                    \n",
    "                    draft_prob_remapped = draft_probs_remapped[i]\n",
    "                    target_prob = target_probs_batch[:, i, :]\n",
    "                    \n",
    "                    # Get probability of the token under both models (in target vocab space)\n",
    "                    p_target = target_prob[0, draft_token_id_target].item()\n",
    "                    p_draft = draft_prob_remapped[0, draft_token_id_target].item()\n",
    "                    \n",
    "                    # Rejection sampling\n",
    "                    if p_draft > 0:\n",
    "                        acceptance_prob = min(1.0, p_target / p_draft)\n",
    "                    else:\n",
    "                        acceptance_prob = 1.0 if p_target > 0 else 0.0\n",
    "                    \n",
    "                    if torch.rand(1).item() < acceptance_prob:\n",
    "                        # Accept this token (store in target vocab space)\n",
    "                        accepted_bin = self._get_action_bin_from_target_token(draft_token_id_target)\n",
    "                        accepted_action = self._get_continuous_action_from_bin(accepted_bin)\n",
    "                        print(f\"\\033[38;2;0;255;0m[ACCEPT]\\033[0m token[{i}]: target_tok={draft_token_id_target}  bin={accepted_bin}  action={accepted_action:.4f} | p_target={p_target:.4f}, p_draft={p_draft:.4f}, accept_prob={acceptance_prob:.4f}\")\n",
    "                        generated_token_ids.append(draft_token_id_target)\n",
    "                        n_accepted += 1\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        call_stats.total_draft_tokens_accepted += 1\n",
    "                        \n",
    "                        if len(generated_token_ids) >= action_dim:\n",
    "                            print(f\"\\033[38;2;255;165;0m[SRP] -> \\033[0m Generated {len(generated_token_ids)}/{action_dim} tokens, done.\")\n",
    "                            break\n",
    "                    else:\n",
    "                        # Reject - sample from adjusted distribution\n",
    "                        adjusted_probs = max_fn(target_prob - draft_prob_remapped)\n",
    "                        if adjusted_probs.sum() > 0:\n",
    "                            corrected_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
    "                        else:\n",
    "                            corrected_token = self._sample_token(target_prob.unsqueeze(0))\n",
    "                        \n",
    "                        corrected_token_id = int(corrected_token.item())\n",
    "                        rejected_bin = self._get_action_bin_from_target_token(draft_token_id_target)\n",
    "                        corrected_bin = self._get_action_bin_from_target_token(corrected_token_id)\n",
    "                        rejected_action = self._get_continuous_action_from_bin(rejected_bin)\n",
    "                        corrected_action = self._get_continuous_action_from_bin(corrected_bin)\n",
    "                        bin_diff = abs(rejected_bin - corrected_bin) if rejected_bin >= 0 and corrected_bin >= 0 else -1\n",
    "                        \n",
    "                        print(f\"\\033[38;2;255;100;100m[REJECT]\\033[0m token[{i}]: p_target={p_target:.4f} < p_draft={p_draft:.4f}, accept_prob={acceptance_prob:.4f}\")\n",
    "                        print(f\"  Rejected:  target_tok={draft_token_id_target}  bin={rejected_bin}  action={rejected_action:.4f}\")\n",
    "                        print(f\"  Corrected: target_tok={corrected_token_id}  bin={corrected_bin}  action={corrected_action:.4f}\")\n",
    "                        print(f\"  Bin difference: {bin_diff} | Action difference: {abs(rejected_action - corrected_action):.4f}\")\n",
    "                        \n",
    "                        # Store corrected token (already in target vocab space)\n",
    "                        generated_token_ids.append(corrected_token_id)\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        n_accepted = i  # Number of accepted tokens (before this rejection)\n",
    "                        break\n",
    "                                    \n",
    "                # Update caches after acceptance/rejection\n",
    "                if n_accepted == gamma and len(generated_token_ids) < action_dim:\n",
    "                    # All accepted - need to sample one more from target\n",
    "                    print(f\"\\033[38;2;0;255;0m[ALL ACCEPTED]\\033[0m All {gamma} draft tokens accepted! Sampling bonus token from target...\")\n",
    "                    target_cache = target_cache_for_verify\n",
    "                    target_logits = last_target_logits  # Logits after feeding all gamma tokens\n",
    "                    \n",
    "                    # Sample additional token from target (in target vocab space)\n",
    "                    bonus_token_target = self._sample_token(target_logits)\n",
    "                    bonus_token_id_target = int(bonus_token_target.item())\n",
    "                    bonus_bin = self._get_action_bin_from_target_token(bonus_token_id_target)\n",
    "                    bonus_action = self._get_continuous_action_from_bin(bonus_bin)\n",
    "                    print(f\"  Bonus token: target_tok={bonus_token_id_target}  bin={bonus_bin}  action={bonus_action:.4f}\")\n",
    "                    generated_token_ids.append(bonus_token_id_target)\n",
    "                    call_stats.total_tokens_generated += 1\n",
    "                    \n",
    "                    # Update target cache\n",
    "                    target_step = self.target(\n",
    "                        input_ids=bonus_token_target,\n",
    "                        past_key_values=target_cache,\n",
    "                        use_cache=self.use_cache,\n",
    "                    )\n",
    "                    target_cache = target_step.past_key_values\n",
    "                    target_logits = target_step.logits[:, -1, :]\n",
    "                    call_stats.total_target_forward_passes += 1\n",
    "                    \n",
    "                    # Map bonus token to draft vocab and update draft cache\n",
    "                    bonus_token_id_draft = self._target_token_to_draft(bonus_token_id_target)\n",
    "                    bonus_draft_bin = self._get_action_bin_from_draft_token(bonus_token_id_draft)\n",
    "                    print(f\"  Mapped to draft: draft_tok={bonus_token_id_draft}  bin={bonus_draft_bin} {'' if bonus_bin == bonus_draft_bin else ' MISMATCH!'}\")\n",
    "                    bonus_token_draft = torch.tensor([[bonus_token_id_draft]], device=self.device)\n",
    "                    \n",
    "                    draft_cache = current_draft_cache\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=bonus_token_draft,\n",
    "                            past_key_values=draft_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                    draft_cache = draft_step.past_key_values\n",
    "                    draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                    \n",
    "                else:\n",
    "                    # Some tokens rejected - prune caches\n",
    "                    tokens_to_discard = gamma - n_accepted\n",
    "                    if tokens_to_discard > 0 and self.use_cache:\n",
    "                        # We need to prune and resync\n",
    "                        # Use the cache state after the accepted tokens\n",
    "                        target_cache = target_cache_for_verify\n",
    "                        if tokens_to_discard > 0:\n",
    "                            target_cache = prune_cache(target_cache, tokens_to_discard)\n",
    "                        \n",
    "                        # Rebuild draft cache\n",
    "                        draft_cache = prune_cache(current_draft_cache, gamma - n_accepted)\n",
    "                    \n",
    "                    # Get logits for next round\n",
    "                    if len(generated_token_ids) < action_dim:\n",
    "                        # Last token is in target vocab space\n",
    "                        last_token_id_target = generated_token_ids[-1]\n",
    "                        last_token_target = torch.tensor([[last_token_id_target]], device=self.device)\n",
    "                        \n",
    "                        target_step = self.target(\n",
    "                            input_ids=last_token_target,\n",
    "                            past_key_values=target_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                        target_cache = target_step.past_key_values\n",
    "                        target_logits = target_step.logits[:, -1, :]\n",
    "                        call_stats.total_target_forward_passes += 1\n",
    "                        \n",
    "                        # Map to draft vocab for draft model\n",
    "                        last_token_id_draft = self._target_token_to_draft(last_token_id_target)\n",
    "                        last_token_draft = torch.tensor([[last_token_id_draft]], device=self.device)\n",
    "                        \n",
    "                        with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                            draft_step = self.draft(\n",
    "                                input_ids=last_token_draft,\n",
    "                                past_key_values=draft_cache,\n",
    "                                use_cache=self.use_cache,\n",
    "                            )\n",
    "                        draft_cache = draft_step.past_key_values\n",
    "                        draft_logits = draft_step.logits[:, -1, :]\n",
    "                        call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Call stats: {call_stats}\")\n",
    "            \n",
    "        # Decode tokens to actions\n",
    "        predicted_action_token_ids = np.array(generated_token_ids[:action_dim], dtype=np.int64)\n",
    "        \n",
    "        # DEBUG: Show all generated tokens with their action values\n",
    "        print(f\"\\033[38;2;100;200;255m[DEBUG] Final generated tokens summary:\\033[0m\")\n",
    "        for dim_idx, tok_id in enumerate(predicted_action_token_ids):\n",
    "            bin_idx = self._get_action_bin_from_target_token(int(tok_id))\n",
    "            action_val = self._get_continuous_action_from_bin(bin_idx)\n",
    "            print(f\"  dim[{dim_idx}]: target_tok={tok_id}  bin={bin_idx}  normalized_action={action_val:.4f}\")\n",
    "        \n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Predicted action token ids: {predicted_action_token_ids}\")\n",
    "        \n",
    "        # Use target model's decoding (vocab_size - token_id approach)\n",
    "        vocab_size = self.target.vocab_size\n",
    "        discretized_actions = vocab_size - predicted_action_token_ids\n",
    "        discretized_actions = np.clip(discretized_actions - 1, a_min=0, a_max=self.target.bin_centers.shape[0] - 1)\n",
    "        normalized_actions = self.target.bin_centers[discretized_actions]\n",
    "        \n",
    "        # Un-normalize actions\n",
    "        action_norm_stats = self.target.get_action_stats(unnorm_key_target)\n",
    "        mask = action_norm_stats.get(\"mask\", np.ones_like(action_norm_stats[\"q01\"], dtype=bool))\n",
    "        action_high, action_low = np.array(action_norm_stats[\"q99\"]), np.array(action_norm_stats[\"q01\"])\n",
    "        actions = np.where(\n",
    "            mask,\n",
    "            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,\n",
    "            normalized_actions,\n",
    "        )\n",
    "        \n",
    "        # Update global stats\n",
    "        self.stats.total_tokens_generated += call_stats.total_tokens_generated\n",
    "        self.stats.total_draft_tokens_proposed += call_stats.total_draft_tokens_proposed\n",
    "        self.stats.total_draft_tokens_accepted += call_stats.total_draft_tokens_accepted\n",
    "        self.stats.total_target_forward_passes += call_stats.total_target_forward_passes\n",
    "        self.stats.total_draft_forward_passes += call_stats.total_draft_forward_passes\n",
    "        \n",
    "        return actions, call_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLASpeculativeDecoderDDDR:\n",
    "    \"\"\"\n",
    "    Speculative decoding for VLA models.\n",
    "    \n",
    "    Uses a draft model (MiniVLA) to propose action tokens and a target model \n",
    "    (OpenVLA) to verify them.\n",
    "    \n",
    "    IMPORTANT: For speculative decoding to work correctly, both models should\n",
    "    share the same tokenizer/vocabulary. If they don't, action token remapping\n",
    "    is attempted but this only works for action tokens (last 256 tokens of vocab).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model,\n",
    "        draft_model,\n",
    "        target_processor=None,\n",
    "        gamma: int = 4,  # Number of draft tokens to propose at once\n",
    "        use_cache: bool = True,\n",
    "        temperature: float = 0.0,  # 0 = greedy/argmax\n",
    "        n_action_bins: int = 256,  # Number of action bins (tokens)\n",
    "        relaxed_acceptance_r: int = 0,  # Relaxed acceptance radius (0 = standard spec dec)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the speculative decoder.\n",
    "        \n",
    "        Args:\n",
    "            target_model: OpenVLA model (larger, slower, more accurate)\n",
    "            draft_model: MiniVLA model (smaller, faster)\n",
    "            target_processor: HuggingFace processor for target model\n",
    "            gamma: Number of tokens to speculate at each step\n",
    "            use_cache: Whether to use KV caching\n",
    "            temperature: Sampling temperature (0 = greedy/argmax)\n",
    "            n_action_bins: Number of action token bins (typically 256)\n",
    "            relaxed_acceptance_r: Relaxed acceptance radius. If the draft token's bin\n",
    "                is within r bins of the target's preferred bin, accept it.\n",
    "                Set to 0 for standard speculative decoding behavior.\n",
    "        \"\"\"\n",
    "        self.target = target_model\n",
    "        self.draft = draft_model\n",
    "        self.target_processor = target_processor\n",
    "        self.gamma = gamma\n",
    "        self.use_cache = use_cache\n",
    "        self.temperature = temperature\n",
    "        self.n_action_bins = n_action_bins\n",
    "        self.relaxed_acceptance_r = relaxed_acceptance_r\n",
    "        \n",
    "        # Get device\n",
    "        self.device = next(target_model.parameters()).device\n",
    "        \n",
    "        # Stats tracking\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "        \n",
    "        # Check vocabulary compatibility and setup token mapping\n",
    "        self._setup_token_mapping()\n",
    "    \n",
    "    def _setup_token_mapping(self):\n",
    "        \"\"\"Setup token mapping between draft and target vocabularies.\"\"\"\n",
    "        # Get vocabulary sizes\n",
    "        # Target model (OpenVLA/HF style) - use ACTUAL embedding dimension, not vocab_size attribute\n",
    "        # The embedding may be padded to \"multiple of\" for efficiency\n",
    "        if hasattr(self.target, 'language_model') and hasattr(self.target.language_model, 'model'):\n",
    "            # Actual embedding dimension (includes padding)\n",
    "            self.target_logit_dim = self.target.language_model.model.embed_tokens.weight.shape[0]\n",
    "        elif hasattr(self.target, 'get_output_embeddings'):\n",
    "            self.target_logit_dim = self.target.get_output_embeddings().weight.shape[0]\n",
    "        else:\n",
    "            self.target_logit_dim = self.target.config.vocab_size\n",
    "        \n",
    "        # Also get the \"logical\" vocab size (without padding) for action token calculation\n",
    "        if hasattr(self.target, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.vocab_size\n",
    "        elif hasattr(self.target, 'config') and hasattr(self.target.config, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.config.vocab_size\n",
    "        else:\n",
    "            self.target_vocab_size = self.target_logit_dim\n",
    "        \n",
    "        # Draft model (Prismatic style)\n",
    "        if hasattr(self.draft, 'llm_backbone'):\n",
    "            draft_tokenizer = self.draft.llm_backbone.tokenizer\n",
    "            # Qwen2 uses len(tokenizer) for full vocab including added tokens\n",
    "            self.draft_vocab_size = len(draft_tokenizer) if hasattr(draft_tokenizer, '__len__') else draft_tokenizer.vocab_size\n",
    "            # Get actual logit dimension from draft model\n",
    "            if hasattr(self.draft.llm_backbone, 'llm') and hasattr(self.draft.llm_backbone.llm, 'lm_head'):\n",
    "                self.draft_logit_dim = self.draft.llm_backbone.llm.lm_head.weight.shape[0]\n",
    "            else:\n",
    "                self.draft_logit_dim = self.draft_vocab_size\n",
    "        else:\n",
    "            self.draft_vocab_size = self.draft.config.vocab_size\n",
    "            self.draft_logit_dim = self.draft_vocab_size\n",
    "        \n",
    "        # Check if vocabularies match\n",
    "        self.vocab_compatible = (self.target_logit_dim == self.draft_logit_dim)\n",
    "        \n",
    "        # Compute action token ranges\n",
    "        # Action tokens are the LAST n_action_bins tokens before padding\n",
    "        # For target: use vocab_size (not padded logit_dim)\n",
    "        self.target_action_start = self.target_vocab_size - self.n_action_bins\n",
    "        self.draft_action_start = self.draft_vocab_size - self.n_action_bins\n",
    "        \n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Target vocab_size: {self.target_vocab_size}, logit_dim: {self.target_logit_dim}, action tokens: [{self.target_action_start}, {self.target_vocab_size})\")\n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Draft vocab_size: {self.draft_vocab_size}, logit_dim: {self.draft_logit_dim}, action tokens: [{self.draft_action_start}, {self.draft_vocab_size})\")\n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Vocabularies compatible: {self.vocab_compatible}\")\n",
    "        \n",
    "        if not self.vocab_compatible:\n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"WARNING: Vocabulary mismatch! Token remapping will be used for action tokens only.\")\n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"This may affect acceptance rates. Consider using models with matching tokenizers.\")\n",
    "        \n",
    "        # DEBUG: Verify mapping with example tokens\n",
    "        print(\"\\033[38;2;100;200;255m[DEBUG] Token mapping verification examples:\\033[0m\")\n",
    "        for bin_idx in [0, 127, 255]:  # First, middle, last action bins\n",
    "            draft_tok = self.draft_action_start + bin_idx\n",
    "            target_tok = self._draft_token_to_target(draft_tok)\n",
    "            draft_bin = self._get_action_bin_from_draft_token(draft_tok)\n",
    "            target_bin = self._get_action_bin_from_target_token(target_tok)\n",
    "            print(f\"  Bin {bin_idx}: draft_token={draft_tok}  target_token={target_tok} | draft_bin={draft_bin}, target_bin={target_bin} | {'' if draft_bin == target_bin else ''}\")\n",
    "    \n",
    "    def _draft_token_to_target(self, draft_token_id: int) -> int:\n",
    "        \"\"\"Map a draft token ID to target vocabulary.\"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return draft_token_id\n",
    "        \n",
    "        # Check if this is an action token (from end of draft vocabulary)\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            # Map to corresponding action token in target vocabulary\n",
    "            action_bin = draft_token_id - self.draft_action_start\n",
    "            target_token = self.target_action_start + action_bin\n",
    "            return target_token\n",
    "        else:\n",
    "            # Non-action token - this shouldn't happen during action generation\n",
    "            # Return as-is but clamp to valid range\n",
    "            return min(draft_token_id, self.target_vocab_size - 1)\n",
    "    \n",
    "    def _target_token_to_draft(self, target_token_id: int) -> int:\n",
    "        \"\"\"Map a target token ID to draft vocabulary.\"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return target_token_id\n",
    "        \n",
    "        # Check if this is an action token\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            action_bin = target_token_id - self.target_action_start\n",
    "            draft_token = self.draft_action_start + action_bin\n",
    "            return draft_token\n",
    "        else:\n",
    "            return min(target_token_id, self.draft_vocab_size - 1)\n",
    "    \n",
    "    def _remap_logits_draft_to_target(self, draft_logits: torch.Tensor, target_logit_dim: int = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Remap draft logits to target vocabulary space.\n",
    "        Only remaps action tokens; non-action tokens get -inf.\n",
    "        \n",
    "        Args:\n",
    "            draft_logits: Logits from draft model\n",
    "            target_logit_dim: Actual dimension of target logits (may differ from vocab_size due to padding)\n",
    "        \"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return draft_logits\n",
    "        \n",
    "        # Use provided dimension or fall back to stored logit_dim\n",
    "        if target_logit_dim is None:\n",
    "            target_logit_dim = self.target_logit_dim\n",
    "        \n",
    "        # Create target-sized logits filled with -inf (use actual logit dimension, not vocab_size)\n",
    "        target_logits = torch.full(\n",
    "            (draft_logits.shape[0], target_logit_dim),\n",
    "            float('-inf'),\n",
    "            device=draft_logits.device,\n",
    "            dtype=draft_logits.dtype\n",
    "        )\n",
    "        \n",
    "        # Copy action token logits from draft to corresponding target positions\n",
    "        # Draft action tokens: [draft_action_start, draft_vocab_size)\n",
    "        # Target action tokens: [target_action_start, target_vocab_size)\n",
    "        draft_action_logits = draft_logits[:, self.draft_action_start:self.draft_vocab_size]\n",
    "        target_logits[:, self.target_action_start:self.target_vocab_size] = draft_action_logits\n",
    "        \n",
    "        return target_logits\n",
    "    \n",
    "    # =========================================================================\n",
    "    # DEBUG: Token mapping verification methods\n",
    "    # =========================================================================\n",
    "    \n",
    "    def _get_action_bin_from_draft_token(self, draft_token_id: int) -> int:\n",
    "        \"\"\"Get action bin index from draft token ID.\"\"\"\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            return draft_token_id - self.draft_action_start\n",
    "        return -1  # Not an action token\n",
    "    \n",
    "    def _get_action_bin_from_target_token(self, target_token_id: int) -> int:\n",
    "        \"\"\"Get action bin index from target token ID.\"\"\"\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            return target_token_id - self.target_action_start\n",
    "        return -1  # Not an action token\n",
    "    \n",
    "    def _get_continuous_action_from_bin(self, bin_idx: int) -> float:\n",
    "        \"\"\"Convert action bin index to continuous action value using target's bin centers.\"\"\"\n",
    "        if 0 <= bin_idx < len(self.target.bin_centers):\n",
    "            return self.target.bin_centers[bin_idx]\n",
    "        return float('nan')\n",
    "    \n",
    "    def _debug_token_mapping(self, draft_token_id: int, target_token_id: int, prefix: str = \"\"):\n",
    "        \"\"\"Debug print showing token mapping verification.\"\"\"\n",
    "        draft_bin = self._get_action_bin_from_draft_token(draft_token_id)\n",
    "        target_bin = self._get_action_bin_from_target_token(target_token_id)\n",
    "        \n",
    "        draft_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "        target_action = self._get_continuous_action_from_bin(target_bin)\n",
    "        \n",
    "        match_status = \" MATCH\" if draft_bin == target_bin else \" MISMATCH\"\n",
    "        \n",
    "        print(f\"\\033[38;2;100;200;255m[DEBUG TOKEN MAP] {prefix}\\033[0m\")\n",
    "        print(f\"  Draft token:  {draft_token_id}  bin {draft_bin}  action {draft_action:.4f}\")\n",
    "        print(f\"  Target token: {target_token_id}  bin {target_bin}  action {target_action:.4f}\")\n",
    "        print(f\"  Bins match: {match_status}\")\n",
    "        \n",
    "        return draft_bin == target_bin\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        \"\"\"Reset statistics counters.\"\"\"\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "    \n",
    "    def _sample_token(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Sample a token from logits.\"\"\"\n",
    "        if self.temperature <= 0:\n",
    "            return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            probs = F.softmax(logits / self.temperature, dim=-1)\n",
    "            return torch.multinomial(probs.squeeze(0), num_samples=1).unsqueeze(0)\n",
    "    \n",
    "    def _get_probs(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert logits to probabilities.\"\"\"\n",
    "        if self.temperature <= 0:\n",
    "            # For greedy, use a very low temperature to approximate argmax\n",
    "            return F.softmax(logits / 0.01, dim=-1)\n",
    "        return F.softmax(logits / self.temperature, dim=-1)\n",
    "    \n",
    "    def _prepare_target_inputs(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Prepare inputs for the target (OpenVLA) model.\"\"\"\n",
    "        prompt = f\"In: What action should the robot take to {instruction.lower()}?\\nOut:\"\n",
    "        inputs = self.target_processor(prompt, image).to(self.device, dtype=torch.bfloat16)\n",
    "        \n",
    "        # IMPORTANT: Add the special empty token (29871) to input_ids if not present\n",
    "        # This is what OpenVLA's predict_action does - the action tokens come AFTER this token\n",
    "        if not torch.all(inputs[\"input_ids\"][:, -1] == 29871):\n",
    "            inputs[\"input_ids\"] = torch.cat(\n",
    "                (inputs[\"input_ids\"], torch.tensor([[29871]], device=self.device)),\n",
    "                dim=1\n",
    "            )\n",
    "            # Also extend attention mask\n",
    "            if \"attention_mask\" in inputs:\n",
    "                inputs[\"attention_mask\"] = torch.cat(\n",
    "                    (inputs[\"attention_mask\"], torch.ones((1, 1), device=self.device, dtype=inputs[\"attention_mask\"].dtype)),\n",
    "                    dim=1\n",
    "                )\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def _prepare_draft_inputs(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Prepare inputs for the draft (MiniVLA) model.\"\"\"\n",
    "        # Get prompt using draft model's prompt builder\n",
    "        prompt_builder = self.draft.get_prompt_builder()\n",
    "        prompt_builder.add_turn(role=\"human\", message=f\"What action should the robot take to {instruction.lower()}?\")\n",
    "        prompt_text = prompt_builder.get_prompt()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenizer = self.draft.llm_backbone.tokenizer\n",
    "        input_ids = tokenizer(prompt_text, truncation=True, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        \n",
    "        # Handle special token for Llama tokenizer\n",
    "        from transformers import LlamaTokenizerFast\n",
    "        if isinstance(tokenizer, LlamaTokenizerFast):\n",
    "            if not torch.all(input_ids[:, -1] == 29871):\n",
    "                input_ids = torch.cat(\n",
    "                    (input_ids, torch.tensor([[29871]], device=self.device)),\n",
    "                    dim=1\n",
    "                )\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Process image\n",
    "        image_transform = self.draft.vision_backbone.get_image_transform()\n",
    "        pixel_values = image_transform(image)\n",
    "        if isinstance(pixel_values, torch.Tensor):\n",
    "            pixel_values = pixel_values[None, ...].to(self.device)\n",
    "        elif isinstance(pixel_values, dict):\n",
    "            pixel_values = {k: v[None, ...].to(self.device) for k, v in pixel_values.items()}\n",
    "        \n",
    "        return input_ids, attention_mask, pixel_values\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict_action_speculative(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "        unnorm_key_target: str,\n",
    "    ) -> Tuple[np.ndarray, SpeculativeDecodingStats]:\n",
    "        \"\"\"\n",
    "        Generate action using speculative decoding.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image observation\n",
    "            instruction: Task instruction string\n",
    "            unnorm_key_target: Key for action un-normalization statistics of the target model\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (unnormalized action array, decoding statistics)\n",
    "        \"\"\"\n",
    "        # Reset per-call stats\n",
    "        call_stats = SpeculativeDecodingStats()\n",
    "        \n",
    "        action_dim = self.target.get_action_dim(unnorm_key_target)\n",
    "        \n",
    "        \n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Target vocab size: {self.target.vocab_size}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Target embedding size: {self.target.language_model.model.embed_tokens.weight.shape[0]}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Draft tokenizer vocab size: {self.draft.llm_backbone.tokenizer.vocab_size}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Action dim: {action_dim}\")\n",
    "        \n",
    "        # Prepare inputs for both models\n",
    "        target_inputs = self._prepare_target_inputs(image, instruction)\n",
    "        draft_input_ids, draft_attention_mask, draft_pixel_values = self._prepare_draft_inputs(image, instruction)\n",
    "        \n",
    "        # Cast draft inputs to appropriate dtype\n",
    "        autocast_dtype = self.draft.llm_backbone.half_precision_dtype\n",
    "        \n",
    "        # Initialize caches\n",
    "        target_cache = None\n",
    "        draft_cache = None\n",
    "        \n",
    "        generated_token_ids = []\n",
    "        \n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16, enabled=True):\n",
    "            # === Initial forward pass to get KV cache and initial logits ===\n",
    "            # NOTE: After adding token 29871 to input, the model should output action tokens directly\n",
    "            \n",
    "            # Target model initial forward (prefill)\n",
    "            target_out = self.target(\n",
    "                **target_inputs,\n",
    "                past_key_values=None,\n",
    "                use_cache=self.use_cache,\n",
    "            )\n",
    "            target_cache = target_out.past_key_values\n",
    "            target_logits = target_out.logits[:, -1, :]\n",
    "            call_stats.total_target_forward_passes += 1\n",
    "            \n",
    "            # DEBUG: Check what the target model wants to output first\n",
    "            top_target_token = torch.argmax(target_logits, dim=-1).item()\n",
    "            top_target_bin = self._get_action_bin_from_target_token(top_target_token)\n",
    "            print(f\"\\033[38;2;100;200;255m[DEBUG] After prefill, target top token:\\033[0m\")\n",
    "            print(f\"  top_token={top_target_token}  bin={top_target_bin}  is_action_token={top_target_bin >= 0}\")\n",
    "            \n",
    "            # Draft model initial forward (prefill)\n",
    "            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                draft_out = self.draft(\n",
    "                    input_ids=draft_input_ids,\n",
    "                    attention_mask=draft_attention_mask,\n",
    "                    pixel_values=draft_pixel_values,\n",
    "                    past_key_values=None,\n",
    "                    use_cache=self.use_cache,\n",
    "                )\n",
    "            draft_cache = draft_out.past_key_values\n",
    "            draft_logits = draft_out.logits[:, -1, :]\n",
    "            call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            # DEBUG: Check what the draft model wants to output first\n",
    "            top_draft_token = torch.argmax(draft_logits, dim=-1).item()\n",
    "            top_draft_bin = self._get_action_bin_from_draft_token(top_draft_token)\n",
    "            print(f\"\\033[38;2;100;200;255m[DEBUG] After prefill, draft top token:\\033[0m\")\n",
    "            print(f\"  top_token={top_draft_token}  bin={top_draft_bin}  is_action_token={top_draft_bin >= 0}\")\n",
    "            \n",
    "            # === Main speculative decoding loop ===\n",
    "            # We start directly with drafting - no need to sample a first token separately\n",
    "            while len(generated_token_ids) < action_dim:\n",
    "                # Determine how many tokens to speculate\n",
    "                gamma = min(self.gamma, action_dim - len(generated_token_ids))\n",
    "                \n",
    "                # Generate gamma draft tokens\n",
    "                draft_tokens = []\n",
    "                draft_probs_list = []\n",
    "                \n",
    "                current_draft_cache = draft_cache\n",
    "                current_draft_logits = draft_logits\n",
    "                \n",
    "                for _ in range(gamma):\n",
    "                    draft_probs = self._get_probs(current_draft_logits)\n",
    "                    draft_token = self._sample_token(current_draft_logits)\n",
    "                    \n",
    "                    draft_tokens.append(draft_token)\n",
    "                    draft_probs_list.append(draft_probs)\n",
    "                    \n",
    "                    # Advance draft model\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=draft_token.to(self.device),\n",
    "                            past_key_values=current_draft_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                    current_draft_cache = draft_step.past_key_values\n",
    "                    current_draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                \n",
    "                call_stats.total_draft_tokens_proposed += gamma\n",
    "                \n",
    "                # Map draft tokens to target vocab space for verification\n",
    "                draft_token_ids_target = []\n",
    "                print(f\"\\033[38;2;100;200;255m[DEBUG] Mapping {gamma} draft tokens to target vocab:\\033[0m\")\n",
    "                for idx, dt in enumerate(draft_tokens):\n",
    "                    draft_id = dt.item()\n",
    "                    target_id = self._draft_token_to_target(draft_id)\n",
    "                    draft_token_ids_target.append(target_id)\n",
    "                    \n",
    "                    # Verify mapping preserves action bin\n",
    "                    draft_bin = self._get_action_bin_from_draft_token(draft_id)\n",
    "                    target_bin = self._get_action_bin_from_target_token(target_id)\n",
    "                    draft_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "                    target_action = self._get_continuous_action_from_bin(target_bin)\n",
    "                    match = \"\" if draft_bin == target_bin else \" MISMATCH!\"\n",
    "                    print(f\"  [{idx}] draft_tok={draft_id}  target_tok={target_id} | bin: {draft_bin}{target_bin} | action: {draft_action:.4f}{target_action:.4f} {match}\")\n",
    "                \n",
    "                # Verify with target model\n",
    "                # IMPORTANT: We need to collect logits BEFORE feeding each token\n",
    "                # - target_logits (from prefill or last step) is used to evaluate draft_token[0]\n",
    "                # - logits after feeding token[0] is used to evaluate draft_token[1]\n",
    "                # etc.\n",
    "                \n",
    "                target_cache_for_verify = target_cache\n",
    "                # Start with current target_logits for evaluating first draft token\n",
    "                target_logits_for_verification = [target_logits.unsqueeze(1)]  # [1, 1, vocab]\n",
    "                last_target_logits = None  # Will hold logits after feeding last token\n",
    "                \n",
    "                for i in range(gamma):\n",
    "                    target_token_input = torch.tensor([[draft_token_ids_target[i]]], device=self.device)\n",
    "                    target_step = self.target(\n",
    "                        input_ids=target_token_input,\n",
    "                        past_key_values=target_cache_for_verify,\n",
    "                        use_cache=self.use_cache,\n",
    "                    )\n",
    "                    target_cache_for_verify = target_step.past_key_values\n",
    "                    last_target_logits = target_step.logits[:, -1, :]  # Always save last logits\n",
    "                    # Store logits for evaluating the NEXT token (if there is one)\n",
    "                    if i < gamma - 1:\n",
    "                        target_logits_for_verification.append(target_step.logits[:, -1:, :])\n",
    "                \n",
    "                call_stats.total_target_forward_passes += gamma\n",
    "                \n",
    "                # Stack target logits - now target_logits_for_verification[i] evaluates draft_token[i]\n",
    "                target_logits_batch = torch.cat(target_logits_for_verification, dim=1)  # [1, gamma, actual_vocab_dim]\n",
    "                target_probs_batch = self._get_probs(target_logits_batch)\n",
    "                \n",
    "                # DEBUG: Show which token the target actually wants at each position\n",
    "                print(f\"\\033[38;2;100;200;255m[DEBUG] Target's preferred tokens at each position:\\033[0m\")\n",
    "                for i in range(gamma):\n",
    "                    top_tok = torch.argmax(target_logits_batch[0, i, :]).item()\n",
    "                    top_bin = self._get_action_bin_from_target_token(top_tok)\n",
    "                    draft_tok = draft_token_ids_target[i]\n",
    "                    draft_bin = self._get_action_bin_from_target_token(draft_tok)\n",
    "                    p_top = target_probs_batch[0, i, top_tok].item()\n",
    "                    p_draft = target_probs_batch[0, i, draft_tok].item()\n",
    "                    print(f\"  pos[{i}]: target_wants={top_tok}(bin={top_bin}, p={p_top:.4f}) | draft_proposed={draft_tok}(bin={draft_bin}, p={p_draft:.4f})\")\n",
    "                \n",
    "                # Get actual target logit dimension from the output\n",
    "                actual_target_logit_dim = target_logits_batch.shape[-1]\n",
    "                \n",
    "                # Remap draft probs to target vocab space for comparison\n",
    "                # Use actual target logit dimension to ensure tensor size match\n",
    "                draft_probs_remapped = [self._remap_logits_draft_to_target(dp, actual_target_logit_dim) for dp in draft_probs_list]\n",
    "                draft_probs_remapped = [self._get_probs(dp) for dp in draft_probs_remapped]\n",
    "                \n",
    "                # Rejection sampling loop\n",
    "                n_accepted = 0\n",
    "                for i in range(gamma):\n",
    "                    draft_token_id_draft = draft_tokens[i].item()  # In draft vocab\n",
    "                    draft_token_id_target = draft_token_ids_target[i]  # Mapped to target vocab\n",
    "                    \n",
    "                    draft_prob_remapped = draft_probs_remapped[i]\n",
    "                    target_prob = target_probs_batch[:, i, :]\n",
    "                    \n",
    "                    # Get probability of the token under both models (in target vocab space)\n",
    "                    p_target = target_prob[0, draft_token_id_target].item()\n",
    "                    p_draft = draft_prob_remapped[0, draft_token_id_target].item()\n",
    "                    \n",
    "                    # Get the target's preferred token and compute bin distances\n",
    "                    target_preferred_token = torch.argmax(target_prob, dim=-1).item()\n",
    "                    draft_bin = self._get_action_bin_from_target_token(draft_token_id_target)\n",
    "                    target_bin = self._get_action_bin_from_target_token(target_preferred_token)\n",
    "                    \n",
    "                    # Compute bin distance (for relaxed acceptance)\n",
    "                    if draft_bin >= 0 and target_bin >= 0:\n",
    "                        bin_distance = abs(draft_bin - target_bin)\n",
    "                    else:\n",
    "                        bin_distance = float('inf')  # Non-action tokens don't benefit from relaxed acceptance\n",
    "                    \n",
    "                    # Relaxed acceptance: accept if within r bins of target's preference\n",
    "                    relaxed_accept = (self.relaxed_acceptance_r > 0 and bin_distance <= self.relaxed_acceptance_r)\n",
    "                    \n",
    "                    # Standard rejection sampling\n",
    "                    if p_draft > 0:\n",
    "                        acceptance_prob = min(1.0, p_target / p_draft)\n",
    "                    else:\n",
    "                        acceptance_prob = 1.0 if p_target > 0 else 0.0\n",
    "                    \n",
    "                    standard_accept = (torch.rand(1).item() < acceptance_prob)\n",
    "                    \n",
    "                    # Accept if either relaxed acceptance or standard acceptance passes\n",
    "                    if relaxed_accept or standard_accept:\n",
    "                        # Accept this token (store in target vocab space)\n",
    "                        accepted_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "                        accept_reason = \"RELAXED\" if relaxed_accept and not standard_accept else \"STANDARD\"\n",
    "                        print(f\"\\033[38;2;0;255;0m[ACCEPT-{accept_reason}]\\033[0m token[{i}]: target_tok={draft_token_id_target}  bin={draft_bin}  action={accepted_action:.4f}\")\n",
    "                        print(f\"  target_preferred: bin={target_bin} | bin_distance={bin_distance} | r={self.relaxed_acceptance_r}\")\n",
    "                        print(f\"  p_target={p_target:.4f}, p_draft={p_draft:.4f}, accept_prob={acceptance_prob:.4f}\")\n",
    "                        generated_token_ids.append(draft_token_id_target)\n",
    "                        n_accepted += 1\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        call_stats.total_draft_tokens_accepted += 1\n",
    "                        \n",
    "                        if len(generated_token_ids) >= action_dim:\n",
    "                            print(f\"\\033[38;2;255;165;0m[SRP] -> \\033[0m Generated {len(generated_token_ids)}/{action_dim} tokens, done.\")\n",
    "                            break\n",
    "                    else:\n",
    "                        # Reject - sample from adjusted distribution\n",
    "                        adjusted_probs = max_fn(target_prob - draft_prob_remapped)\n",
    "                        if adjusted_probs.sum() > 0:\n",
    "                            corrected_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
    "                        else:\n",
    "                            corrected_token = self._sample_token(target_prob.unsqueeze(0))\n",
    "                        \n",
    "                        corrected_token_id = int(corrected_token.item())\n",
    "                        corrected_bin = self._get_action_bin_from_target_token(corrected_token_id)\n",
    "                        rejected_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "                        corrected_action = self._get_continuous_action_from_bin(corrected_bin)\n",
    "                        bin_diff = abs(draft_bin - corrected_bin) if draft_bin >= 0 and corrected_bin >= 0 else -1\n",
    "                        \n",
    "                        print(f\"\\033[38;2;255;100;100m[REJECT]\\033[0m token[{i}]: bin_distance={bin_distance} > r={self.relaxed_acceptance_r}\")\n",
    "                        print(f\"  Draft proposed: target_tok={draft_token_id_target}  bin={draft_bin}  action={rejected_action:.4f}\")\n",
    "                        print(f\"  Target prefers: target_tok={target_preferred_token}  bin={target_bin}\")\n",
    "                        print(f\"  Corrected to:   target_tok={corrected_token_id}  bin={corrected_bin}  action={corrected_action:.4f}\")\n",
    "                        print(f\"  p_target={p_target:.4f}, p_draft={p_draft:.4f}, accept_prob={acceptance_prob:.4f}\")\n",
    "                        \n",
    "                        # Store corrected token (already in target vocab space)\n",
    "                        generated_token_ids.append(corrected_token_id)\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        n_accepted = i  # Number of accepted tokens (before this rejection)\n",
    "                        break\n",
    "                                    \n",
    "                # Update caches after acceptance/rejection\n",
    "                if n_accepted == gamma and len(generated_token_ids) < action_dim:\n",
    "                    # All accepted - need to sample one more from target\n",
    "                    print(f\"\\033[38;2;0;255;0m[ALL ACCEPTED]\\033[0m All {gamma} draft tokens accepted! Sampling bonus token from target...\")\n",
    "                    target_cache = target_cache_for_verify\n",
    "                    target_logits = last_target_logits  # Logits after feeding all gamma tokens\n",
    "                    \n",
    "                    # Sample additional token from target (in target vocab space)\n",
    "                    bonus_token_target = self._sample_token(target_logits)\n",
    "                    bonus_token_id_target = int(bonus_token_target.item())\n",
    "                    bonus_bin = self._get_action_bin_from_target_token(bonus_token_id_target)\n",
    "                    bonus_action = self._get_continuous_action_from_bin(bonus_bin)\n",
    "                    print(f\"  Bonus token: target_tok={bonus_token_id_target}  bin={bonus_bin}  action={bonus_action:.4f}\")\n",
    "                    generated_token_ids.append(bonus_token_id_target)\n",
    "                    call_stats.total_tokens_generated += 1\n",
    "                    \n",
    "                    # Update target cache\n",
    "                    target_step = self.target(\n",
    "                        input_ids=bonus_token_target,\n",
    "                        past_key_values=target_cache,\n",
    "                        use_cache=self.use_cache,\n",
    "                    )\n",
    "                    target_cache = target_step.past_key_values\n",
    "                    target_logits = target_step.logits[:, -1, :]\n",
    "                    call_stats.total_target_forward_passes += 1\n",
    "                    \n",
    "                    # Map bonus token to draft vocab and update draft cache\n",
    "                    bonus_token_id_draft = self._target_token_to_draft(bonus_token_id_target)\n",
    "                    bonus_draft_bin = self._get_action_bin_from_draft_token(bonus_token_id_draft)\n",
    "                    print(f\"  Mapped to draft: draft_tok={bonus_token_id_draft}  bin={bonus_draft_bin} {'' if bonus_bin == bonus_draft_bin else ' MISMATCH!'}\")\n",
    "                    bonus_token_draft = torch.tensor([[bonus_token_id_draft]], device=self.device)\n",
    "                    \n",
    "                    draft_cache = current_draft_cache\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=bonus_token_draft,\n",
    "                            past_key_values=draft_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                    draft_cache = draft_step.past_key_values\n",
    "                    draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                    \n",
    "                else:\n",
    "                    # Some tokens rejected - prune caches\n",
    "                    tokens_to_discard = gamma - n_accepted\n",
    "                    if tokens_to_discard > 0 and self.use_cache:\n",
    "                        # We need to prune and resync\n",
    "                        # Use the cache state after the accepted tokens\n",
    "                        target_cache = target_cache_for_verify\n",
    "                        if tokens_to_discard > 0:\n",
    "                            target_cache = prune_cache(target_cache, tokens_to_discard)\n",
    "                        \n",
    "                        # Rebuild draft cache\n",
    "                        draft_cache = prune_cache(current_draft_cache, gamma - n_accepted)\n",
    "                    \n",
    "                    # Get logits for next round\n",
    "                    if len(generated_token_ids) < action_dim:\n",
    "                        # Last token is in target vocab space\n",
    "                        last_token_id_target = generated_token_ids[-1]\n",
    "                        last_token_target = torch.tensor([[last_token_id_target]], device=self.device)\n",
    "                        \n",
    "                        target_step = self.target(\n",
    "                            input_ids=last_token_target,\n",
    "                            past_key_values=target_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                        target_cache = target_step.past_key_values\n",
    "                        target_logits = target_step.logits[:, -1, :]\n",
    "                        call_stats.total_target_forward_passes += 1\n",
    "                        \n",
    "                        # Map to draft vocab for draft model\n",
    "                        last_token_id_draft = self._target_token_to_draft(last_token_id_target)\n",
    "                        last_token_draft = torch.tensor([[last_token_id_draft]], device=self.device)\n",
    "                        \n",
    "                        with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                            draft_step = self.draft(\n",
    "                                input_ids=last_token_draft,\n",
    "                                past_key_values=draft_cache,\n",
    "                                use_cache=self.use_cache,\n",
    "                            )\n",
    "                        draft_cache = draft_step.past_key_values\n",
    "                        draft_logits = draft_step.logits[:, -1, :]\n",
    "                        call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Call stats: {call_stats}\")\n",
    "            \n",
    "        # Decode tokens to actions\n",
    "        predicted_action_token_ids = np.array(generated_token_ids[:action_dim], dtype=np.int64)\n",
    "        \n",
    "        # DEBUG: Show all generated tokens with their action values\n",
    "        print(f\"\\033[38;2;100;200;255m[DEBUG] Final generated tokens summary:\\033[0m\")\n",
    "        for dim_idx, tok_id in enumerate(predicted_action_token_ids):\n",
    "            bin_idx = self._get_action_bin_from_target_token(int(tok_id))\n",
    "            action_val = self._get_continuous_action_from_bin(bin_idx)\n",
    "            print(f\"  dim[{dim_idx}]: target_tok={tok_id}  bin={bin_idx}  normalized_action={action_val:.4f}\")\n",
    "        \n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Predicted action token ids: {predicted_action_token_ids}\")\n",
    "        \n",
    "        # Use target model's decoding (vocab_size - token_id approach)\n",
    "        vocab_size = self.target.vocab_size\n",
    "        discretized_actions = vocab_size - predicted_action_token_ids\n",
    "        discretized_actions = np.clip(discretized_actions - 1, a_min=0, a_max=self.target.bin_centers.shape[0] - 1)\n",
    "        normalized_actions = self.target.bin_centers[discretized_actions]\n",
    "        \n",
    "        # Un-normalize actions\n",
    "        action_norm_stats = self.target.get_action_stats(unnorm_key_target)\n",
    "        mask = action_norm_stats.get(\"mask\", np.ones_like(action_norm_stats[\"q01\"], dtype=bool))\n",
    "        action_high, action_low = np.array(action_norm_stats[\"q99\"]), np.array(action_norm_stats[\"q01\"])\n",
    "        actions = np.where(\n",
    "            mask,\n",
    "            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,\n",
    "            normalized_actions,\n",
    "        )\n",
    "        \n",
    "        # Update global stats\n",
    "        self.stats.total_tokens_generated += call_stats.total_tokens_generated\n",
    "        self.stats.total_draft_tokens_proposed += call_stats.total_draft_tokens_proposed\n",
    "        self.stats.total_draft_tokens_accepted += call_stats.total_draft_tokens_accepted\n",
    "        self.stats.total_target_forward_passes += call_stats.total_target_forward_passes\n",
    "        self.stats.total_draft_forward_passes += call_stats.total_draft_forward_passes\n",
    "        \n",
    "        return actions, call_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLASpeculativeDecoderDDDRKVB:\n",
    "    \"\"\"\n",
    "    Speculative decoding for VLA models.\n",
    "    \n",
    "    Uses a draft model (MiniVLA) to propose action tokens and a target model \n",
    "    (OpenVLA) to verify them.\n",
    "    \n",
    "    IMPORTANT: For speculative decoding to work correctly, both models should\n",
    "    share the same tokenizer/vocabulary. If they don't, action token remapping\n",
    "    is attempted but this only works for action tokens (last 256 tokens of vocab).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model,\n",
    "        draft_model,\n",
    "        target_processor=None,\n",
    "        gamma: int = 4,  # Number of draft tokens to propose at once\n",
    "        use_cache: bool = True,\n",
    "        temperature: float = 0.0,  # 0 = greedy/argmax\n",
    "        n_action_bins: int = 256,  # Number of action bins (tokens)\n",
    "        relaxed_acceptance_r: int = 0,  # Relaxed acceptance radius (0 = standard spec dec)\n",
    "        use_batched_verification: bool = False,  # Use single forward pass for verification (no cache)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the speculative decoder.\n",
    "        \n",
    "        Args:\n",
    "            target_model: OpenVLA model (larger, slower, more accurate)\n",
    "            draft_model: MiniVLA model (smaller, faster)\n",
    "            target_processor: HuggingFace processor for target model\n",
    "            gamma: Number of tokens to speculate at each step\n",
    "            use_cache: Whether to use KV caching\n",
    "            temperature: Sampling temperature (0 = greedy/argmax)\n",
    "            n_action_bins: Number of action token bins (typically 256)\n",
    "            relaxed_acceptance_r: Relaxed acceptance radius. If the draft token's bin\n",
    "                is within r bins of the target's preferred bin, accept it.\n",
    "                Set to 0 for standard speculative decoding behavior.\n",
    "            use_batched_verification: If True, run verification in a single forward pass\n",
    "                without KV cache. This re-processes the image but requires only 1 forward\n",
    "                pass for all gamma tokens.\n",
    "        \"\"\"\n",
    "        self.target = target_model\n",
    "        self.draft = draft_model\n",
    "        self.target_processor = target_processor\n",
    "        self.gamma = gamma\n",
    "        self.use_cache = use_cache\n",
    "        self.temperature = temperature\n",
    "        self.n_action_bins = n_action_bins\n",
    "        self.relaxed_acceptance_r = relaxed_acceptance_r\n",
    "        self.use_batched_verification = use_batched_verification\n",
    "        \n",
    "        # Get device\n",
    "        self.device = next(target_model.parameters()).device\n",
    "        \n",
    "        # Stats tracking\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "        \n",
    "        # Check vocabulary compatibility and setup token mapping\n",
    "        self._setup_token_mapping()\n",
    "    \n",
    "    def _setup_token_mapping(self):\n",
    "        \"\"\"Setup token mapping between draft and target vocabularies.\"\"\"\n",
    "        # Get vocabulary sizes\n",
    "        # Target model (OpenVLA/HF style) - use ACTUAL embedding dimension, not vocab_size attribute\n",
    "        # The embedding may be padded to \"multiple of\" for efficiency\n",
    "        if hasattr(self.target, 'language_model') and hasattr(self.target.language_model, 'model'):\n",
    "            # Actual embedding dimension (includes padding)\n",
    "            self.target_logit_dim = self.target.language_model.model.embed_tokens.weight.shape[0]\n",
    "        elif hasattr(self.target, 'get_output_embeddings'):\n",
    "            self.target_logit_dim = self.target.get_output_embeddings().weight.shape[0]\n",
    "        else:\n",
    "            self.target_logit_dim = self.target.config.vocab_size\n",
    "        \n",
    "        # Also get the \"logical\" vocab size (without padding) for action token calculation\n",
    "        if hasattr(self.target, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.vocab_size\n",
    "        elif hasattr(self.target, 'config') and hasattr(self.target.config, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.config.vocab_size\n",
    "        else:\n",
    "            self.target_vocab_size = self.target_logit_dim\n",
    "        \n",
    "        # Draft model (Prismatic style)\n",
    "        if hasattr(self.draft, 'llm_backbone'):\n",
    "            draft_tokenizer = self.draft.llm_backbone.tokenizer\n",
    "            # Qwen2 uses len(tokenizer) for full vocab including added tokens\n",
    "            self.draft_vocab_size = len(draft_tokenizer) if hasattr(draft_tokenizer, '__len__') else draft_tokenizer.vocab_size\n",
    "            # Get actual logit dimension from draft model\n",
    "            if hasattr(self.draft.llm_backbone, 'llm') and hasattr(self.draft.llm_backbone.llm, 'lm_head'):\n",
    "                self.draft_logit_dim = self.draft.llm_backbone.llm.lm_head.weight.shape[0]\n",
    "            else:\n",
    "                self.draft_logit_dim = self.draft_vocab_size\n",
    "        else:\n",
    "            self.draft_vocab_size = self.draft.config.vocab_size\n",
    "            self.draft_logit_dim = self.draft_vocab_size\n",
    "        \n",
    "        # Check if vocabularies match\n",
    "        self.vocab_compatible = (self.target_logit_dim == self.draft_logit_dim)\n",
    "        \n",
    "        # Compute action token ranges\n",
    "        # Action tokens are the LAST n_action_bins tokens before padding\n",
    "        # For target: use vocab_size (not padded logit_dim)\n",
    "        self.target_action_start = self.target_vocab_size - self.n_action_bins\n",
    "        self.draft_action_start = self.draft_vocab_size - self.n_action_bins\n",
    "        \n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Target vocab_size: {self.target_vocab_size}, logit_dim: {self.target_logit_dim}, action tokens: [{self.target_action_start}, {self.target_vocab_size})\")\n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Draft vocab_size: {self.draft_vocab_size}, logit_dim: {self.draft_logit_dim}, action tokens: [{self.draft_action_start}, {self.draft_vocab_size})\")\n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Vocabularies compatible: {self.vocab_compatible}\")\n",
    "        \n",
    "        if not self.vocab_compatible:\n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"WARNING: Vocabulary mismatch! Token remapping will be used for action tokens only.\")\n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"This may affect acceptance rates. Consider using models with matching tokenizers.\")\n",
    "        \n",
    "        # DEBUG: Verify mapping with example tokens\n",
    "        print(\"\\033[38;2;100;200;255m[DEBUG] Token mapping verification examples:\\033[0m\")\n",
    "        for bin_idx in [0, 127, 255]:  # First, middle, last action bins\n",
    "            draft_tok = self.draft_action_start + bin_idx\n",
    "            target_tok = self._draft_token_to_target(draft_tok)\n",
    "            draft_bin = self._get_action_bin_from_draft_token(draft_tok)\n",
    "            target_bin = self._get_action_bin_from_target_token(target_tok)\n",
    "            print(f\"  Bin {bin_idx}: draft_token={draft_tok}  target_token={target_tok} | draft_bin={draft_bin}, target_bin={target_bin} | {'' if draft_bin == target_bin else ''}\")\n",
    "    \n",
    "    def _draft_token_to_target(self, draft_token_id: int) -> int:\n",
    "        \"\"\"Map a draft token ID to target vocabulary.\"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return draft_token_id\n",
    "        \n",
    "        # Check if this is an action token (from end of draft vocabulary)\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            # Map to corresponding action token in target vocabulary\n",
    "            action_bin = draft_token_id - self.draft_action_start\n",
    "            target_token = self.target_action_start + action_bin\n",
    "            return target_token\n",
    "        else:\n",
    "            # Non-action token - this shouldn't happen during action generation\n",
    "            # Return as-is but clamp to valid range\n",
    "            return min(draft_token_id, self.target_vocab_size - 1)\n",
    "    \n",
    "    def _target_token_to_draft(self, target_token_id: int) -> int:\n",
    "        \"\"\"Map a target token ID to draft vocabulary.\"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return target_token_id\n",
    "        \n",
    "        # Check if this is an action token\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            action_bin = target_token_id - self.target_action_start\n",
    "            draft_token = self.draft_action_start + action_bin\n",
    "            return draft_token\n",
    "        else:\n",
    "            return min(target_token_id, self.draft_vocab_size - 1)\n",
    "    \n",
    "    def _remap_logits_draft_to_target(self, draft_logits: torch.Tensor, target_logit_dim: int = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Remap draft logits to target vocabulary space.\n",
    "        Only remaps action tokens; non-action tokens get -inf.\n",
    "        \n",
    "        Args:\n",
    "            draft_logits: Logits from draft model\n",
    "            target_logit_dim: Actual dimension of target logits (may differ from vocab_size due to padding)\n",
    "        \"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return draft_logits\n",
    "        \n",
    "        # Use provided dimension or fall back to stored logit_dim\n",
    "        if target_logit_dim is None:\n",
    "            target_logit_dim = self.target_logit_dim\n",
    "        \n",
    "        # Create target-sized logits filled with -inf (use actual logit dimension, not vocab_size)\n",
    "        target_logits = torch.full(\n",
    "            (draft_logits.shape[0], target_logit_dim),\n",
    "            float('-inf'),\n",
    "            device=draft_logits.device,\n",
    "            dtype=draft_logits.dtype\n",
    "        )\n",
    "        \n",
    "        # Copy action token logits from draft to corresponding target positions\n",
    "        # Draft action tokens: [draft_action_start, draft_vocab_size)\n",
    "        # Target action tokens: [target_action_start, target_vocab_size)\n",
    "        draft_action_logits = draft_logits[:, self.draft_action_start:self.draft_vocab_size]\n",
    "        target_logits[:, self.target_action_start:self.target_vocab_size] = draft_action_logits\n",
    "        \n",
    "        return target_logits\n",
    "    \n",
    "    # =========================================================================\n",
    "    # DEBUG: Token mapping verification methods\n",
    "    # =========================================================================\n",
    "    \n",
    "    def _get_action_bin_from_draft_token(self, draft_token_id: int) -> int:\n",
    "        \"\"\"Get action bin index from draft token ID.\"\"\"\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            return draft_token_id - self.draft_action_start\n",
    "        return -1  # Not an action token\n",
    "    \n",
    "    def _get_action_bin_from_target_token(self, target_token_id: int) -> int:\n",
    "        \"\"\"Get action bin index from target token ID.\"\"\"\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            return target_token_id - self.target_action_start\n",
    "        return -1  # Not an action token\n",
    "    \n",
    "    def _get_continuous_action_from_bin(self, bin_idx: int) -> float:\n",
    "        \"\"\"Convert action bin index to continuous action value using target's bin centers.\"\"\"\n",
    "        if 0 <= bin_idx < len(self.target.bin_centers):\n",
    "            return self.target.bin_centers[bin_idx]\n",
    "        return float('nan')\n",
    "    \n",
    "    def _debug_token_mapping(self, draft_token_id: int, target_token_id: int, prefix: str = \"\"):\n",
    "        \"\"\"Debug print showing token mapping verification.\"\"\"\n",
    "        draft_bin = self._get_action_bin_from_draft_token(draft_token_id)\n",
    "        target_bin = self._get_action_bin_from_target_token(target_token_id)\n",
    "        \n",
    "        draft_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "        target_action = self._get_continuous_action_from_bin(target_bin)\n",
    "        \n",
    "        match_status = \" MATCH\" if draft_bin == target_bin else \" MISMATCH\"\n",
    "        \n",
    "        print(f\"\\033[38;2;100;200;255m[DEBUG TOKEN MAP] {prefix}\\033[0m\")\n",
    "        print(f\"  Draft token:  {draft_token_id}  bin {draft_bin}  action {draft_action:.4f}\")\n",
    "        print(f\"  Target token: {target_token_id}  bin {target_bin}  action {target_action:.4f}\")\n",
    "        print(f\"  Bins match: {match_status}\")\n",
    "        \n",
    "        return draft_bin == target_bin\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        \"\"\"Reset statistics counters.\"\"\"\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "    \n",
    "    def _sample_token(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Sample a token from logits.\"\"\"\n",
    "        if self.temperature <= 0:\n",
    "            return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            probs = F.softmax(logits / self.temperature, dim=-1)\n",
    "            return torch.multinomial(probs.squeeze(0), num_samples=1).unsqueeze(0)\n",
    "    \n",
    "    def _get_probs(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert logits to probabilities.\"\"\"\n",
    "        if self.temperature <= 0:\n",
    "            # For greedy, use a very low temperature to approximate argmax\n",
    "            return F.softmax(logits / 0.01, dim=-1)\n",
    "        return F.softmax(logits / self.temperature, dim=-1)\n",
    "    \n",
    "    def _prepare_target_inputs(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Prepare inputs for the target (OpenVLA) model.\"\"\"\n",
    "        prompt = f\"In: What action should the robot take to {instruction.lower()}?\\nOut:\"\n",
    "        inputs = self.target_processor(prompt, image).to(self.device, dtype=torch.bfloat16)\n",
    "        \n",
    "        # IMPORTANT: Add the special empty token (29871) to input_ids if not present\n",
    "        # This is what OpenVLA's predict_action does - the action tokens come AFTER this token\n",
    "        if not torch.all(inputs[\"input_ids\"][:, -1] == 29871):\n",
    "            inputs[\"input_ids\"] = torch.cat(\n",
    "                (inputs[\"input_ids\"], torch.tensor([[29871]], device=self.device)),\n",
    "                dim=1\n",
    "            )\n",
    "            # Also extend attention mask\n",
    "            if \"attention_mask\" in inputs:\n",
    "                inputs[\"attention_mask\"] = torch.cat(\n",
    "                    (inputs[\"attention_mask\"], torch.ones((1, 1), device=self.device, dtype=inputs[\"attention_mask\"].dtype)),\n",
    "                    dim=1\n",
    "                )\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def _prepare_draft_inputs(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Prepare inputs for the draft (MiniVLA) model.\"\"\"\n",
    "        # Get prompt using draft model's prompt builder\n",
    "        prompt_builder = self.draft.get_prompt_builder()\n",
    "        prompt_builder.add_turn(role=\"human\", message=f\"What action should the robot take to {instruction.lower()}?\")\n",
    "        prompt_text = prompt_builder.get_prompt()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenizer = self.draft.llm_backbone.tokenizer\n",
    "        input_ids = tokenizer(prompt_text, truncation=True, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        \n",
    "        # Handle special token for Llama tokenizer\n",
    "        from transformers import LlamaTokenizerFast\n",
    "        if isinstance(tokenizer, LlamaTokenizerFast):\n",
    "            if not torch.all(input_ids[:, -1] == 29871):\n",
    "                input_ids = torch.cat(\n",
    "                    (input_ids, torch.tensor([[29871]], device=self.device)),\n",
    "                    dim=1\n",
    "                )\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Process image\n",
    "        image_transform = self.draft.vision_backbone.get_image_transform()\n",
    "        pixel_values = image_transform(image)\n",
    "        if isinstance(pixel_values, torch.Tensor):\n",
    "            pixel_values = pixel_values[None, ...].to(self.device)\n",
    "        elif isinstance(pixel_values, dict):\n",
    "            pixel_values = {k: v[None, ...].to(self.device) for k, v in pixel_values.items()}\n",
    "        \n",
    "        return input_ids, attention_mask, pixel_values\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict_action_speculative(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "        unnorm_key_target: str,\n",
    "    ) -> Tuple[np.ndarray, SpeculativeDecodingStats]:\n",
    "        \"\"\"\n",
    "        Generate action using speculative decoding.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image observation\n",
    "            instruction: Task instruction string\n",
    "            unnorm_key_target: Key for action un-normalization statistics of the target model\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (unnormalized action array, decoding statistics)\n",
    "        \"\"\"\n",
    "        # Reset per-call stats\n",
    "        call_stats = SpeculativeDecodingStats()\n",
    "        \n",
    "        action_dim = self.target.get_action_dim(unnorm_key_target)\n",
    "        \n",
    "        \n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Target vocab size: {self.target.vocab_size}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Target embedding size: {self.target.language_model.model.embed_tokens.weight.shape[0]}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Draft tokenizer vocab size: {self.draft.llm_backbone.tokenizer.vocab_size}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Action dim: {action_dim}\")\n",
    "        \n",
    "        # Prepare inputs for both models\n",
    "        target_inputs = self._prepare_target_inputs(image, instruction)\n",
    "        draft_input_ids, draft_attention_mask, draft_pixel_values = self._prepare_draft_inputs(image, instruction)\n",
    "        \n",
    "        # Cast draft inputs to appropriate dtype\n",
    "        autocast_dtype = self.draft.llm_backbone.half_precision_dtype\n",
    "        \n",
    "        # Initialize caches\n",
    "        target_cache = None\n",
    "        draft_cache = None\n",
    "        \n",
    "        generated_token_ids = []\n",
    "        \n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16, enabled=True):\n",
    "            # === Initial forward pass to get KV cache and initial logits ===\n",
    "            # NOTE: After adding token 29871 to input, the model should output action tokens directly\n",
    "            \n",
    "            # Target model initial forward (prefill)\n",
    "            target_out = self.target(\n",
    "                **target_inputs,\n",
    "                past_key_values=None,\n",
    "                use_cache=self.use_cache,\n",
    "            )\n",
    "            target_cache = target_out.past_key_values\n",
    "            target_logits = target_out.logits[:, -1, :]\n",
    "            call_stats.total_target_forward_passes += 1\n",
    "            \n",
    "            # DEBUG: Check what the target model wants to output first\n",
    "            top_target_token = torch.argmax(target_logits, dim=-1).item()\n",
    "            top_target_bin = self._get_action_bin_from_target_token(top_target_token)\n",
    "            print(f\"\\033[38;2;100;200;255m[DEBUG] After prefill, target top token:\\033[0m\")\n",
    "            print(f\"  top_token={top_target_token}  bin={top_target_bin}  is_action_token={top_target_bin >= 0}\")\n",
    "            \n",
    "            # Draft model initial forward (prefill)\n",
    "            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                draft_out = self.draft(\n",
    "                    input_ids=draft_input_ids,\n",
    "                    attention_mask=draft_attention_mask,\n",
    "                    pixel_values=draft_pixel_values,\n",
    "                    past_key_values=None,\n",
    "                    use_cache=self.use_cache,\n",
    "                )\n",
    "            draft_cache = draft_out.past_key_values\n",
    "            draft_logits = draft_out.logits[:, -1, :]\n",
    "            call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            # DEBUG: Check what the draft model wants to output first\n",
    "            top_draft_token = torch.argmax(draft_logits, dim=-1).item()\n",
    "            top_draft_bin = self._get_action_bin_from_draft_token(top_draft_token)\n",
    "            print(f\"\\033[38;2;100;200;255m[DEBUG] After prefill, draft top token:\\033[0m\")\n",
    "            print(f\"  top_token={top_draft_token}  bin={top_draft_bin}  is_action_token={top_draft_bin >= 0}\")\n",
    "            \n",
    "            # === Main speculative decoding loop ===\n",
    "            # We start directly with drafting - no need to sample a first token separately\n",
    "            while len(generated_token_ids) < action_dim:\n",
    "                # Determine how many tokens to speculate\n",
    "                gamma = min(self.gamma, action_dim - len(generated_token_ids))\n",
    "                \n",
    "                # Generate gamma draft tokens\n",
    "                draft_tokens = []\n",
    "                draft_probs_list = []\n",
    "                \n",
    "                current_draft_cache = draft_cache\n",
    "                current_draft_logits = draft_logits\n",
    "                \n",
    "                for _ in range(gamma):\n",
    "                    draft_probs = self._get_probs(current_draft_logits)\n",
    "                    draft_token = self._sample_token(current_draft_logits)\n",
    "                    \n",
    "                    draft_tokens.append(draft_token)\n",
    "                    draft_probs_list.append(draft_probs)\n",
    "                    \n",
    "                    # Advance draft model\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=draft_token.to(self.device),\n",
    "                            past_key_values=current_draft_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                    current_draft_cache = draft_step.past_key_values\n",
    "                    current_draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                \n",
    "                call_stats.total_draft_tokens_proposed += gamma\n",
    "                \n",
    "                # Map draft tokens to target vocab space for verification\n",
    "                draft_token_ids_in_target_vocab = []\n",
    "                print(f\"\\033[38;2;100;200;255m[DEBUG] Mapping {gamma} draft tokens to target vocab:\\033[0m\")\n",
    "                for idx, dt in enumerate(draft_tokens):\n",
    "                    draft_id = dt.item()\n",
    "                    target_id = self._draft_token_to_target(draft_id)\n",
    "                    draft_token_ids_in_target_vocab.append(target_id)\n",
    "                    \n",
    "                    # Verify mapping preserves action bin\n",
    "                    draft_bin = self._get_action_bin_from_draft_token(draft_id)\n",
    "                    target_bin = self._get_action_bin_from_target_token(target_id)\n",
    "                    draft_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "                    target_action = self._get_continuous_action_from_bin(target_bin)\n",
    "                    match = \"\" if draft_bin == target_bin else \" MISMATCH!\"\n",
    "                    print(f\"  [{idx}] draft_tok={draft_id}  target_tok={target_id} | bin: {draft_bin}{target_bin} | action: {draft_action:.4f}{target_action:.4f} {match}\")\n",
    "                \n",
    "                # Verify with target model\n",
    "                if self.use_batched_verification:\n",
    "                    # BATCHED VERIFICATION: Single forward pass without cache\n",
    "                    # Re-processes the image but only 1 forward pass for all tokens\n",
    "                    \n",
    "                    # Build full input: original prompt + all draft tokens\n",
    "                    # We need to re-run the full forward pass with image\n",
    "                    draft_tokens_tensor = torch.tensor([draft_token_ids_in_target_vocab], device=self.device)  # [1, gamma]\n",
    "                    \n",
    "                    # Concatenate original input_ids with draft tokens\n",
    "                    full_input_ids = torch.cat([\n",
    "                        target_inputs[\"input_ids\"],\n",
    "                        draft_tokens_tensor\n",
    "                    ], dim=1)\n",
    "                    \n",
    "                    # Run full forward pass (no cache, with image)\n",
    "                    target_verify_out = self.target(\n",
    "                        input_ids=full_input_ids,\n",
    "                        attention_mask=torch.ones_like(full_input_ids),\n",
    "                        pixel_values=target_inputs.get(\"pixel_values\"),\n",
    "                        past_key_values=None,  # No cache!\n",
    "                        use_cache=False,\n",
    "                    )\n",
    "                    call_stats.total_target_forward_passes += 1\n",
    "                    \n",
    "                    # print(f\"\\033[38;2;100;200;255m[SRP] -- Target verify out:\\033[0m {target_verify_out}, shape: {target_verify_out.logits.shape}\")\n",
    "                    \n",
    "                    # Extract logits for draft token positions\n",
    "                    # Original input has length L, draft tokens are at positions L to L+gamma-1\n",
    "                    # Logits at position i predict token at position i+1\n",
    "                    # So logits at L-1 predict first draft token, logits at L predict second, etc.\n",
    "                    original_len = target_inputs[\"input_ids\"].shape[1]\n",
    "                    \n",
    "                    # target_verify_out.logits has shape [1, L + gamma, vocab]\n",
    "                    # We need logits at positions [L-1, L, L+1, ..., L+gamma-2] to evaluate draft tokens [0, 1, ..., gamma-1]\n",
    "                    target_logits_batch = target_verify_out.logits[:, original_len-1:original_len-1+gamma, :]  # [1, gamma, vocab]\n",
    "                    target_probs_batch = self._get_probs(target_logits_batch)\n",
    "                    \n",
    "                    # Last logits for bonus token (at position L+gamma-1)\n",
    "                    last_target_logits = target_verify_out.logits[:, -1, :]\n",
    "                    \n",
    "                    # No cache to update in batched mode\n",
    "                    target_cache_for_verify = None\n",
    "                    \n",
    "                    print(f\"\\033[38;2;100;200;255m[DEBUG] BATCHED verification: 1 forward pass for {gamma} tokens\\033[0m\")\n",
    "                    \n",
    "                else:\n",
    "                    # SEQUENTIAL VERIFICATION: One forward pass per token with KV cache\n",
    "                    # NOTE: OpenVLA/Prismatic models only support single-token inference with KV cache\n",
    "                    \n",
    "                    target_cache_for_verify = target_cache\n",
    "                    # Start with current target_logits for evaluating first draft token\n",
    "                    target_logits_for_verification = [target_logits.unsqueeze(1)]  # [1, 1, vocab]\n",
    "                    last_target_logits = None\n",
    "                    \n",
    "                    for i in range(gamma):\n",
    "                        target_token_input = torch.tensor([[draft_token_ids_in_target_vocab[i]]], device=self.device)\n",
    "                        target_step = self.target(\n",
    "                            input_ids=target_token_input,\n",
    "                            past_key_values=target_cache_for_verify,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                        target_cache_for_verify = target_step.past_key_values\n",
    "                        last_target_logits = target_step.logits[:, -1, :]\n",
    "                        # Store logits for evaluating the NEXT token (positions 1 to gamma-1)\n",
    "                        if i < gamma - 1:\n",
    "                            target_logits_for_verification.append(target_step.logits[:, -1:, :])\n",
    "                        call_stats.total_target_forward_passes += 1\n",
    "                    \n",
    "                    # Stack target logits - target_logits_for_verification[i] evaluates draft_token[i]\n",
    "                    target_logits_batch = torch.cat(target_logits_for_verification, dim=1)  # [1, gamma, vocab]\n",
    "                    target_probs_batch = self._get_probs(target_logits_batch)\n",
    "                \n",
    "                # DEBUG: Show which token the target actually wants at each position\n",
    "                print(f\"\\033[38;2;100;200;255m[DEBUG] Target's preferred tokens at each position:\\033[0m\")\n",
    "                for i in range(gamma):\n",
    "                    top_tok = torch.argmax(target_logits_batch[0, i, :]).item()\n",
    "                    top_bin = self._get_action_bin_from_target_token(top_tok)\n",
    "                    draft_tok = draft_token_ids_in_target_vocab[i]\n",
    "                    draft_bin = self._get_action_bin_from_target_token(draft_tok)\n",
    "                    p_top = target_probs_batch[0, i, top_tok].item()\n",
    "                    p_draft = target_probs_batch[0, i, draft_tok].item()\n",
    "                    print(f\"  pos[{i}]: target_wants={top_tok}(bin={top_bin}, p={p_top:.4f}) | draft_proposed={draft_tok}(bin={draft_bin}, p={p_draft:.4f})\")\n",
    "                \n",
    "                # Get actual target logit dimension from the output\n",
    "                actual_target_logit_dim = target_logits_batch.shape[-1]\n",
    "                \n",
    "                # Remap draft probs to target vocab space for comparison\n",
    "                # Use actual target logit dimension to ensure tensor size match\n",
    "                draft_probs_remapped = [self._remap_logits_draft_to_target(dp, actual_target_logit_dim) for dp in draft_probs_list]\n",
    "                draft_probs_remapped = [self._get_probs(dp) for dp in draft_probs_remapped]\n",
    "                \n",
    "                # Rejection sampling loop\n",
    "                n_accepted = 0\n",
    "                for i in range(gamma):\n",
    "                    draft_token_id_draft = draft_tokens[i].item()  # In draft vocab\n",
    "                    draft_token_id_target = draft_token_ids_in_target_vocab[i]  # Mapped to target vocab\n",
    "                    \n",
    "                    draft_prob_remapped = draft_probs_remapped[i]\n",
    "                    target_prob = target_probs_batch[:, i, :]\n",
    "                    \n",
    "                    # Get probability of the token under both models (in target vocab space)\n",
    "                    p_target = target_prob[0, draft_token_id_target].item()\n",
    "                    p_draft = draft_prob_remapped[0, draft_token_id_target].item()\n",
    "                    \n",
    "                    # Get the target's preferred token and compute bin distances\n",
    "                    target_preferred_token = torch.argmax(target_prob, dim=-1).item()\n",
    "                    draft_bin = self._get_action_bin_from_target_token(draft_token_id_target)\n",
    "                    target_bin = self._get_action_bin_from_target_token(target_preferred_token)\n",
    "                    \n",
    "                    # Compute bin distance (for relaxed acceptance)\n",
    "                    if draft_bin >= 0 and target_bin >= 0:\n",
    "                        bin_distance = abs(draft_bin - target_bin)\n",
    "                    else:\n",
    "                        bin_distance = float('inf')  # Non-action tokens don't benefit from relaxed acceptance\n",
    "                    \n",
    "                    # Relaxed acceptance: accept if within r bins of target's preference\n",
    "                    relaxed_accept = (self.relaxed_acceptance_r > 0 and bin_distance <= self.relaxed_acceptance_r)\n",
    "                    \n",
    "                    # Standard rejection sampling\n",
    "                    if p_draft > 0:\n",
    "                        acceptance_prob = min(1.0, p_target / p_draft)\n",
    "                    else:\n",
    "                        acceptance_prob = 1.0 if p_target > 0 else 0.0\n",
    "                    \n",
    "                    standard_accept = (torch.rand(1).item() < acceptance_prob)\n",
    "                    \n",
    "                    # Accept if either relaxed acceptance or standard acceptance passes\n",
    "                    if relaxed_accept or standard_accept:\n",
    "                        # Accept this token (store in target vocab space)\n",
    "                        accepted_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "                        accept_reason = \"RELAXED\" if relaxed_accept and not standard_accept else \"STANDARD\"\n",
    "                        print(f\"\\033[38;2;0;255;0m[ACCEPT-{accept_reason}]\\033[0m token[{i}]: target_tok={draft_token_id_target}  bin={draft_bin}  action={accepted_action:.4f}\")\n",
    "                        print(f\"  target_preferred: bin={target_bin} | bin_distance={bin_distance} | r={self.relaxed_acceptance_r}\")\n",
    "                        print(f\"  p_target={p_target:.4f}, p_draft={p_draft:.4f}, accept_prob={acceptance_prob:.4f}\")\n",
    "                        generated_token_ids.append(draft_token_id_target)\n",
    "                        n_accepted += 1\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        call_stats.total_draft_tokens_accepted += 1\n",
    "                        \n",
    "                        if len(generated_token_ids) >= action_dim:\n",
    "                            print(f\"\\033[38;2;255;165;0m[SRP] -> \\033[0m Generated {len(generated_token_ids)}/{action_dim} tokens, done.\")\n",
    "                            break\n",
    "                    else:\n",
    "                        # Reject - sample from adjusted distribution\n",
    "                        adjusted_probs = max_fn(target_prob - draft_prob_remapped)\n",
    "                        if adjusted_probs.sum() > 0:\n",
    "                            corrected_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
    "                        else:\n",
    "                            corrected_token = self._sample_token(target_prob.unsqueeze(0))\n",
    "                        \n",
    "                        corrected_token_id = int(corrected_token.item())\n",
    "                        corrected_bin = self._get_action_bin_from_target_token(corrected_token_id)\n",
    "                        rejected_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "                        corrected_action = self._get_continuous_action_from_bin(corrected_bin)\n",
    "                        bin_diff = abs(draft_bin - corrected_bin) if draft_bin >= 0 and corrected_bin >= 0 else -1\n",
    "                        \n",
    "                        print(f\"\\033[38;2;255;100;100m[REJECT]\\033[0m token[{i}]: bin_distance={bin_distance} > r={self.relaxed_acceptance_r}\")\n",
    "                        print(f\"  Draft proposed: target_tok={draft_token_id_target}  bin={draft_bin}  action={rejected_action:.4f}\")\n",
    "                        print(f\"  Target prefers: target_tok={target_preferred_token}  bin={target_bin}\")\n",
    "                        print(f\"  Corrected to:   target_tok={corrected_token_id}  bin={corrected_bin}  action={corrected_action:.4f}\")\n",
    "                        print(f\"  p_target={p_target:.4f}, p_draft={p_draft:.4f}, accept_prob={acceptance_prob:.4f}\")\n",
    "                        \n",
    "                        # Store corrected token (already in target vocab space)\n",
    "                        generated_token_ids.append(corrected_token_id)\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        n_accepted = i  # Number of accepted tokens (before this rejection)\n",
    "                        break\n",
    "                                    \n",
    "                # Update caches after acceptance/rejection\n",
    "                if n_accepted == gamma and len(generated_token_ids) < action_dim:\n",
    "                    # All accepted - need to sample one more from target\n",
    "                    print(f\"\\033[38;2;0;255;0m[ALL ACCEPTED]\\033[0m All {gamma} draft tokens accepted! Sampling bonus token from target...\")\n",
    "                    if not self.use_batched_verification:\n",
    "                        target_cache = target_cache_for_verify\n",
    "                    target_logits = last_target_logits  # Logits after feeding all gamma tokens\n",
    "                    \n",
    "                    # Sample additional token from target (in target vocab space)\n",
    "                    bonus_token_target = self._sample_token(target_logits)\n",
    "                    bonus_token_id_target = int(bonus_token_target.item())\n",
    "                    bonus_bin = self._get_action_bin_from_target_token(bonus_token_id_target)\n",
    "                    bonus_action = self._get_continuous_action_from_bin(bonus_bin)\n",
    "                    print(f\"  Bonus token: target_tok={bonus_token_id_target}  bin={bonus_bin}  action={bonus_action:.4f}\")\n",
    "                    generated_token_ids.append(bonus_token_id_target)\n",
    "                    call_stats.total_tokens_generated += 1\n",
    "                    \n",
    "                    # if self.use_batched_verification:\n",
    "                    #     # In batched mode, rebuild context for next round\n",
    "                    #     # Actually, if all tokens were accepted and we generated a bonus token,\n",
    "                    #     # we need to rebuild context with all tokens for the next speculation round\n",
    "                    #     if len(generated_token_ids) < action_dim:\n",
    "                    #         generated_tokens_tensor = torch.tensor([generated_token_ids], device=self.device)\n",
    "                    #         full_input_ids = torch.cat([\n",
    "                    #             target_inputs[\"input_ids\"],\n",
    "                    #             generated_tokens_tensor\n",
    "                    #         ], dim=1)\n",
    "                            \n",
    "                    #         target_step = self.target(\n",
    "                    #             input_ids=full_input_ids,\n",
    "                    #             attention_mask=torch.ones_like(full_input_ids),\n",
    "                    #             pixel_values=target_inputs.get(\"pixel_values\"),\n",
    "                    #             past_key_values=None,\n",
    "                    #             use_cache=False,\n",
    "                    #         )\n",
    "                    #         target_logits = target_step.logits[:, -1, :]\n",
    "                    #         call_stats.total_target_forward_passes += 1\n",
    "                            \n",
    "                    #         # Same for draft\n",
    "                    #         generated_tokens_draft = [self._target_token_to_draft(t) for t in generated_token_ids]\n",
    "                    #         generated_tokens_draft_tensor = torch.tensor([generated_tokens_draft], device=self.device)\n",
    "                    #         with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                    #             draft_full_ids = torch.cat([draft_input_ids, generated_tokens_draft_tensor], dim=1)\n",
    "                    #             draft_step = self.draft(\n",
    "                    #                 input_ids=draft_full_ids,\n",
    "                    #                 attention_mask=torch.ones_like(draft_full_ids),\n",
    "                    #                 pixel_values=draft_pixel_values,\n",
    "                    #                 past_key_values=None,\n",
    "                    #                 use_cache=False,\n",
    "                    #             )\n",
    "                    #         draft_logits = draft_step.logits[:, -1, :]\n",
    "                    #         call_stats.total_draft_forward_passes += 1\n",
    "                    # else:\n",
    "                    if not self.use_batched_verification:\n",
    "                        # Update target cache\n",
    "                        target_step = self.target(\n",
    "                            input_ids=bonus_token_target,\n",
    "                            past_key_values=target_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                        target_cache = target_step.past_key_values\n",
    "                        target_logits = target_step.logits[:, -1, :]\n",
    "                        call_stats.total_target_forward_passes += 1\n",
    "                        \n",
    "                        # Map bonus token to draft vocab and update draft cache\n",
    "                        bonus_token_id_draft = self._target_token_to_draft(bonus_token_id_target)\n",
    "                        bonus_draft_bin = self._get_action_bin_from_draft_token(bonus_token_id_draft)\n",
    "                        print(f\"  Mapped to draft: draft_tok={bonus_token_id_draft}  bin={bonus_draft_bin} {'' if bonus_bin == bonus_draft_bin else ' MISMATCH!'}\")\n",
    "                        bonus_token_draft = torch.tensor([[bonus_token_id_draft]], device=self.device)\n",
    "                        \n",
    "                        draft_cache = current_draft_cache\n",
    "                        with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                            draft_step = self.draft(\n",
    "                                input_ids=bonus_token_draft,\n",
    "                                past_key_values=draft_cache,\n",
    "                                use_cache=self.use_cache,\n",
    "                            )\n",
    "                        draft_cache = draft_step.past_key_values\n",
    "                        draft_logits = draft_step.logits[:, -1, :]\n",
    "                        call_stats.total_draft_forward_passes += 1\n",
    "                    \n",
    "                else:\n",
    "                    # Some tokens rejected\n",
    "                    if self.use_batched_verification:\n",
    "                        # In batched mode, we don't have caches\n",
    "                        # For next round, we'll rebuild everything from scratch\n",
    "                        # We just need to get the logits for the next draft round\n",
    "                        if len(generated_token_ids) < action_dim:\n",
    "                            # Rebuild full context with all generated tokens so far\n",
    "                            generated_tokens_tensor = torch.tensor([generated_token_ids], device=self.device)\n",
    "                            full_input_ids = torch.cat([\n",
    "                                target_inputs[\"input_ids\"],\n",
    "                                generated_tokens_tensor\n",
    "                            ], dim=1)\n",
    "                            \n",
    "                            # Run forward pass to get logits for next position\n",
    "                            target_step = self.target(\n",
    "                                input_ids=full_input_ids,\n",
    "                                attention_mask=torch.ones_like(full_input_ids),\n",
    "                                pixel_values=target_inputs.get(\"pixel_values\"),\n",
    "                                past_key_values=None,\n",
    "                                use_cache=False,\n",
    "                            )\n",
    "                            target_logits = target_step.logits[:, -1, :]\n",
    "                            call_stats.total_target_forward_passes += 1\n",
    "                            \n",
    "                            # Same for draft model\n",
    "                            generated_tokens_draft = [self._target_token_to_draft(t) for t in generated_token_ids]\n",
    "                            generated_tokens_draft_tensor = torch.tensor([generated_tokens_draft], device=self.device)\n",
    "                            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                                # Need to rebuild draft input too\n",
    "                                draft_full_ids = torch.cat([draft_input_ids, generated_tokens_draft_tensor], dim=1)\n",
    "                                draft_step = self.draft(\n",
    "                                    input_ids=draft_full_ids,\n",
    "                                    attention_mask=torch.ones_like(draft_full_ids),\n",
    "                                    pixel_values=draft_pixel_values,\n",
    "                                    past_key_values=None,\n",
    "                                    use_cache=False,\n",
    "                                )\n",
    "                            draft_logits = draft_step.logits[:, -1, :]\n",
    "                            call_stats.total_draft_forward_passes += 1\n",
    "                    else:\n",
    "                        # Sequential mode - prune caches\n",
    "                        tokens_to_discard = gamma - n_accepted\n",
    "                        if tokens_to_discard > 0 and self.use_cache:\n",
    "                            # We need to prune and resync\n",
    "                            # Use the cache state after the accepted tokens\n",
    "                            target_cache = target_cache_for_verify\n",
    "                            if tokens_to_discard > 0:\n",
    "                                target_cache = prune_cache(target_cache, tokens_to_discard)\n",
    "                            \n",
    "                            # Rebuild draft cache\n",
    "                            draft_cache = prune_cache(current_draft_cache, gamma - n_accepted)\n",
    "                        \n",
    "                        # Get logits for next round\n",
    "                        if len(generated_token_ids) < action_dim:\n",
    "                            # Last token is in target vocab space\n",
    "                            last_token_id_target = generated_token_ids[-1]\n",
    "                            last_token_target = torch.tensor([[last_token_id_target]], device=self.device)\n",
    "                            \n",
    "                            target_step = self.target(\n",
    "                                input_ids=last_token_target,\n",
    "                                past_key_values=target_cache,\n",
    "                                use_cache=self.use_cache,\n",
    "                            )\n",
    "                            target_cache = target_step.past_key_values\n",
    "                            target_logits = target_step.logits[:, -1, :]\n",
    "                            call_stats.total_target_forward_passes += 1\n",
    "                            \n",
    "                            # Map to draft vocab for draft model\n",
    "                            last_token_id_draft = self._target_token_to_draft(last_token_id_target)\n",
    "                            last_token_draft = torch.tensor([[last_token_id_draft]], device=self.device)\n",
    "                            \n",
    "                            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                                draft_step = self.draft(\n",
    "                                    input_ids=last_token_draft,\n",
    "                                    past_key_values=draft_cache,\n",
    "                                    use_cache=self.use_cache,\n",
    "                                )\n",
    "                            draft_cache = draft_step.past_key_values\n",
    "                            draft_logits = draft_step.logits[:, -1, :]\n",
    "                            call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Call stats: {call_stats}\")\n",
    "            \n",
    "        # Decode tokens to actions\n",
    "        predicted_action_token_ids = np.array(generated_token_ids[:action_dim], dtype=np.int64)\n",
    "        \n",
    "        # DEBUG: Show all generated tokens with their action values\n",
    "        print(f\"\\033[38;2;100;200;255m[DEBUG] Final generated tokens summary:\\033[0m\")\n",
    "        for dim_idx, tok_id in enumerate(predicted_action_token_ids):\n",
    "            bin_idx = self._get_action_bin_from_target_token(int(tok_id))\n",
    "            action_val = self._get_continuous_action_from_bin(bin_idx)\n",
    "            print(f\"  dim[{dim_idx}]: target_tok={tok_id}  bin={bin_idx}  normalized_action={action_val:.4f}\")\n",
    "        \n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Predicted action token ids: {predicted_action_token_ids}\")\n",
    "        \n",
    "        # Use target model's decoding (vocab_size - token_id approach)\n",
    "        vocab_size = self.target.vocab_size\n",
    "        discretized_actions = vocab_size - predicted_action_token_ids\n",
    "        discretized_actions = np.clip(discretized_actions - 1, a_min=0, a_max=self.target.bin_centers.shape[0] - 1)\n",
    "        normalized_actions = self.target.bin_centers[discretized_actions]\n",
    "        \n",
    "        # Un-normalize actions\n",
    "        action_norm_stats = self.target.get_action_stats(unnorm_key_target)\n",
    "        mask = action_norm_stats.get(\"mask\", np.ones_like(action_norm_stats[\"q01\"], dtype=bool))\n",
    "        action_high, action_low = np.array(action_norm_stats[\"q99\"]), np.array(action_norm_stats[\"q01\"])\n",
    "        actions = np.where(\n",
    "            mask,\n",
    "            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,\n",
    "            normalized_actions,\n",
    "        )\n",
    "        \n",
    "        # Update global stats\n",
    "        self.stats.total_tokens_generated += call_stats.total_tokens_generated\n",
    "        self.stats.total_draft_tokens_proposed += call_stats.total_draft_tokens_proposed\n",
    "        self.stats.total_draft_tokens_accepted += call_stats.total_draft_tokens_accepted\n",
    "        self.stats.total_target_forward_passes += call_stats.total_target_forward_passes\n",
    "        self.stats.total_draft_forward_passes += call_stats.total_draft_forward_passes\n",
    "        \n",
    "        return actions, call_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLASpeculativeDecoderDDDRKVB:\n",
    "    \"\"\"\n",
    "    Speculative decoding for VLA models.\n",
    "    \n",
    "    Uses a draft model (MiniVLA) to propose action tokens and a target model \n",
    "    (OpenVLA) to verify them.\n",
    "    \n",
    "    IMPORTANT: For speculative decoding to work correctly, both models should\n",
    "    share the same tokenizer/vocabulary. If they don't, action token remapping\n",
    "    is attempted but this only works for action tokens (last 256 tokens of vocab).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model,\n",
    "        draft_model,\n",
    "        target_processor=None,\n",
    "        gamma: int = 4,  # Number of draft tokens to propose at once\n",
    "        use_cache: bool = True,\n",
    "        temperature: float = 0.0,  # 0 = greedy/argmax\n",
    "        n_action_bins: int = 256,  # Number of action bins (tokens)\n",
    "        relaxed_acceptance_r: int = 0,  # Relaxed acceptance radius (0 = standard spec dec)\n",
    "        use_batched_verification: bool = False,  # Use single forward pass for verification (no cache)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the speculative decoder.\n",
    "        \n",
    "        Args:\n",
    "            target_model: OpenVLA model (larger, slower, more accurate)\n",
    "            draft_model: MiniVLA model (smaller, faster)\n",
    "            target_processor: HuggingFace processor for target model\n",
    "            gamma: Number of tokens to speculate at each step\n",
    "            use_cache: Whether to use KV caching\n",
    "            temperature: Sampling temperature (0 = greedy/argmax)\n",
    "            n_action_bins: Number of action token bins (typically 256)\n",
    "            relaxed_acceptance_r: Relaxed acceptance radius. If the draft token's bin\n",
    "                is within r bins of the target's preferred bin, accept it.\n",
    "                Set to 0 for standard speculative decoding behavior.\n",
    "            use_batched_verification: If True, run verification in a single forward pass\n",
    "                without KV cache. This re-processes the image but requires only 1 forward\n",
    "                pass for all gamma tokens.\n",
    "        \"\"\"\n",
    "        self.target = target_model\n",
    "        self.draft = draft_model\n",
    "        self.target_processor = target_processor\n",
    "        self.gamma = gamma\n",
    "        self.use_cache = use_cache\n",
    "        self.temperature = temperature\n",
    "        self.n_action_bins = n_action_bins\n",
    "        self.relaxed_acceptance_r = relaxed_acceptance_r\n",
    "        self.use_batched_verification = use_batched_verification\n",
    "        \n",
    "        # Get device\n",
    "        self.device = next(target_model.parameters()).device\n",
    "        \n",
    "        # Stats tracking\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "        \n",
    "        # Check vocabulary compatibility and setup token mapping\n",
    "        self._setup_token_mapping()\n",
    "    \n",
    "    def _setup_token_mapping(self):\n",
    "        \"\"\"Setup token mapping between draft and target vocabularies.\"\"\"\n",
    "        # Get vocabulary sizes\n",
    "        # Target model (OpenVLA/HF style) - use ACTUAL embedding dimension, not vocab_size attribute\n",
    "        # The embedding may be padded to \"multiple of\" for efficiency\n",
    "        if hasattr(self.target, 'language_model') and hasattr(self.target.language_model, 'model'):\n",
    "            # Actual embedding dimension (includes padding)\n",
    "            self.target_logit_dim = self.target.language_model.model.embed_tokens.weight.shape[0]\n",
    "        elif hasattr(self.target, 'get_output_embeddings'):\n",
    "            self.target_logit_dim = self.target.get_output_embeddings().weight.shape[0]\n",
    "        else:\n",
    "            self.target_logit_dim = self.target.config.vocab_size\n",
    "        \n",
    "        # Also get the \"logical\" vocab size (without padding) for action token calculation\n",
    "        if hasattr(self.target, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.vocab_size\n",
    "        elif hasattr(self.target, 'config') and hasattr(self.target.config, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.config.vocab_size\n",
    "        else:\n",
    "            self.target_vocab_size = self.target_logit_dim\n",
    "        \n",
    "        # Draft model (Prismatic style)\n",
    "        if hasattr(self.draft, 'llm_backbone'):\n",
    "            draft_tokenizer = self.draft.llm_backbone.tokenizer\n",
    "            # Qwen2 uses len(tokenizer) for full vocab including added tokens\n",
    "            self.draft_vocab_size = len(draft_tokenizer) if hasattr(draft_tokenizer, '__len__') else draft_tokenizer.vocab_size\n",
    "            # Get actual logit dimension from draft model\n",
    "            if hasattr(self.draft.llm_backbone, 'llm') and hasattr(self.draft.llm_backbone.llm, 'lm_head'):\n",
    "                self.draft_logit_dim = self.draft.llm_backbone.llm.lm_head.weight.shape[0]\n",
    "            else:\n",
    "                self.draft_logit_dim = self.draft_vocab_size\n",
    "        else:\n",
    "            self.draft_vocab_size = self.draft.config.vocab_size\n",
    "            self.draft_logit_dim = self.draft_vocab_size\n",
    "        \n",
    "        # Check if vocabularies match\n",
    "        self.vocab_compatible = (self.target_logit_dim == self.draft_logit_dim)\n",
    "        \n",
    "        # Compute action token ranges\n",
    "        # Action tokens are the LAST n_action_bins tokens before padding\n",
    "        # For target: use vocab_size (not padded logit_dim)\n",
    "        self.target_action_start = self.target_vocab_size - self.n_action_bins\n",
    "        self.draft_action_start = self.draft_vocab_size - self.n_action_bins\n",
    "        \n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Target vocab_size: {self.target_vocab_size}, logit_dim: {self.target_logit_dim}, action tokens: [{self.target_action_start}, {self.target_vocab_size})\")\n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Draft vocab_size: {self.draft_vocab_size}, logit_dim: {self.draft_logit_dim}, action tokens: [{self.draft_action_start}, {self.draft_vocab_size})\")\n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"Vocabularies compatible: {self.vocab_compatible}\")\n",
    "        \n",
    "        if not self.vocab_compatible:\n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"WARNING: Vocabulary mismatch! Token remapping will be used for action tokens only.\")\n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"This may affect acceptance rates. Consider using models with matching tokenizers.\")\n",
    "        \n",
    "        # DEBUG: Verify mapping with example tokens\n",
    "        print(\"\\033[38;2;100;200;255m[DEBUG] Token mapping verification examples:\\033[0m\")\n",
    "        for bin_idx in [0, 127, 255]:  # First, middle, last action bins\n",
    "            draft_tok = self.draft_action_start + bin_idx\n",
    "            target_tok = self._draft_token_to_target(draft_tok)\n",
    "            draft_bin = self._get_action_bin_from_draft_token(draft_tok)\n",
    "            target_bin = self._get_action_bin_from_target_token(target_tok)\n",
    "            print(f\"  Bin {bin_idx}: draft_token={draft_tok}  target_token={target_tok} | draft_bin={draft_bin}, target_bin={target_bin} | {'' if draft_bin == target_bin else ''}\")\n",
    "    \n",
    "    def _draft_token_to_target(self, draft_token_id: int) -> int:\n",
    "        \"\"\"Map a draft token ID to target vocabulary.\"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return draft_token_id\n",
    "        \n",
    "        # Check if this is an action token (from end of draft vocabulary)\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            # Map to corresponding action token in target vocabulary\n",
    "            action_bin = draft_token_id - self.draft_action_start\n",
    "            target_token = self.target_action_start + action_bin\n",
    "            return target_token\n",
    "        else:\n",
    "            # Non-action token - this shouldn't happen during action generation\n",
    "            # Return as-is but clamp to valid range\n",
    "            return min(draft_token_id, self.target_vocab_size - 1)\n",
    "    \n",
    "    def _target_token_to_draft(self, target_token_id: int) -> int:\n",
    "        \"\"\"Map a target token ID to draft vocabulary.\"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return target_token_id\n",
    "        \n",
    "        # Check if this is an action token\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            action_bin = target_token_id - self.target_action_start\n",
    "            draft_token = self.draft_action_start + action_bin\n",
    "            return draft_token\n",
    "        else:\n",
    "            return min(target_token_id, self.draft_vocab_size - 1)\n",
    "    \n",
    "    def _remap_logits_draft_to_target(self, draft_logits: torch.Tensor, target_logit_dim: int = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Remap draft logits to target vocabulary space.\n",
    "        Only remaps action tokens; non-action tokens get -inf.\n",
    "        \n",
    "        Args:\n",
    "            draft_logits: Logits from draft model\n",
    "            target_logit_dim: Actual dimension of target logits (may differ from vocab_size due to padding)\n",
    "        \"\"\"\n",
    "        if self.vocab_compatible:\n",
    "            return draft_logits\n",
    "        \n",
    "        # Use provided dimension or fall back to stored logit_dim\n",
    "        if target_logit_dim is None:\n",
    "            target_logit_dim = self.target_logit_dim\n",
    "        \n",
    "        # Create target-sized logits filled with -inf (use actual logit dimension, not vocab_size)\n",
    "        target_logits = torch.full(\n",
    "            (draft_logits.shape[0], target_logit_dim),\n",
    "            float('-inf'),\n",
    "            device=draft_logits.device,\n",
    "            dtype=draft_logits.dtype\n",
    "        )\n",
    "        \n",
    "        # Copy action token logits from draft to corresponding target positions\n",
    "        # Draft action tokens: [draft_action_start, draft_vocab_size)\n",
    "        # Target action tokens: [target_action_start, target_vocab_size)\n",
    "        draft_action_logits = draft_logits[:, self.draft_action_start:self.draft_vocab_size]\n",
    "        target_logits[:, self.target_action_start:self.target_vocab_size] = draft_action_logits\n",
    "        \n",
    "        return target_logits\n",
    "    \n",
    "    # =========================================================================\n",
    "    # DEBUG: Token mapping verification methods\n",
    "    # =========================================================================\n",
    "    \n",
    "    def _get_action_bin_from_draft_token(self, draft_token_id: int) -> int:\n",
    "        \"\"\"Get action bin index from draft token ID.\"\"\"\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            return draft_token_id - self.draft_action_start\n",
    "        return -1  # Not an action token\n",
    "    \n",
    "    def _get_action_bin_from_target_token(self, target_token_id: int) -> int:\n",
    "        \"\"\"Get action bin index from target token ID.\"\"\"\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            return target_token_id - self.target_action_start\n",
    "        return -1  # Not an action token\n",
    "    \n",
    "    def _get_continuous_action_from_bin(self, bin_idx: int) -> float:\n",
    "        \"\"\"Convert action bin index to continuous action value using target's bin centers.\"\"\"\n",
    "        if 0 <= bin_idx < len(self.target.bin_centers):\n",
    "            return self.target.bin_centers[bin_idx]\n",
    "        return float('nan')\n",
    "    \n",
    "    def _debug_token_mapping(self, draft_token_id: int, target_token_id: int, prefix: str = \"\"):\n",
    "        \"\"\"Debug print showing token mapping verification.\"\"\"\n",
    "        draft_bin = self._get_action_bin_from_draft_token(draft_token_id)\n",
    "        target_bin = self._get_action_bin_from_target_token(target_token_id)\n",
    "        \n",
    "        draft_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "        target_action = self._get_continuous_action_from_bin(target_bin)\n",
    "        \n",
    "        match_status = \" MATCH\" if draft_bin == target_bin else \" MISMATCH\"\n",
    "        \n",
    "        print(f\"\\033[38;2;100;200;255m[DEBUG TOKEN MAP] {prefix}\\033[0m\")\n",
    "        print(f\"  Draft token:  {draft_token_id}  bin {draft_bin}  action {draft_action:.4f}\")\n",
    "        print(f\"  Target token: {target_token_id}  bin {target_bin}  action {target_action:.4f}\")\n",
    "        print(f\"  Bins match: {match_status}\")\n",
    "        \n",
    "        return draft_bin == target_bin\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        \"\"\"Reset statistics counters.\"\"\"\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "    \n",
    "    def _sample_token(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Sample a token from logits.\"\"\"\n",
    "        if self.temperature <= 0:\n",
    "            return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            probs = F.softmax(logits / self.temperature, dim=-1)\n",
    "            return torch.multinomial(probs.squeeze(0), num_samples=1).unsqueeze(0)\n",
    "    \n",
    "    def _get_probs(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert logits to probabilities.\"\"\"\n",
    "        if self.temperature <= 0:\n",
    "            # For greedy, use a very low temperature to approximate argmax\n",
    "            return F.softmax(logits / 0.01, dim=-1)\n",
    "        return F.softmax(logits / self.temperature, dim=-1)\n",
    "    \n",
    "    def _prepare_target_inputs(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Prepare inputs for the target (OpenVLA) model.\"\"\"\n",
    "        prompt = f\"In: What action should the robot take to {instruction.lower()}?\\nOut:\"\n",
    "        inputs = self.target_processor(prompt, image).to(self.device, dtype=torch.bfloat16)\n",
    "        \n",
    "        # IMPORTANT: Add the special empty token (29871) to input_ids if not present\n",
    "        # This is what OpenVLA's predict_action does - the action tokens come AFTER this token\n",
    "        if not torch.all(inputs[\"input_ids\"][:, -1] == 29871):\n",
    "            inputs[\"input_ids\"] = torch.cat(\n",
    "                (inputs[\"input_ids\"], torch.tensor([[29871]], device=self.device)),\n",
    "                dim=1\n",
    "            )\n",
    "            # Also extend attention mask\n",
    "            if \"attention_mask\" in inputs:\n",
    "                inputs[\"attention_mask\"] = torch.cat(\n",
    "                    (inputs[\"attention_mask\"], torch.ones((1, 1), device=self.device, dtype=inputs[\"attention_mask\"].dtype)),\n",
    "                    dim=1\n",
    "                )\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def _prepare_draft_inputs(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Prepare inputs for the draft (MiniVLA) model.\"\"\"\n",
    "        # Get prompt using draft model's prompt builder\n",
    "        prompt_builder = self.draft.get_prompt_builder()\n",
    "        prompt_builder.add_turn(role=\"human\", message=f\"What action should the robot take to {instruction.lower()}?\")\n",
    "        prompt_text = prompt_builder.get_prompt()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenizer = self.draft.llm_backbone.tokenizer\n",
    "        input_ids = tokenizer(prompt_text, truncation=True, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        \n",
    "        # Handle special token for Llama tokenizer\n",
    "        from transformers import LlamaTokenizerFast\n",
    "        if isinstance(tokenizer, LlamaTokenizerFast):\n",
    "            if not torch.all(input_ids[:, -1] == 29871):\n",
    "                input_ids = torch.cat(\n",
    "                    (input_ids, torch.tensor([[29871]], device=self.device)),\n",
    "                    dim=1\n",
    "                )\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Process image\n",
    "        image_transform = self.draft.vision_backbone.get_image_transform()\n",
    "        pixel_values = image_transform(image)\n",
    "        if isinstance(pixel_values, torch.Tensor):\n",
    "            pixel_values = pixel_values[None, ...].to(self.device)\n",
    "        elif isinstance(pixel_values, dict):\n",
    "            pixel_values = {k: v[None, ...].to(self.device) for k, v in pixel_values.items()}\n",
    "        \n",
    "        return input_ids, attention_mask, pixel_values\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict_action_speculative(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "        unnorm_key_target: str,\n",
    "    ) -> Tuple[np.ndarray, SpeculativeDecodingStats]:\n",
    "        \"\"\"\n",
    "        Generate action using speculative decoding.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image observation\n",
    "            instruction: Task instruction string\n",
    "            unnorm_key_target: Key for action un-normalization statistics of the target model\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (unnormalized action array, decoding statistics)\n",
    "        \"\"\"\n",
    "        # Reset per-call stats\n",
    "        call_stats = SpeculativeDecodingStats()\n",
    "        \n",
    "        action_dim = self.target.get_action_dim(unnorm_key_target)\n",
    "        \n",
    "        \n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Target vocab size: {self.target.vocab_size}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Target embedding size: {self.target.language_model.model.embed_tokens.weight.shape[0]}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Draft tokenizer vocab size: {self.draft.llm_backbone.tokenizer.vocab_size}\")\n",
    "        # print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Action dim: {action_dim}\")\n",
    "        \n",
    "        # Prepare inputs for both models\n",
    "        target_inputs = self._prepare_target_inputs(image, instruction)\n",
    "        draft_input_ids, draft_attention_mask, draft_pixel_values = self._prepare_draft_inputs(image, instruction)\n",
    "        \n",
    "        # Cast draft inputs to appropriate dtype\n",
    "        autocast_dtype = self.draft.llm_backbone.half_precision_dtype\n",
    "        \n",
    "        # Initialize caches\n",
    "        target_cache = None\n",
    "        draft_cache = None\n",
    "        \n",
    "        generated_token_ids = []\n",
    "        \n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16, enabled=True):\n",
    "            # === Initial forward pass to get KV cache and initial logits ===\n",
    "            # NOTE: After adding token 29871 to input, the model should output action tokens directly\n",
    "            \n",
    "            # Target model initial forward (prefill)\n",
    "            target_out = self.target(\n",
    "                **target_inputs,\n",
    "                past_key_values=None,\n",
    "                use_cache=self.use_cache,\n",
    "            )\n",
    "            target_cache = target_out.past_key_values\n",
    "            target_logits = target_out.logits[:, -1, :]\n",
    "            call_stats.total_target_forward_passes += 1\n",
    "            \n",
    "            # Calculate number of patch embeddings inserted by the VLM\n",
    "            # After multimodal forward: seq_len = original_len + num_patches\n",
    "            original_input_len = target_inputs[\"input_ids\"].shape[1]\n",
    "            seq_len_after_prefill = target_out.logits.shape[1]\n",
    "            num_patches = seq_len_after_prefill - original_input_len\n",
    "            print(f\"\\033[38;2;100;200;255m[DEBUG] VLM inserted {num_patches} patch embeddings\\033[0m\")\n",
    "            \n",
    "            # DEBUG: Check what the target model wants to output first\n",
    "            top_target_token = torch.argmax(target_logits, dim=-1).item()\n",
    "            top_target_bin = self._get_action_bin_from_target_token(top_target_token)\n",
    "            print(f\"\\033[38;2;100;200;255m[DEBUG] After prefill, target top token:\\033[0m\")\n",
    "            print(f\"  top_token={top_target_token}  bin={top_target_bin}  is_action_token={top_target_bin >= 0}\")\n",
    "            \n",
    "            # Draft model initial forward (prefill)\n",
    "            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                draft_out = self.draft(\n",
    "                    input_ids=draft_input_ids,\n",
    "                    attention_mask=draft_attention_mask,\n",
    "                    pixel_values=draft_pixel_values,\n",
    "                    past_key_values=None,\n",
    "                    use_cache=self.use_cache,\n",
    "                )\n",
    "            draft_cache = draft_out.past_key_values\n",
    "            draft_logits = draft_out.logits[:, -1, :]\n",
    "            call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            # DEBUG: Check what the draft model wants to output first\n",
    "            top_draft_token = torch.argmax(draft_logits, dim=-1).item()\n",
    "            top_draft_bin = self._get_action_bin_from_draft_token(top_draft_token)\n",
    "            print(f\"\\033[38;2;100;200;255m[DEBUG] After prefill, draft top token:\\033[0m\")\n",
    "            print(f\"  top_token={top_draft_token}  bin={top_draft_bin}  is_action_token={top_draft_bin >= 0}\")\n",
    "            \n",
    "            # === Main speculative decoding loop ===\n",
    "            # We start directly with drafting - no need to sample a first token separately\n",
    "            while len(generated_token_ids) < action_dim:\n",
    "                # Determine how many tokens to speculate\n",
    "                gamma = min(self.gamma, action_dim - len(generated_token_ids))\n",
    "                \n",
    "                # Generate gamma draft tokens\n",
    "                draft_tokens = []\n",
    "                draft_probs_list = []\n",
    "                \n",
    "                current_draft_cache = draft_cache\n",
    "                current_draft_logits = draft_logits\n",
    "                \n",
    "                for _ in range(gamma):\n",
    "                    draft_probs = self._get_probs(current_draft_logits)\n",
    "                    draft_token = self._sample_token(current_draft_logits)\n",
    "                    \n",
    "                    draft_tokens.append(draft_token)\n",
    "                    draft_probs_list.append(draft_probs)\n",
    "                    \n",
    "                    # Advance draft model\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=draft_token.to(self.device),\n",
    "                            past_key_values=current_draft_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                    current_draft_cache = draft_step.past_key_values\n",
    "                    current_draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                \n",
    "                call_stats.total_draft_tokens_proposed += gamma\n",
    "                \n",
    "                # Map draft tokens to target vocab space for verification\n",
    "                draft_token_ids_target = []\n",
    "                print(f\"\\033[38;2;100;200;255m[DEBUG] Mapping {gamma} draft tokens to target vocab:\\033[0m\")\n",
    "                for idx, dt in enumerate(draft_tokens):\n",
    "                    draft_id = dt.item()\n",
    "                    target_id = self._draft_token_to_target(draft_id)\n",
    "                    draft_token_ids_target.append(target_id)\n",
    "                    \n",
    "                    # Verify mapping preserves action bin\n",
    "                    draft_bin = self._get_action_bin_from_draft_token(draft_id)\n",
    "                    target_bin = self._get_action_bin_from_target_token(target_id)\n",
    "                    draft_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "                    target_action = self._get_continuous_action_from_bin(target_bin)\n",
    "                    match = \"\" if draft_bin == target_bin else \" MISMATCH!\"\n",
    "                    print(f\"  [{idx}] draft_tok={draft_id}  target_tok={target_id} | bin: {draft_bin}{target_bin} | action: {draft_action:.4f}{target_action:.4f} {match}\")\n",
    "                \n",
    "                # Verify with target model\n",
    "                if self.use_batched_verification:\n",
    "                    # BATCHED VERIFICATION: Single forward pass without cache\n",
    "                    # Re-processes the image but only 1 forward pass for all tokens\n",
    "                    \n",
    "                    # Build full input: original prompt + PREVIOUSLY VERIFIED tokens + new draft tokens\n",
    "                    draft_tokens_tensor = torch.tensor([draft_token_ids_target], device=self.device)  # [1, gamma]\n",
    "                    \n",
    "                    # Include previously generated tokens (if any)\n",
    "                    if len(generated_token_ids) > 0:\n",
    "                        prev_tokens_tensor = torch.tensor([generated_token_ids], device=self.device)\n",
    "                        full_input_ids = torch.cat([\n",
    "                            target_inputs[\"input_ids\"],\n",
    "                            prev_tokens_tensor,\n",
    "                            draft_tokens_tensor\n",
    "                        ], dim=1)\n",
    "                        num_prev_tokens = len(generated_token_ids)\n",
    "                    else:\n",
    "                        full_input_ids = torch.cat([\n",
    "                            target_inputs[\"input_ids\"],\n",
    "                            draft_tokens_tensor\n",
    "                        ], dim=1)\n",
    "                        num_prev_tokens = 0\n",
    "                    \n",
    "                    # Run full forward pass (no cache, with image)\n",
    "                    target_verify_out = self.target(\n",
    "                        input_ids=full_input_ids,\n",
    "                        attention_mask=torch.ones_like(full_input_ids),\n",
    "                        pixel_values=target_inputs.get(\"pixel_values\"),\n",
    "                        past_key_values=None,  # No cache!\n",
    "                        use_cache=False,\n",
    "                    )\n",
    "                    call_stats.total_target_forward_passes += 1\n",
    "                    \n",
    "                    # Extract logits for draft token positions\n",
    "                    # IMPORTANT: The VLM inserts patch embeddings after BOS, so the sequence is:\n",
    "                    # [BOS] [num_patches embeddings] [rest of prompt] [prev verified tokens] [draft tokens]\n",
    "                    # Output logits have shape [1, original_len + num_patches + num_prev + gamma, vocab]\n",
    "                    # \n",
    "                    # To evaluate draft_token[i], we need logits at position:\n",
    "                    #   original_len + num_patches + num_prev_tokens - 1 + i\n",
    "                    # (because logits at position P predict token at position P+1)\n",
    "                    \n",
    "                    start_idx = original_input_len + num_patches + num_prev_tokens - 1\n",
    "                    target_logits_batch = target_verify_out.logits[:, start_idx:start_idx+gamma, :]  # [1, gamma, vocab]\n",
    "                    target_probs_batch = self._get_probs(target_logits_batch)\n",
    "                    \n",
    "                    # Last logits for bonus token\n",
    "                    last_target_logits = target_verify_out.logits[:, -1, :]\n",
    "                    \n",
    "                    # No cache to update in batched mode\n",
    "                    target_cache_for_verify = None\n",
    "                    \n",
    "                    print(f\"\\033[38;2;100;200;255m[DEBUG] BATCHED verification: 1 forward pass for {gamma} tokens\\033[0m\")\n",
    "                    print(f\"  full_input_ids shape: {full_input_ids.shape}, output logits shape: {target_verify_out.logits.shape}\")\n",
    "                    print(f\"  num_patches={num_patches}, num_prev_tokens={num_prev_tokens}\")\n",
    "                    print(f\"  extracting logits from positions [{start_idx}, {start_idx+gamma})\")\n",
    "                    \n",
    "                else:\n",
    "                    # SEQUENTIAL VERIFICATION: One forward pass per token with KV cache\n",
    "                    # NOTE: OpenVLA/Prismatic models only support single-token inference with KV cache\n",
    "                    \n",
    "                    target_cache_for_verify = target_cache\n",
    "                    # Start with current target_logits for evaluating first draft token\n",
    "                    target_logits_for_verification = [target_logits.unsqueeze(1)]  # [1, 1, vocab]\n",
    "                    last_target_logits = None\n",
    "                    \n",
    "                    for i in range(gamma):\n",
    "                        target_token_input = torch.tensor([[draft_token_ids_target[i]]], device=self.device)\n",
    "                        target_step = self.target(\n",
    "                            input_ids=target_token_input,\n",
    "                            past_key_values=target_cache_for_verify,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                        target_cache_for_verify = target_step.past_key_values\n",
    "                        last_target_logits = target_step.logits[:, -1, :]\n",
    "                        # Store logits for evaluating the NEXT token (positions 1 to gamma-1)\n",
    "                        if i < gamma - 1:\n",
    "                            target_logits_for_verification.append(target_step.logits[:, -1:, :])\n",
    "                        call_stats.total_target_forward_passes += 1\n",
    "                    \n",
    "                    # Stack target logits - target_logits_for_verification[i] evaluates draft_token[i]\n",
    "                    target_logits_batch = torch.cat(target_logits_for_verification, dim=1)  # [1, gamma, vocab]\n",
    "                    target_probs_batch = self._get_probs(target_logits_batch)\n",
    "                \n",
    "                # DEBUG: Show which token the target actually wants at each position\n",
    "                print(f\"\\033[38;2;100;200;255m[DEBUG] Target's preferred tokens at each position:\\033[0m\")\n",
    "                for i in range(gamma):\n",
    "                    top_tok = torch.argmax(target_logits_batch[0, i, :]).item()\n",
    "                    top_bin = self._get_action_bin_from_target_token(top_tok)\n",
    "                    draft_tok = draft_token_ids_target[i]\n",
    "                    draft_bin = self._get_action_bin_from_target_token(draft_tok)\n",
    "                    p_top = target_probs_batch[0, i, top_tok].item()\n",
    "                    p_draft = target_probs_batch[0, i, draft_tok].item()\n",
    "                    print(f\"  pos[{i}]: target_wants={top_tok}(bin={top_bin}, p={p_top:.4f}) | draft_proposed={draft_tok}(bin={draft_bin}, p={p_draft:.4f})\")\n",
    "                \n",
    "                # Get actual target logit dimension from the output\n",
    "                actual_target_logit_dim = target_logits_batch.shape[-1]\n",
    "                \n",
    "                # Remap draft probs to target vocab space for comparison\n",
    "                # Use actual target logit dimension to ensure tensor size match\n",
    "                draft_probs_remapped = [self._remap_logits_draft_to_target(dp, actual_target_logit_dim) for dp in draft_probs_list]\n",
    "                draft_probs_remapped = [self._get_probs(dp) for dp in draft_probs_remapped]\n",
    "                \n",
    "                # Rejection sampling loop\n",
    "                n_accepted = 0\n",
    "                for i in range(gamma):\n",
    "                    draft_token_id_draft = draft_tokens[i].item()  # In draft vocab\n",
    "                    draft_token_id_target = draft_token_ids_target[i]  # Mapped to target vocab\n",
    "                    \n",
    "                    draft_prob_remapped = draft_probs_remapped[i]\n",
    "                    target_prob = target_probs_batch[:, i, :]\n",
    "                    \n",
    "                    # Get probability of the token under both models (in target vocab space)\n",
    "                    p_target = target_prob[0, draft_token_id_target].item()\n",
    "                    p_draft = draft_prob_remapped[0, draft_token_id_target].item()\n",
    "                    \n",
    "                    # Get the target's preferred token and compute bin distances\n",
    "                    target_preferred_token = torch.argmax(target_prob, dim=-1).item()\n",
    "                    draft_bin = self._get_action_bin_from_target_token(draft_token_id_target)\n",
    "                    target_bin = self._get_action_bin_from_target_token(target_preferred_token)\n",
    "                    \n",
    "                    # Compute bin distance (for relaxed acceptance)\n",
    "                    if draft_bin >= 0 and target_bin >= 0:\n",
    "                        bin_distance = abs(draft_bin - target_bin)\n",
    "                    else:\n",
    "                        bin_distance = float('inf')  # Non-action tokens don't benefit from relaxed acceptance\n",
    "                    \n",
    "                    # Relaxed acceptance: accept if within r bins of target's preference\n",
    "                    relaxed_accept = (self.relaxed_acceptance_r > 0 and bin_distance <= self.relaxed_acceptance_r)\n",
    "                    \n",
    "                    # Standard rejection sampling\n",
    "                    if p_draft > 0:\n",
    "                        acceptance_prob = min(1.0, p_target / p_draft)\n",
    "                    else:\n",
    "                        acceptance_prob = 1.0 if p_target > 0 else 0.0\n",
    "                    \n",
    "                    standard_accept = (torch.rand(1).item() < acceptance_prob)\n",
    "                    \n",
    "                    # Accept if either relaxed acceptance or standard acceptance passes\n",
    "                    if relaxed_accept or standard_accept:\n",
    "                        # Accept this token (store in target vocab space)\n",
    "                        accepted_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "                        accept_reason = \"RELAXED\" if relaxed_accept and not standard_accept else \"STANDARD\"\n",
    "                        print(f\"\\033[38;2;0;255;0m[ACCEPT-{accept_reason}]\\033[0m token[{i}]: target_tok={draft_token_id_target}  bin={draft_bin}  action={accepted_action:.4f}\")\n",
    "                        print(f\"  target_preferred: bin={target_bin} | bin_distance={bin_distance} | r={self.relaxed_acceptance_r}\")\n",
    "                        print(f\"  p_target={p_target:.4f}, p_draft={p_draft:.4f}, accept_prob={acceptance_prob:.4f}\")\n",
    "                        generated_token_ids.append(draft_token_id_target)\n",
    "                        n_accepted += 1\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        call_stats.total_draft_tokens_accepted += 1\n",
    "                        \n",
    "                        if len(generated_token_ids) >= action_dim:\n",
    "                            print(f\"\\033[38;2;255;165;0m[SRP] -> \\033[0m Generated {len(generated_token_ids)}/{action_dim} tokens, done.\")\n",
    "                            break\n",
    "                    else:\n",
    "                        # Reject - sample from adjusted distribution\n",
    "                        adjusted_probs = max_fn(target_prob - draft_prob_remapped)\n",
    "                        if adjusted_probs.sum() > 0:\n",
    "                            corrected_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
    "                        else:\n",
    "                            corrected_token = self._sample_token(target_prob.unsqueeze(0))\n",
    "                        \n",
    "                        corrected_token_id = int(corrected_token.item())\n",
    "                        corrected_bin = self._get_action_bin_from_target_token(corrected_token_id)\n",
    "                        rejected_action = self._get_continuous_action_from_bin(draft_bin)\n",
    "                        corrected_action = self._get_continuous_action_from_bin(corrected_bin)\n",
    "                        bin_diff = abs(draft_bin - corrected_bin) if draft_bin >= 0 and corrected_bin >= 0 else -1\n",
    "                        \n",
    "                        print(f\"\\033[38;2;255;100;100m[REJECT]\\033[0m token[{i}]: bin_distance={bin_distance} > r={self.relaxed_acceptance_r}\")\n",
    "                        print(f\"  Draft proposed: target_tok={draft_token_id_target}  bin={draft_bin}  action={rejected_action:.4f}\")\n",
    "                        print(f\"  Target prefers: target_tok={target_preferred_token}  bin={target_bin}\")\n",
    "                        print(f\"  Corrected to:   target_tok={corrected_token_id}  bin={corrected_bin}  action={corrected_action:.4f}\")\n",
    "                        print(f\"  p_target={p_target:.4f}, p_draft={p_draft:.4f}, accept_prob={acceptance_prob:.4f}\")\n",
    "                        \n",
    "                        # Store corrected token (already in target vocab space)\n",
    "                        generated_token_ids.append(corrected_token_id)\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        n_accepted = i  # Number of accepted tokens (before this rejection)\n",
    "                        break\n",
    "                                    \n",
    "                # Update caches after acceptance/rejection\n",
    "                if n_accepted == gamma and len(generated_token_ids) < action_dim:\n",
    "                    # All accepted - need to sample one more from target\n",
    "                    print(f\"\\033[38;2;0;255;0m[ALL ACCEPTED]\\033[0m All {gamma} draft tokens accepted! Sampling bonus token from target...\")\n",
    "                    if not self.use_batched_verification:\n",
    "                        target_cache = target_cache_for_verify\n",
    "                    target_logits = last_target_logits  # Logits after feeding all gamma tokens\n",
    "                    \n",
    "                    # Sample additional token from target (in target vocab space)\n",
    "                    bonus_token_target = self._sample_token(target_logits)\n",
    "                    bonus_token_id_target = int(bonus_token_target.item())\n",
    "                    bonus_bin = self._get_action_bin_from_target_token(bonus_token_id_target)\n",
    "                    bonus_action = self._get_continuous_action_from_bin(bonus_bin)\n",
    "                    print(f\"  Bonus token: target_tok={bonus_token_id_target}  bin={bonus_bin}  action={bonus_action:.4f}\")\n",
    "                    generated_token_ids.append(bonus_token_id_target)\n",
    "                    call_stats.total_tokens_generated += 1\n",
    "                    \n",
    "                    if self.use_batched_verification:\n",
    "                        # In batched mode, rebuild context for next round\n",
    "                        # Actually, if all tokens were accepted and we generated a bonus token,\n",
    "                        # we need to rebuild context with all tokens for the next speculation round\n",
    "                        if len(generated_token_ids) < action_dim:\n",
    "                            generated_tokens_tensor = torch.tensor([generated_token_ids], device=self.device)\n",
    "                            full_input_ids = torch.cat([\n",
    "                                target_inputs[\"input_ids\"],\n",
    "                                generated_tokens_tensor\n",
    "                            ], dim=1)\n",
    "                            \n",
    "                            target_step = self.target(\n",
    "                                input_ids=full_input_ids,\n",
    "                                attention_mask=torch.ones_like(full_input_ids),\n",
    "                                pixel_values=target_inputs.get(\"pixel_values\"),\n",
    "                                past_key_values=None,\n",
    "                                use_cache=False,\n",
    "                            )\n",
    "                            target_logits = target_step.logits[:, -1, :]\n",
    "                            call_stats.total_target_forward_passes += 1\n",
    "                            \n",
    "                            # Same for draft\n",
    "                            generated_tokens_draft = [self._target_token_to_draft(t) for t in generated_token_ids]\n",
    "                            generated_tokens_draft_tensor = torch.tensor([generated_tokens_draft], device=self.device)\n",
    "                            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                                draft_full_ids = torch.cat([draft_input_ids, generated_tokens_draft_tensor], dim=1)\n",
    "                                draft_step = self.draft(\n",
    "                                    input_ids=draft_full_ids,\n",
    "                                    attention_mask=torch.ones_like(draft_full_ids),\n",
    "                                    pixel_values=draft_pixel_values,\n",
    "                                    past_key_values=None,\n",
    "                                    use_cache=False,\n",
    "                                )\n",
    "                            draft_logits = draft_step.logits[:, -1, :]\n",
    "                            call_stats.total_draft_forward_passes += 1\n",
    "                    else:\n",
    "                        # Update target cache\n",
    "                        target_step = self.target(\n",
    "                            input_ids=bonus_token_target,\n",
    "                            past_key_values=target_cache,\n",
    "                            use_cache=self.use_cache,\n",
    "                        )\n",
    "                        target_cache = target_step.past_key_values\n",
    "                        target_logits = target_step.logits[:, -1, :]\n",
    "                        call_stats.total_target_forward_passes += 1\n",
    "                        \n",
    "                        # Map bonus token to draft vocab and update draft cache\n",
    "                        bonus_token_id_draft = self._target_token_to_draft(bonus_token_id_target)\n",
    "                        bonus_draft_bin = self._get_action_bin_from_draft_token(bonus_token_id_draft)\n",
    "                        print(f\"  Mapped to draft: draft_tok={bonus_token_id_draft}  bin={bonus_draft_bin} {'' if bonus_bin == bonus_draft_bin else ' MISMATCH!'}\")\n",
    "                        bonus_token_draft = torch.tensor([[bonus_token_id_draft]], device=self.device)\n",
    "                        \n",
    "                        draft_cache = current_draft_cache\n",
    "                        with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                            draft_step = self.draft(\n",
    "                                input_ids=bonus_token_draft,\n",
    "                                past_key_values=draft_cache,\n",
    "                                use_cache=self.use_cache,\n",
    "                            )\n",
    "                        draft_cache = draft_step.past_key_values\n",
    "                        draft_logits = draft_step.logits[:, -1, :]\n",
    "                        call_stats.total_draft_forward_passes += 1\n",
    "                    \n",
    "                else:\n",
    "                    # Some tokens rejected\n",
    "                    if self.use_batched_verification:\n",
    "                        # In batched mode, we don't have caches\n",
    "                        # For next round, we'll rebuild everything from scratch\n",
    "                        # We just need to get the logits for the next draft round\n",
    "                        if len(generated_token_ids) < action_dim:\n",
    "                            # Rebuild full context with all generated tokens so far\n",
    "                            generated_tokens_tensor = torch.tensor([generated_token_ids], device=self.device)\n",
    "                            full_input_ids = torch.cat([\n",
    "                                target_inputs[\"input_ids\"],\n",
    "                                generated_tokens_tensor\n",
    "                            ], dim=1)\n",
    "                            \n",
    "                            # Run forward pass to get logits for next position\n",
    "                            target_step = self.target(\n",
    "                                input_ids=full_input_ids,\n",
    "                                attention_mask=torch.ones_like(full_input_ids),\n",
    "                                pixel_values=target_inputs.get(\"pixel_values\"),\n",
    "                                past_key_values=None,\n",
    "                                use_cache=False,\n",
    "                            )\n",
    "                            target_logits = target_step.logits[:, -1, :]\n",
    "                            call_stats.total_target_forward_passes += 1\n",
    "                            \n",
    "                            # Same for draft model\n",
    "                            generated_tokens_draft = [self._target_token_to_draft(t) for t in generated_token_ids]\n",
    "                            generated_tokens_draft_tensor = torch.tensor([generated_tokens_draft], device=self.device)\n",
    "                            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                                # Need to rebuild draft input too\n",
    "                                draft_full_ids = torch.cat([draft_input_ids, generated_tokens_draft_tensor], dim=1)\n",
    "                                draft_step = self.draft(\n",
    "                                    input_ids=draft_full_ids,\n",
    "                                    attention_mask=torch.ones_like(draft_full_ids),\n",
    "                                    pixel_values=draft_pixel_values,\n",
    "                                    past_key_values=None,\n",
    "                                    use_cache=False,\n",
    "                                )\n",
    "                            draft_logits = draft_step.logits[:, -1, :]\n",
    "                            call_stats.total_draft_forward_passes += 1\n",
    "                    else:\n",
    "                        # Sequential mode - prune caches\n",
    "                        tokens_to_discard = gamma - n_accepted\n",
    "                        if tokens_to_discard > 0 and self.use_cache:\n",
    "                            # We need to prune and resync\n",
    "                            # Use the cache state after the accepted tokens\n",
    "                            target_cache = target_cache_for_verify\n",
    "                            if tokens_to_discard > 0:\n",
    "                                target_cache = prune_cache(target_cache, tokens_to_discard)\n",
    "                            \n",
    "                            # Rebuild draft cache\n",
    "                            draft_cache = prune_cache(current_draft_cache, gamma - n_accepted)\n",
    "                        \n",
    "                        # Get logits for next round\n",
    "                        if len(generated_token_ids) < action_dim:\n",
    "                            # Last token is in target vocab space\n",
    "                            last_token_id_target = generated_token_ids[-1]\n",
    "                            last_token_target = torch.tensor([[last_token_id_target]], device=self.device)\n",
    "                            \n",
    "                            target_step = self.target(\n",
    "                                input_ids=last_token_target,\n",
    "                                past_key_values=target_cache,\n",
    "                                use_cache=self.use_cache,\n",
    "                            )\n",
    "                            target_cache = target_step.past_key_values\n",
    "                            target_logits = target_step.logits[:, -1, :]\n",
    "                            call_stats.total_target_forward_passes += 1\n",
    "                            \n",
    "                            # Map to draft vocab for draft model\n",
    "                            last_token_id_draft = self._target_token_to_draft(last_token_id_target)\n",
    "                            last_token_draft = torch.tensor([[last_token_id_draft]], device=self.device)\n",
    "                            \n",
    "                            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                                draft_step = self.draft(\n",
    "                                    input_ids=last_token_draft,\n",
    "                                    past_key_values=draft_cache,\n",
    "                                    use_cache=self.use_cache,\n",
    "                                )\n",
    "                            draft_cache = draft_step.past_key_values\n",
    "                            draft_logits = draft_step.logits[:, -1, :]\n",
    "                            call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Call stats: {call_stats}\")\n",
    "            \n",
    "        # Decode tokens to actions\n",
    "        predicted_action_token_ids = np.array(generated_token_ids[:action_dim], dtype=np.int64)\n",
    "        \n",
    "        # DEBUG: Show all generated tokens with their action values\n",
    "        print(f\"\\033[38;2;100;200;255m[DEBUG] Final generated tokens summary:\\033[0m\")\n",
    "        for dim_idx, tok_id in enumerate(predicted_action_token_ids):\n",
    "            bin_idx = self._get_action_bin_from_target_token(int(tok_id))\n",
    "            action_val = self._get_continuous_action_from_bin(bin_idx)\n",
    "            print(f\"  dim[{dim_idx}]: target_tok={tok_id}  bin={bin_idx}  normalized_action={action_val:.4f}\")\n",
    "        \n",
    "        print(\"\\033[38;2;255;165;0m[SRP] -> \\033[0m\", f\"[DEBUG] Predicted action token ids: {predicted_action_token_ids}\")\n",
    "        \n",
    "        # Use target model's decoding (vocab_size - token_id approach)\n",
    "        vocab_size = self.target.vocab_size\n",
    "        discretized_actions = vocab_size - predicted_action_token_ids\n",
    "        discretized_actions = np.clip(discretized_actions - 1, a_min=0, a_max=self.target.bin_centers.shape[0] - 1)\n",
    "        normalized_actions = self.target.bin_centers[discretized_actions]\n",
    "        \n",
    "        # Un-normalize actions\n",
    "        action_norm_stats = self.target.get_action_stats(unnorm_key_target)\n",
    "        mask = action_norm_stats.get(\"mask\", np.ones_like(action_norm_stats[\"q01\"], dtype=bool))\n",
    "        action_high, action_low = np.array(action_norm_stats[\"q99\"]), np.array(action_norm_stats[\"q01\"])\n",
    "        actions = np.where(\n",
    "            mask,\n",
    "            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,\n",
    "            normalized_actions,\n",
    "        )\n",
    "        \n",
    "        # Update global stats\n",
    "        self.stats.total_tokens_generated += call_stats.total_tokens_generated\n",
    "        self.stats.total_draft_tokens_proposed += call_stats.total_draft_tokens_proposed\n",
    "        self.stats.total_draft_tokens_accepted += call_stats.total_draft_tokens_accepted\n",
    "        self.stats.total_target_forward_passes += call_stats.total_target_forward_passes\n",
    "        self.stats.total_draft_forward_passes += call_stats.total_draft_forward_passes\n",
    "        \n",
    "        return actions, call_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VLASpeculativeDecoderBatchedLM: Uses language model directly for batched verification\n",
    "# Key insight from SpecVLA: After multimodal prefill, call language_model directly\n",
    "# with inputs_embeds and cached KV, bypassing the restrictive multimodal forward.\n",
    "# =============================================================================\n",
    "\n",
    "class VLASpeculativeDecoderBatchedLM:\n",
    "    \"\"\"\n",
    "    Speculative decoding for VLA models with EFFICIENT batched verification.\n",
    "    \n",
    "    Key difference from VLASpeculativeDecoderDDDRKVB:\n",
    "    - Initial prefill: Full multimodal forward (processes image once)\n",
    "    - Verification: Calls language_model DIRECTLY with embeddings + cached KV\n",
    "    \n",
    "    Why 10 forward passes for 7 tokens?\n",
    "    1 prefill\n",
    "    4 rejection rounds * (1 verify + 1 advance) = 8\n",
    "    1 acceptance round * 1 verify = 1\n",
    "    Total = 10\n",
    "    The problem is: after each rejection, we do 2 target forward passes:\n",
    "    Batched verification (wasted - we only needed position 0's logits)\n",
    "    Advance with corrected token (to get logits for next round)\n",
    "    - Early rejection: Skips batched verification if first draft token will definitely be rejected\n",
    "    \n",
    "    This bypasses the Prismatic multimodal forward restriction that only allows\n",
    "    single-token inference with KV cache, enabling true batched verification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model,\n",
    "        draft_model,\n",
    "        target_processor=None,\n",
    "        gamma: int = 4,\n",
    "        temperature: float = 0.0,\n",
    "        n_action_bins: int = 256,\n",
    "        relaxed_acceptance_r: int = 0,\n",
    "    ):\n",
    "        self.target = target_model\n",
    "        self.draft = draft_model\n",
    "        self.target_processor = target_processor\n",
    "        self.gamma = gamma\n",
    "        self.temperature = temperature\n",
    "        self.n_action_bins = n_action_bins\n",
    "        self.relaxed_acceptance_r = relaxed_acceptance_r\n",
    "        \n",
    "        self.device = next(target_model.parameters()).device\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "        self._setup_token_mapping()\n",
    "    \n",
    "    def _setup_token_mapping(self):\n",
    "        \"\"\"Setup token mapping between draft and target vocabularies.\"\"\"\n",
    "        # Target model\n",
    "        if hasattr(self.target, 'language_model') and hasattr(self.target.language_model, 'model'):\n",
    "            self.target_logit_dim = self.target.language_model.model.embed_tokens.weight.shape[0]\n",
    "        elif hasattr(self.target, 'get_output_embeddings'):\n",
    "            self.target_logit_dim = self.target.get_output_embeddings().weight.shape[0]\n",
    "        else:\n",
    "            self.target_logit_dim = self.target.config.vocab_size\n",
    "        \n",
    "        if hasattr(self.target, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.vocab_size\n",
    "        elif hasattr(self.target, 'config') and hasattr(self.target.config, 'vocab_size'):\n",
    "            self.target_vocab_size = self.target.config.vocab_size\n",
    "        else:\n",
    "            self.target_vocab_size = self.target_logit_dim\n",
    "        \n",
    "        # Draft model\n",
    "        if hasattr(self.draft, 'llm_backbone'):\n",
    "            draft_tokenizer = self.draft.llm_backbone.tokenizer\n",
    "            self.draft_vocab_size = len(draft_tokenizer) if hasattr(draft_tokenizer, '__len__') else draft_tokenizer.vocab_size\n",
    "            if hasattr(self.draft.llm_backbone, 'llm') and hasattr(self.draft.llm_backbone.llm, 'lm_head'):\n",
    "                self.draft_logit_dim = self.draft.llm_backbone.llm.lm_head.weight.shape[0]\n",
    "            else:\n",
    "                self.draft_logit_dim = self.draft_vocab_size\n",
    "        else:\n",
    "            self.draft_vocab_size = self.draft.config.vocab_size\n",
    "            self.draft_logit_dim = self.draft_vocab_size\n",
    "        \n",
    "        self.vocab_compatible = (self.target_logit_dim == self.draft_logit_dim)\n",
    "        \n",
    "        # Action token ranges\n",
    "        self.target_action_start = self.target_vocab_size - self.n_action_bins\n",
    "        self.draft_action_start = self.draft_vocab_size - self.n_action_bins\n",
    "        \n",
    "        print(\"\\033[38;2;255;165;0m[BatchedLM] -> \\033[0m\", f\"Target vocab_size: {self.target_vocab_size}, logit_dim: {self.target_logit_dim}\")\n",
    "        print(\"\\033[38;2;255;165;0m[BatchedLM] -> \\033[0m\", f\"Draft vocab_size: {self.draft_vocab_size}, logit_dim: {self.draft_logit_dim}\")\n",
    "        print(\"\\033[38;2;255;165;0m[BatchedLM] -> \\033[0m\", f\"Vocabularies compatible: {self.vocab_compatible}\")\n",
    "    \n",
    "    def _draft_token_to_target(self, draft_token_id: int) -> int:\n",
    "        if self.vocab_compatible:\n",
    "            return draft_token_id\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            action_bin = draft_token_id - self.draft_action_start\n",
    "            return self.target_action_start + action_bin\n",
    "        return min(draft_token_id, self.target_vocab_size - 1)\n",
    "    \n",
    "    def _target_token_to_draft(self, target_token_id: int) -> int:\n",
    "        if self.vocab_compatible:\n",
    "            return target_token_id\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            action_bin = target_token_id - self.target_action_start\n",
    "            return self.draft_action_start + action_bin\n",
    "        return min(target_token_id, self.draft_vocab_size - 1)\n",
    "    \n",
    "    def _remap_logits_draft_to_target(self, draft_logits: torch.Tensor, target_logit_dim: int = None) -> torch.Tensor:\n",
    "        if self.vocab_compatible:\n",
    "            return draft_logits\n",
    "        if target_logit_dim is None:\n",
    "            target_logit_dim = self.target_logit_dim\n",
    "        target_logits = torch.full(\n",
    "            (draft_logits.shape[0], target_logit_dim), float('-inf'),\n",
    "            device=draft_logits.device, dtype=draft_logits.dtype\n",
    "        )\n",
    "        draft_action_logits = draft_logits[:, self.draft_action_start:self.draft_vocab_size]\n",
    "        target_logits[:, self.target_action_start:self.target_vocab_size] = draft_action_logits\n",
    "        return target_logits\n",
    "    \n",
    "    def _get_action_bin_from_draft_token(self, draft_token_id: int) -> int:\n",
    "        if draft_token_id >= self.draft_action_start:\n",
    "            return draft_token_id - self.draft_action_start\n",
    "        return -1\n",
    "    \n",
    "    def _get_action_bin_from_target_token(self, target_token_id: int) -> int:\n",
    "        if target_token_id >= self.target_action_start:\n",
    "            return target_token_id - self.target_action_start\n",
    "        return -1\n",
    "    \n",
    "    def _get_continuous_action_from_bin(self, bin_idx: int) -> float:\n",
    "        if 0 <= bin_idx < len(self.target.bin_centers):\n",
    "            return self.target.bin_centers[bin_idx]\n",
    "        return float('nan')\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        self.stats = SpeculativeDecodingStats()\n",
    "    \n",
    "    def _sample_token(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        if self.temperature <= 0:\n",
    "            return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        probs = F.softmax(logits / self.temperature, dim=-1)\n",
    "        return torch.multinomial(probs.squeeze(0), num_samples=1).unsqueeze(0)\n",
    "    \n",
    "    def _get_probs(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        if self.temperature <= 0:\n",
    "            return F.softmax(logits / 0.01, dim=-1)\n",
    "        return F.softmax(logits / self.temperature, dim=-1)\n",
    "    \n",
    "    def _prepare_target_inputs(self, image: Image.Image, instruction: str) -> Dict[str, torch.Tensor]:\n",
    "        prompt = f\"In: What action should the robot take to {instruction.lower()}?\\nOut:\"\n",
    "        inputs = self.target_processor(prompt, image).to(self.device, dtype=torch.bfloat16)\n",
    "        # Add special token 29871 if not present\n",
    "        if not torch.all(inputs[\"input_ids\"][:, -1] == 29871):\n",
    "            inputs[\"input_ids\"] = torch.cat(\n",
    "                (inputs[\"input_ids\"], torch.tensor([[29871]], device=self.device)), dim=1\n",
    "            )\n",
    "            if \"attention_mask\" in inputs:\n",
    "                inputs[\"attention_mask\"] = torch.cat(\n",
    "                    (inputs[\"attention_mask\"], torch.ones((1, 1), device=self.device, dtype=inputs[\"attention_mask\"].dtype)), dim=1\n",
    "                )\n",
    "        return inputs\n",
    "    \n",
    "    def _prepare_draft_inputs(self, image: Image.Image, instruction: str) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        prompt_builder = self.draft.get_prompt_builder()\n",
    "        prompt_builder.add_turn(role=\"human\", message=f\"What action should the robot take to {instruction.lower()}?\")\n",
    "        prompt_text = prompt_builder.get_prompt()\n",
    "        \n",
    "        tokenizer = self.draft.llm_backbone.tokenizer\n",
    "        input_ids = tokenizer(prompt_text, truncation=True, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        \n",
    "        from transformers import LlamaTokenizerFast\n",
    "        if isinstance(tokenizer, LlamaTokenizerFast):\n",
    "            if not torch.all(input_ids[:, -1] == 29871):\n",
    "                input_ids = torch.cat((input_ids, torch.tensor([[29871]], device=self.device)), dim=1)\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        image_transform = self.draft.vision_backbone.get_image_transform()\n",
    "        pixel_values = image_transform(image)\n",
    "        if isinstance(pixel_values, torch.Tensor):\n",
    "            pixel_values = pixel_values[None, ...].to(self.device)\n",
    "        elif isinstance(pixel_values, dict):\n",
    "            pixel_values = {k: v[None, ...].to(self.device) for k, v in pixel_values.items()}\n",
    "        \n",
    "        return input_ids, attention_mask, pixel_values\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict_action_speculative(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        instruction: str,\n",
    "        unnorm_key_target: str,\n",
    "    ) -> Tuple[np.ndarray, SpeculativeDecodingStats]:\n",
    "        \"\"\"\n",
    "        Generate action using speculative decoding with efficient batched verification.\n",
    "        \n",
    "        The key innovation: After multimodal prefill, we call the language model DIRECTLY\n",
    "        with embeddings and cached KV, bypassing the restrictive multimodal forward.\n",
    "        \"\"\"\n",
    "        call_stats = SpeculativeDecodingStats()\n",
    "        action_dim = self.target.get_action_dim(unnorm_key_target)\n",
    "        \n",
    "        # Prepare inputs\n",
    "        target_inputs = self._prepare_target_inputs(image, instruction)\n",
    "        draft_input_ids, draft_attention_mask, draft_pixel_values = self._prepare_draft_inputs(image, instruction)\n",
    "        autocast_dtype = self.draft.llm_backbone.half_precision_dtype\n",
    "        \n",
    "        generated_token_ids = []\n",
    "        \n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16, enabled=True):\n",
    "            # === PHASE 1: Multimodal Prefill (processes image ONCE) ===\n",
    "            print(\"\\033[38;2;100;200;255m[BatchedLM] Phase 1: Multimodal Prefill\\033[0m\")\n",
    "            \n",
    "            # Target prefill - get KV cache with image embeddings\n",
    "            target_out = self.target(\n",
    "                **target_inputs,\n",
    "                past_key_values=None,\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "            target_cache = target_out.past_key_values\n",
    "            target_logits = target_out.logits[:, -1, :]\n",
    "            call_stats.total_target_forward_passes += 1\n",
    "            \n",
    "            # Get the sequence length after multimodal prefill (includes patch embeddings)\n",
    "            kv_seq_len = target_cache[0][0].shape[2]  # [batch, heads, seq_len, head_dim]\n",
    "            print(f\"  Target KV cache seq_len after prefill: {kv_seq_len}\")\n",
    "            \n",
    "            # Draft prefill\n",
    "            with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                draft_out = self.draft(\n",
    "                    input_ids=draft_input_ids,\n",
    "                    attention_mask=draft_attention_mask,\n",
    "                    pixel_values=draft_pixel_values,\n",
    "                    past_key_values=None,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "            draft_cache = draft_out.past_key_values\n",
    "            draft_logits = draft_out.logits[:, -1, :]\n",
    "            call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            # Get embed_tokens function for converting tokens to embeddings\n",
    "            embed_tokens = self.target.language_model.model.embed_tokens\n",
    "            \n",
    "            # === PHASE 2: Speculative Decoding Loop ===\n",
    "            print(\"\\033[38;2;100;200;255m[BatchedLM] Phase 2: Speculative Decoding\\033[0m\")\n",
    "            \n",
    "            while len(generated_token_ids) < action_dim:\n",
    "                gamma = min(self.gamma, action_dim - len(generated_token_ids))\n",
    "                \n",
    "                # Generate FIRST draft token to check for early rejection\n",
    "                draft_probs_first = self._get_probs(draft_logits)\n",
    "                draft_token_first = self._sample_token(draft_logits)\n",
    "                draft_token_first_target = self._draft_token_to_target(draft_token_first.item())\n",
    "                \n",
    "                # EARLY REJECTION CHECK: Can we determine rejection without batched verification?\n",
    "                # For greedy decoding (temperature=0), we can check bin distance directly\n",
    "                target_preferred = torch.argmax(target_logits, dim=-1).item()\n",
    "                first_draft_bin = self._get_action_bin_from_target_token(draft_token_first_target)\n",
    "                target_preferred_bin = self._get_action_bin_from_target_token(target_preferred)\n",
    "                first_bin_distance = abs(first_draft_bin - target_preferred_bin) if first_draft_bin >= 0 and target_preferred_bin >= 0 else float('inf')\n",
    "                \n",
    "                # Check if first token would be accepted (relaxed or standard greedy)\n",
    "                p_target_first = self._get_probs(target_logits)[0, draft_token_first_target].item()\n",
    "                p_draft_first = draft_probs_first[0, draft_token_first_target].item()\n",
    "                acceptance_prob_first = min(1.0, p_target_first / p_draft_first) if p_draft_first > 0 else (1.0 if p_target_first > 0 else 0.0)\n",
    "                \n",
    "                first_relaxed_accept = (self.relaxed_acceptance_r > 0 and first_bin_distance <= self.relaxed_acceptance_r)\n",
    "                first_standard_accept = (torch.rand(1).item() < acceptance_prob_first)\n",
    "                first_will_accept = first_relaxed_accept or first_standard_accept\n",
    "                \n",
    "                if not first_will_accept:\n",
    "                    # EARLY REJECTION: Skip batched verification entirely!\n",
    "                    # We already have target_logits, so just sample corrected token from it\n",
    "                    call_stats.total_draft_tokens_proposed += 1\n",
    "                    call_stats.total_draft_forward_passes += 1  # We did generate first draft token\n",
    "                    \n",
    "                    # Sample corrected token from target's distribution\n",
    "                    draft_probs_first_remapped = self._get_probs(self._remap_logits_draft_to_target(draft_probs_first, target_logits.shape[-1]))\n",
    "                    target_probs = self._get_probs(target_logits)\n",
    "                    adjusted_probs = max_fn(target_probs - draft_probs_first_remapped)\n",
    "                    if adjusted_probs.sum() > 0:\n",
    "                        corrected_token = torch.multinomial(adjusted_probs, num_samples=1).item()\n",
    "                    else:\n",
    "                        corrected_token = target_preferred  # Use argmax\n",
    "                    \n",
    "                    corrected_bin = self._get_action_bin_from_target_token(corrected_token)\n",
    "                    print(f\"\\033[38;2;255;200;100m[EARLY REJECT]\\033[0m draft_bin={first_draft_bin}, target_bin={target_preferred_bin}, corrected_bin={corrected_bin} (saved verify pass!)\")\n",
    "                    \n",
    "                    generated_token_ids.append(corrected_token)\n",
    "                    call_stats.total_tokens_generated += 1\n",
    "                    \n",
    "                    # Advance target with corrected token (1 forward pass - this is necessary)\n",
    "                    if len(generated_token_ids) < action_dim:\n",
    "                        corrected_embeds = embed_tokens(torch.tensor([[corrected_token]], device=self.device))\n",
    "                        corrected_pos = torch.tensor([[target_cache[0][0].shape[2]]], device=self.device)\n",
    "                        lm_step = self.target.language_model(\n",
    "                            inputs_embeds=corrected_embeds,\n",
    "                            past_key_values=target_cache,\n",
    "                            position_ids=corrected_pos,\n",
    "                            use_cache=True,\n",
    "                        )\n",
    "                        target_cache = lm_step.past_key_values\n",
    "                        target_logits = lm_step.logits[:, -1, :]\n",
    "                        call_stats.total_target_forward_passes += 1\n",
    "                        \n",
    "                        # Update draft with corrected token (in draft vocab)\n",
    "                        corrected_draft = self._target_token_to_draft(corrected_token)\n",
    "                        with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                            draft_step = self.draft(\n",
    "                                input_ids=torch.tensor([[corrected_draft]], device=self.device),\n",
    "                                past_key_values=draft_cache,\n",
    "                                use_cache=True,\n",
    "                            )\n",
    "                        draft_cache = draft_step.past_key_values\n",
    "                        draft_logits = draft_step.logits[:, -1, :]\n",
    "                        call_stats.total_draft_forward_passes += 1\n",
    "                    \n",
    "                    continue  # Next iteration\n",
    "                \n",
    "                # First token looks good - generate remaining draft tokens\n",
    "                draft_tokens = [draft_token_first]\n",
    "                draft_probs_list = [draft_probs_first]\n",
    "                current_draft_cache = draft_cache\n",
    "                \n",
    "                # Advance draft with first token\n",
    "                with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                    draft_step = self.draft(\n",
    "                        input_ids=draft_token_first.to(self.device),\n",
    "                        past_key_values=current_draft_cache,\n",
    "                        use_cache=True,\n",
    "                    )\n",
    "                current_draft_cache = draft_step.past_key_values\n",
    "                current_draft_logits = draft_step.logits[:, -1, :]\n",
    "                call_stats.total_draft_forward_passes += 1\n",
    "                \n",
    "                # Generate remaining gamma-1 draft tokens\n",
    "                for _ in range(gamma - 1):\n",
    "                    draft_probs = self._get_probs(current_draft_logits)\n",
    "                    draft_token = self._sample_token(current_draft_logits)\n",
    "                    draft_tokens.append(draft_token)\n",
    "                    draft_probs_list.append(draft_probs)\n",
    "                    \n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=draft_token.to(self.device),\n",
    "                            past_key_values=current_draft_cache,\n",
    "                            use_cache=True,\n",
    "                        )\n",
    "                    current_draft_cache = draft_step.past_key_values\n",
    "                    current_draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                \n",
    "                call_stats.total_draft_tokens_proposed += gamma\n",
    "                \n",
    "                # Map draft tokens to target vocab\n",
    "                draft_token_ids_target = [self._draft_token_to_target(dt.item()) for dt in draft_tokens]\n",
    "                \n",
    "                # === BATCHED VERIFICATION via Language Model ===\n",
    "                draft_tokens_tensor = torch.tensor([draft_token_ids_target], device=self.device)\n",
    "                draft_embeds = embed_tokens(draft_tokens_tensor)\n",
    "                \n",
    "                current_kv_len = target_cache[0][0].shape[2]\n",
    "                position_ids = torch.arange(current_kv_len, current_kv_len + gamma, device=self.device).unsqueeze(0)\n",
    "                \n",
    "                lm_out = self.target.language_model(\n",
    "                    inputs_embeds=draft_embeds,\n",
    "                    past_key_values=target_cache,\n",
    "                    position_ids=position_ids,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "                call_stats.total_target_forward_passes += 1\n",
    "                \n",
    "                target_logits_batch = lm_out.logits\n",
    "                new_target_cache = lm_out.past_key_values\n",
    "                \n",
    "                # eval_logits[i] evaluates draft_token[i]\n",
    "                eval_logits = torch.cat([target_logits.unsqueeze(1), target_logits_batch[:, :-1, :]], dim=1)\n",
    "                target_probs_batch = self._get_probs(eval_logits)\n",
    "                last_target_logits = target_logits_batch[:, -1, :]\n",
    "                \n",
    "                print(f\"\\033[38;2;100;200;255m[BatchedLM] Batched verification: 1 forward pass for {gamma} tokens\\033[0m\")\n",
    "                \n",
    "                actual_target_logit_dim = target_probs_batch.shape[-1]\n",
    "                draft_probs_remapped = [self._get_probs(self._remap_logits_draft_to_target(dp, actual_target_logit_dim)) for dp in draft_probs_list]\n",
    "                \n",
    "                # Rejection sampling (first token already checked, but re-verify for consistency)\n",
    "                n_accepted = 0\n",
    "                \n",
    "                for i in range(gamma):\n",
    "                    draft_token_id_target = draft_token_ids_target[i]\n",
    "                    draft_prob_remapped = draft_probs_remapped[i]\n",
    "                    target_prob = target_probs_batch[:, i, :]\n",
    "                    \n",
    "                    p_target = target_prob[0, draft_token_id_target].item()\n",
    "                    p_draft = draft_prob_remapped[0, draft_token_id_target].item()\n",
    "                    \n",
    "                    target_preferred = torch.argmax(target_prob, dim=-1).item()\n",
    "                    draft_bin = self._get_action_bin_from_target_token(draft_token_id_target)\n",
    "                    target_bin = self._get_action_bin_from_target_token(target_preferred)\n",
    "                    \n",
    "                    bin_distance = abs(draft_bin - target_bin) if draft_bin >= 0 and target_bin >= 0 else float('inf')\n",
    "                    relaxed_accept = (self.relaxed_acceptance_r > 0 and bin_distance <= self.relaxed_acceptance_r)\n",
    "                    \n",
    "                    acceptance_prob = min(1.0, p_target / p_draft) if p_draft > 0 else (1.0 if p_target > 0 else 0.0)\n",
    "                    standard_accept = (torch.rand(1).item() < acceptance_prob)\n",
    "                    \n",
    "                    if relaxed_accept or standard_accept:\n",
    "                        generated_token_ids.append(draft_token_id_target)\n",
    "                        n_accepted += 1\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        call_stats.total_draft_tokens_accepted += 1\n",
    "                        \n",
    "                        accept_reason = \"RELAXED\" if relaxed_accept and not standard_accept else \"STANDARD\"\n",
    "                        print(f\"\\033[38;2;0;255;0m[ACCEPT-{accept_reason}]\\033[0m [{i}] bin={draft_bin}, target_bin={target_bin}, dist={bin_distance}\")\n",
    "                        \n",
    "                        if len(generated_token_ids) >= action_dim:\n",
    "                            break\n",
    "                    else:\n",
    "                        adjusted_probs = max_fn(target_prob - draft_prob_remapped)\n",
    "                        corrected_token = torch.multinomial(adjusted_probs, num_samples=1).item() if adjusted_probs.sum() > 0 else target_preferred\n",
    "                        \n",
    "                        corrected_bin = self._get_action_bin_from_target_token(corrected_token)\n",
    "                        print(f\"\\033[38;2;255;100;100m[REJECT]\\033[0m [{i}] draft_bin={draft_bin}, target_bin={target_bin}, corrected_bin={corrected_bin}\")\n",
    "                        \n",
    "                        generated_token_ids.append(corrected_token)\n",
    "                        call_stats.total_tokens_generated += 1\n",
    "                        break\n",
    "                \n",
    "                # Update caches based on acceptance\n",
    "                if n_accepted == gamma and len(generated_token_ids) < action_dim:\n",
    "                    # All accepted - sample bonus token\n",
    "                    target_cache = new_target_cache\n",
    "                    target_logits = last_target_logits\n",
    "                    \n",
    "                    bonus_token = self._sample_token(target_logits).item()\n",
    "                    bonus_bin = self._get_action_bin_from_target_token(bonus_token)\n",
    "                    print(f\"\\033[38;2;0;255;0m[ALL ACCEPTED]\\033[0m Bonus token: bin={bonus_bin}\")\n",
    "                    generated_token_ids.append(bonus_token)\n",
    "                    call_stats.total_tokens_generated += 1\n",
    "                    \n",
    "                    # Advance caches\n",
    "                    bonus_embeds = embed_tokens(torch.tensor([[bonus_token]], device=self.device))\n",
    "                    bonus_pos = torch.tensor([[target_cache[0][0].shape[2]]], device=self.device)\n",
    "                    lm_step = self.target.language_model(\n",
    "                        inputs_embeds=bonus_embeds,\n",
    "                        past_key_values=target_cache,\n",
    "                        position_ids=bonus_pos,\n",
    "                        use_cache=True,\n",
    "                    )\n",
    "                    target_cache = lm_step.past_key_values\n",
    "                    target_logits = lm_step.logits[:, -1, :]\n",
    "                    call_stats.total_target_forward_passes += 1\n",
    "                    \n",
    "                    draft_cache = current_draft_cache\n",
    "                    bonus_draft = self._target_token_to_draft(bonus_token)\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=torch.tensor([[bonus_draft]], device=self.device),\n",
    "                            past_key_values=draft_cache,\n",
    "                            use_cache=True,\n",
    "                        )\n",
    "                    draft_cache = draft_step.past_key_values\n",
    "                    draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "                    \n",
    "                elif len(generated_token_ids) < action_dim:\n",
    "                    # Some rejected - update caches\n",
    "                    if n_accepted > 0:\n",
    "                        tokens_to_discard = gamma - n_accepted\n",
    "                        target_cache = prune_cache(new_target_cache, tokens_to_discard) if tokens_to_discard > 0 else new_target_cache\n",
    "                    \n",
    "                    last_token = generated_token_ids[-1]\n",
    "                    last_embeds = embed_tokens(torch.tensor([[last_token]], device=self.device))\n",
    "                    last_pos = torch.tensor([[target_cache[0][0].shape[2]]], device=self.device)\n",
    "                    \n",
    "                    lm_step = self.target.language_model(\n",
    "                        inputs_embeds=last_embeds,\n",
    "                        past_key_values=target_cache,\n",
    "                        position_ids=last_pos,\n",
    "                        use_cache=True,\n",
    "                    )\n",
    "                    target_cache = lm_step.past_key_values\n",
    "                    target_logits = lm_step.logits[:, -1, :]\n",
    "                    call_stats.total_target_forward_passes += 1\n",
    "                    \n",
    "                    draft_cache = prune_cache(current_draft_cache, gamma - n_accepted)\n",
    "                    last_draft = self._target_token_to_draft(last_token)\n",
    "                    with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.draft.enable_mixed_precision_training):\n",
    "                        draft_step = self.draft(\n",
    "                            input_ids=torch.tensor([[last_draft]], device=self.device),\n",
    "                            past_key_values=draft_cache,\n",
    "                            use_cache=True,\n",
    "                        )\n",
    "                    draft_cache = draft_step.past_key_values\n",
    "                    draft_logits = draft_step.logits[:, -1, :]\n",
    "                    call_stats.total_draft_forward_passes += 1\n",
    "            \n",
    "            print(f\"\\033[38;2;255;165;0m[BatchedLM] -> \\033[0m Stats: {call_stats}\")\n",
    "        \n",
    "        # Decode tokens to actions\n",
    "        predicted_action_token_ids = np.array(generated_token_ids[:action_dim], dtype=np.int64)\n",
    "        vocab_size = self.target.vocab_size\n",
    "        discretized_actions = vocab_size - predicted_action_token_ids\n",
    "        discretized_actions = np.clip(discretized_actions - 1, a_min=0, a_max=self.target.bin_centers.shape[0] - 1)\n",
    "        normalized_actions = self.target.bin_centers[discretized_actions]\n",
    "        \n",
    "        # Un-normalize\n",
    "        action_norm_stats = self.target.get_action_stats(unnorm_key_target)\n",
    "        mask = action_norm_stats.get(\"mask\", np.ones_like(action_norm_stats[\"q01\"], dtype=bool))\n",
    "        action_high, action_low = np.array(action_norm_stats[\"q99\"]), np.array(action_norm_stats[\"q01\"])\n",
    "        actions = np.where(\n",
    "            mask,\n",
    "            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,\n",
    "            normalized_actions,\n",
    "        )\n",
    "        \n",
    "        # Update global stats\n",
    "        self.stats.total_tokens_generated += call_stats.total_tokens_generated\n",
    "        self.stats.total_draft_tokens_proposed += call_stats.total_draft_tokens_proposed\n",
    "        self.stats.total_draft_tokens_accepted += call_stats.total_draft_tokens_accepted\n",
    "        self.stats.total_target_forward_passes += call_stats.total_target_forward_passes\n",
    "        self.stats.total_draft_forward_passes += call_stats.total_draft_forward_passes\n",
    "        \n",
    "        return actions, call_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from experiments.specdec.vla_speculative_decoding import (\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     VLASpeculativeDecoder,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     prepare_image,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# FIXME: when temperature=0.0, current implementation uses 0.01...\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mcfg\u001b[49m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create speculative decoder\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[3/4] Creating speculative decoder...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "# from experiments.specdec.vla_speculative_decoding import (\n",
    "#     VLASpeculativeDecoder,\n",
    "#     prepare_image,\n",
    "# )\n",
    "# FIXME: when temperature=0.0, current implementation uses 0.01...\n",
    "cfg.gamma = 4\n",
    "\n",
    "# Create speculative decoder\n",
    "print(\"\\n[3/4] Creating speculative decoder...\")\n",
    "# specdec_decoder = VLASpeculativeDecoderDDD(\n",
    "#     target_model=target_model,\n",
    "#     draft_model=draft_model,\n",
    "#     target_processor=target_processor,\n",
    "#     gamma=cfg.gamma,\n",
    "#     use_cache=True,\n",
    "#     temperature=cfg.temperature,\n",
    "# )\n",
    "# specdec_decoder = VLASpeculativeDecoderDDDR(\n",
    "#     target_model=target_model,\n",
    "#     draft_model=draft_model,\n",
    "#     target_processor=target_processor,\n",
    "#     gamma=cfg.gamma,\n",
    "#     use_cache=True,\n",
    "#     temperature=cfg.temperature,\n",
    "#     relaxed_acceptance_r=7,\n",
    "# )\n",
    "# specdec_decoder = VLASpeculativeDecoderDDDRKVB(\n",
    "#     target_model=target_model,\n",
    "#     draft_model=draft_model,\n",
    "#     target_processor=target_processor,\n",
    "#     gamma=cfg.gamma,\n",
    "#     temperature=cfg.temperature,\n",
    "#     relaxed_acceptance_r=7,\n",
    "#     use_cache=True,\n",
    "#     use_batched_verification=False, # incorrect\n",
    "# )\n",
    "\n",
    "specdec_decoder = VLASpeculativeDecoderBatchedLM(\n",
    "    target_model=target_model,\n",
    "    draft_model=draft_model,\n",
    "    target_processor=target_processor,\n",
    "    gamma=cfg.gamma,\n",
    "    temperature=cfg.temperature,\n",
    "    relaxed_acceptance_r=0,  # Relaxed acceptance with r=7 bins\n",
    ")\n",
    "\n",
    "# Prepare PIL image for specdec\n",
    "pil_image = prepare_image(observation[\"full_image\"], center_crop=cfg.center_crop)\n",
    "\n",
    "def run_specdec_inference():\n",
    "    action, stats = specdec_decoder.predict_action_speculative(\n",
    "        pil_image, task_description, unnorm_key_target\n",
    "    )\n",
    "    return action, stats\n",
    "\n",
    "print(\"  Warming up SPECDEC...\")\n",
    "for _ in range(cfg.warmup_iterations):\n",
    "    run_specdec_inference()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Benchmark SPECDEC\n",
    "print(f\"\\nBenchmarking SPECDEC ({cfg.num_iterations} iterations)...\")\n",
    "specdec_times = []\n",
    "specdec_acceptance_rates = []\n",
    "specdec_tokens_per_forward = []\n",
    "specdec_decoder.reset_stats()\n",
    "\n",
    "for i in range(cfg.num_iterations):\n",
    "    (action, stats), dt = timed_cuda(run_specdec_inference)\n",
    "    specdec_times.append(dt)\n",
    "    specdec_acceptance_rates.append(stats.acceptance_rate)\n",
    "    specdec_tokens_per_forward.append(stats.tokens_per_target_forward)\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Progress: {i+1}/{cfg.num_iterations}, last: {dt*1000:.1f}ms, \"\n",
    "                f\"accept: {stats.acceptance_rate:.2%}\")\n",
    "\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def compute_statistics():\n",
    "    # Compute statistics\n",
    "    target_mean = np.mean(target_times)\n",
    "    target_std = np.std(target_times)\n",
    "    draft_mean = np.mean(draft_times)\n",
    "    draft_std = np.std(draft_times)\n",
    "    specdec_mean = np.mean(specdec_times)\n",
    "    specdec_std = np.std(specdec_times)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"\\nTARGET:\")\n",
    "    print(f\"  Mean time: {target_mean*1000:.2f}  {target_std*1000:.2f} ms\")\n",
    "    print(f\"  Throughput: {1/target_mean:.2f} Hz\")\n",
    "\n",
    "    print(f\"\\nDRAFT:\")\n",
    "    print(f\"  Mean time: {draft_mean*1000:.2f}  {draft_std*1000:.2f} ms\")\n",
    "    print(f\"  Throughput: {1/draft_mean:.2f} Hz\")\n",
    "\n",
    "    print(f\"\\nSPECULATIVE DECODING (gamma={cfg.gamma}):\")\n",
    "    print(f\"  Mean time: {specdec_mean*1000:.2f}  {specdec_std*1000:.2f} ms\")\n",
    "    print(f\"  Throughput: {1/specdec_mean:.2f} Hz\")\n",
    "    print(f\"  Acceptance rate: {np.mean(specdec_acceptance_rates):.2%}  {np.std(specdec_acceptance_rates):.2%}\")\n",
    "    print(f\"  Tokens/target forward: {np.mean(specdec_tokens_per_forward):.2f}\")\n",
    "\n",
    "    print(f\"\\nSPEEDUPS:\")\n",
    "    print(f\"  SpecDec vs Target: {target_mean/specdec_mean:.2f}x\")\n",
    "    print(f\"  Draft vs Target: {target_mean/draft_mean:.2f}x\")\n",
    "\n",
    "    # Overall stats from decoder\n",
    "    global_stats = specdec_decoder.stats\n",
    "    print(f\"\\nOVERALL SPECDEC STATS:\")\n",
    "    print(f\"  Total tokens generated: {global_stats.total_tokens_generated}\")\n",
    "    print(f\"  Total draft tokens proposed: {global_stats.total_draft_tokens_proposed}\")\n",
    "    print(f\"  Total draft tokens accepted: {global_stats.total_draft_tokens_accepted}\")\n",
    "    print(f\"  Overall acceptance rate: {global_stats.acceptance_rate:.2%}\")\n",
    "    print(f\"  Total target forward passes: {global_stats.total_target_forward_passes}\")\n",
    "    print(f\"  Total draft forward passes: {global_stats.total_draft_forward_passes}\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "compute_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvla1311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
